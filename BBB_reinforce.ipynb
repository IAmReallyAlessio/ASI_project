{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12061096,"sourceType":"datasetVersion","datasetId":7591461},{"sourceId":12091614,"sourceType":"datasetVersion","datasetId":7611852}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport math\nfrom torch import nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:40.749420Z","iopub.execute_input":"2025-06-07T19:04:40.750094Z","iopub.status.idle":"2025-06-07T19:04:40.754566Z","shell.execute_reply.started":"2025-06-07T19:04:40.750070Z","shell.execute_reply":"2025-06-07T19:04:40.753714Z"}},"outputs":[],"execution_count":148},{"cell_type":"code","source":"# define class for scale mixture gaussian prior\nclass ScaleMixtureGaussian:                               \n    def __init__(self, mixture_weight, stddev_1, stddev_2):\n        super().__init__()\n        # mixture_weight is the weight for the first gaussian\n        self.mixture_weight = mixture_weight\n        # stddev_1 and stddev_2 are the standard deviations for the two gaussians\n        self.stddev_1 = stddev_1\n        self.stddev_2 = stddev_2\n        # create two normal distributions with the specified standard deviations\n        self.gaussian1 = torch.distributions.Normal(0,stddev_1)\n        self.gaussian2 = torch.distributions.Normal(0,stddev_2)\n\n\n    def log_prob(self, x):\n        prob1 = torch.exp(self.gaussian1.log_prob(x))\n        prob2 = torch.exp(self.gaussian2.log_prob(x))\n        return (torch.log(self.mixture_weight * prob1 + (1-self.mixture_weight) * prob2)).sum()\n    \n# define class for gaussian node\nclass GaussianNode:\n    def __init__(self, mean, rho_param):\n        super().__init__()\n        self.mean = mean\n        self.rho_param = rho_param\n        self.normal = torch.distributions.Normal(0,1)\n    \n    # Calculate the standard deviation from the rho parameter\n    def sigma(self):\n        return torch.log1p(torch.exp(self.rho_param))\n\n    # Sample from the Gaussian node\n    def sample(self):\n        epsilon = self.normal.sample(self.rho_param.size()).cuda()\n        return self.mean + self.sigma() * epsilon\n    \n    # Calculate the KL divergence between the prior and the variational posterior\n    def log_prob(self, x):\n        return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma()) - ((x - self.mean) ** 2) / (2 * self.sigma() ** 2)).sum()\n\nclass BayesianLinear(nn.Module):\n    def __init__(self, in_features, out_features, mu_init, rho_init, prior_init):\n        super().__init__()\n\n        # Initialize the parameters for the weights and biases\n        self.weight_mean = nn.Parameter(torch.empty(out_features, in_features).uniform_(*mu_init))\n        self.weight_rho_param = nn.Parameter(torch.empty(out_features, in_features).uniform_(*rho_init))\n        self.weight = GaussianNode(self.weight_mean, self.weight_rho_param)\n\n        self.bias_mean = nn.Parameter(torch.empty(out_features).uniform_(*mu_init))\n        self.bias_rho_param = nn.Parameter(torch.empty(out_features).uniform_(*rho_init))\n        self.bias = GaussianNode(self.bias_mean, self.bias_rho_param)\n        \n        self.weight_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n        self.bias_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n\n        self.log_prior = 0\n        self.log_variational_posterior = 0\n\n    def forward(self, x):\n        weight = self.weight.sample()\n        bias = self.bias.sample()\n\n        return nn.functional.linear(x, weight, bias)\n\nclass BayesianNetwork(nn.Module):\n    def __init__(self, model_params):\n        super().__init__()\n        self.input_shape = model_params['input_shape']\n        self.classes = model_params['classes']\n        self.batch_size = model_params['batch_size']\n        self.hidden_units = model_params['hidden_units']\n        self.experiment = model_params['experiment']\n        self.mu_init = model_params['mu_init']\n        self.rho_init = model_params['rho_init']\n        self.prior_init = model_params['prior_init']\n\n        self.fc1 = BayesianLinear(self.input_shape, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n        self.fc1_activation = nn.ReLU()\n        self.fc2 = BayesianLinear(self.hidden_units, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n        self.fc2_activation = nn.ReLU()\n        self.fc3 = BayesianLinear(self.hidden_units, self.classes, self.mu_init, self.rho_init, self.prior_init)\n    \n    def forward(self, x):\n        if self.experiment == 'classification':\n            x = x.view(-1, self.input_shape) # Flatten images\n        x = self.fc1_activation(self.fc1(x))\n        x = self.fc2_activation(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def log_prior(self):\n        return self.fc1.log_prior + self.fc2.log_prior + self.fc3.log_prior\n    \n    def log_variational_posterior(self):\n        return self.fc1.log_variational_posterior + self.fc2.log_variational_posterior + self.fc3.log_variational_posterior\n\n\n    def get_nll(self, outputs, target, sigma=1.):\n        if self.experiment == 'regression': #  -(.5 * (target - outputs) ** 2).sum()\n            nll = -torch.distributions.Normal(outputs, sigma).log_prob(target).sum()\n        elif self.experiment == 'classification':\n            nll = nn.CrossEntropyLoss(reduction='sum')(outputs, target)\n        return nll\n\n    def sample_elbo(self, x, target, beta, samples, sigma=1.):\n        log_prior = torch.zeros(1).to(device)\n        log_variational_posterior = torch.zeros(1).to(device)\n        negative_log_likelihood = torch.zeros(1).to(device)\n\n        for i in range(samples):\n            output = self.forward(x)\n            log_prior += self.log_prior()\n            log_variational_posterior += self.log_variational_posterior()\n            negative_log_likelihood += self.get_nll(output, target, sigma)\n\n        log_prior = beta*(log_prior / samples)\n        log_variational_posterior = beta*(log_variational_posterior / samples) \n        negative_log_likelihood = negative_log_likelihood / samples\n        loss = log_variational_posterior - log_prior + negative_log_likelihood\n        return loss, log_prior, log_variational_posterior, negative_log_likelihood    \n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.755606Z","iopub.execute_input":"2025-06-07T19:04:40.755920Z","iopub.status.idle":"2025-06-07T19:04:40.778577Z","shell.execute_reply.started":"2025-06-07T19:04:40.755892Z","shell.execute_reply":"2025-06-07T19:04:40.778025Z"},"trusted":true},"outputs":[],"execution_count":149},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, model_params):\n        super().__init__()\n        self.input_shape = model_params['input_shape']\n        self.classes = model_params['classes']\n        self.batch_size = model_params['batch_size']\n        self.hidden_units = model_params['hidden_units']\n        self.experiment = model_params['experiment']\n\n        self.net = nn.Sequential(\n            nn.Linear(self.input_shape, self.hidden_units),\n            nn.ReLU(),\n            nn.Linear(self.hidden_units, self.hidden_units),\n            nn.ReLU(),\n            nn.Linear(self.hidden_units, self.classes))\n    \n    def forward(self, x):\n        if self.experiment == 'classification':\n            x = x.view(-1, self.input_shape) # Flatten images\n        \n        x = self.net(x)\n        return x\n\nclass MLP_Dropout(nn.Module):\n    def __init__(self, model_params):\n        super().__init__()\n        self.input_shape = model_params['input_shape']\n        self.classes = model_params['classes']\n        self.batch_size = model_params['batch_size']\n        self.hidden_units = model_params['hidden_units']\n        self.experiment = model_params['experiment']\n\n        self.net = nn.Sequential(\n            nn.Linear(self.input_shape, self.hidden_units),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(self.hidden_units, self.hidden_units),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(self.hidden_units, self.classes))\n    \n    def forward(self, x):\n        if self.experiment == 'classification':\n            x = x.view(-1, self.input_shape) # Flatten images\n       \n        x = self.net(x)\n        return x\n\n    def enable_dropout(self):\n        ''' Enable the dropout layers during test-time '''\n        for m in self.modules():\n            if m.__class__.__name__.startswith('Dropout'):\n                m.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:40.779653Z","iopub.execute_input":"2025-06-07T19:04:40.779927Z","iopub.status.idle":"2025-06-07T19:04:40.806068Z","shell.execute_reply.started":"2025-06-07T19:04:40.779904Z","shell.execute_reply":"2025-06-07T19:04:40.805547Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"class RegConfig:\n    # save_dir = './saved_models'\n    train_size = 1024\n    batch_size = 128\n    lr = 1e-3\n    epochs = 100 #1000\n    train_samples = 5                   # number of train samples for MC gradients\n    test_samples = 10                   # number of test samples for MC averaging\n    num_test_points = 400               # number of test points\n    experiment = 'regression'\n    hidden_units = 400                  # number of hidden units\n    noise_tolerance = .1                # log likelihood sigma\n    mu_init = [-0.2, 0.2]               # range for mean \n    rho_init = [-5, -4]                 # range for rho_param\n    prior_init = [0.5, -0, -6]        # mixture weight, log(stddev_1), log(stddev_2)\n   \n\nclass RLConfig:\n    data_dir = '/kaggle/input/mushroom-dataset/mushroom/agaricus-lepiota.data'\n    batch_size = 64\n    num_batches = 64\n    buffer_size = batch_size * num_batches  # buffer to track latest batch of mushrooms\n    lr = 1e-4\n    training_steps = 10000\n    experiment = 'regression'\n    hidden_units = 100                      # number of hidden units\n    mu_init = [-0.2, 0.2]                   # range for mu \n    rho_init = [-5, -4]                     # range for rho\n    prior_init = [0.5, -0, -6]               # mixture weight, log(stddev_1), log(stddev_2)\n\nclass ClassConfig:\n    batch_size = 128\n    lr = 1e-3 # 1e-5 fa schifo, 1e-4 parte da 8% errore, 1e-3 parte da 5%\n    epochs = 600 #100\n    hidden_units = 1200\n    experiment = 'classification'\n    dropout = True\n    train_samples = 1 # 10 Ã¨ troppo lento\n    test_samples = 10\n    x_shape = 28 * 28                       # x shape\n    classes = 10                            # number of output classes\n    mu_init = [-0.2, 0.2]                   # range for mean \n    rho_init = [-5, -4]                     # range for rho_param\n    prior_init = [0.75, 0, -7]             # mixture weight, log(stddev_1), log(stddev_2)","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.806670Z","iopub.execute_input":"2025-06-07T19:04:40.807357Z","iopub.status.idle":"2025-06-07T19:04:40.829172Z","shell.execute_reply.started":"2025-06-07T19:04:40.807339Z","shell.execute_reply":"2025-06-07T19:04:40.828635Z"},"trusted":true},"outputs":[],"execution_count":151},{"cell_type":"code","source":"class PrepareData(Dataset):\n    def __init__(self, X, y):\n        if not torch.is_tensor(X):\n            self.X = torch.from_numpy(X)\n        else:\n            self.X = X\n        if not torch.is_tensor(y):\n            self.y = torch.from_numpy(y)\n        else:\n            self.y = y # vedere\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndef read_data_rl(data_dir):\n    '''\n    Read in data for contextual bandits\n    - transform context and label to one-hot encoded vectors\n    '''\n    df = pd.read_csv(data_dir, sep=',', header=None)\n    df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n    X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n    Y = df['class']\n\n    # transform to one-hot encoding\n    label_encoder = preprocessing.LabelEncoder()\n    label_encoder.fit(Y)\n    Y_ = label_encoder.transform(Y)\n    X_ = X.copy()\n    for feature in X.columns:\n        label_encoder.fit(X[feature])\n        X_[feature] = label_encoder.transform(X[feature])\n\n    oh_encoder = preprocessing.OneHotEncoder()\n    oh_encoder.fit(X_)\n    X_ = oh_encoder.transform(X_).toarray()\n\n    return X_, Y_\n\ndef create_data_reg(train_size):\n    np.random.seed(0)\n    xs = np.random.uniform(low=0., high=0.6, size=train_size)\n    \n    eps = np.random.normal(loc=0., scale=0.02, size=[train_size])\n\n    ys = xs + 0.3 * np.sin(2*np.pi * (xs + eps)) + 0.3 * np.sin(4*np.pi * (xs + eps)) + eps\n\n    xs = torch.from_numpy(xs).reshape(-1,1).float()\n    ys = torch.from_numpy(ys).reshape(-1,1).float()\n\n    return xs, ys","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.858647Z","iopub.execute_input":"2025-06-07T19:04:40.859068Z","iopub.status.idle":"2025-06-07T19:04:40.867631Z","shell.execute_reply.started":"2025-06-07T19:04:40.859052Z","shell.execute_reply":"2025-06-07T19:04:40.866869Z"},"trusted":true},"outputs":[],"execution_count":152},{"cell_type":"code","source":"# def load_bnn_class_model(saved_model):\n#     config = ClassConfig\n\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'mu_init': config.mu_init,\n#         'rho_init': config.rho_init,\n#         'prior_init': config.prior_init\n#     }\n#     model = BayesianNetwork(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()\n\n# def load_mlp_class_model(saved_model):\n#     config = ClassConfig\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#     }\n#     model = MLP(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()\n\n# def load_dropout_class_model(saved_model):\n#     config = ClassConfig\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'dropout': True\n#     }\n#     model = MLP_Dropout(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.868646Z","iopub.execute_input":"2025-06-07T19:04:40.868895Z","iopub.status.idle":"2025-06-07T19:04:40.890666Z","shell.execute_reply.started":"2025-06-07T19:04:40.868876Z","shell.execute_reply":"2025-06-07T19:04:40.890024Z"},"trusted":true},"outputs":[],"execution_count":153},{"cell_type":"code","source":"def create_regression_plot(X_test, y_test, train_ds):\n    fig = plt.figure(figsize=(9, 6))\n    plt.plot(X_test, np.median(y_test, axis=0), label='Median Posterior Predictive')\n    \n    # Range\n    plt.fill_between(\n        X_test.reshape(-1), \n        np.percentile(y_test, 0, axis=0), \n        np.percentile(y_test, 100, axis=0), \n        alpha = 0.2, color='orange', label='Range') #color='blue',\n    \n    # interquartile range\n    plt.fill_between(\n        X_test.reshape(-1), \n        np.percentile(y_test, 25, axis=0), \n        np.percentile(y_test, 75, axis=0), \n        alpha = 0.4,  label='Interquartile Range') #color='red',\n    \n    plt.scatter(train_ds.dataset.X, train_ds.dataset.y, label='Training data', marker='x', alpha=0.5, color='k', s=2)\n    plt.yticks(fontsize=20)\n    plt.xticks(fontsize=20)\n    plt.ylim([-1.5, 1.5])\n    plt.xlim([-0.6, 1.4])\n\n   ","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.891381Z","iopub.execute_input":"2025-06-07T19:04:40.891618Z","iopub.status.idle":"2025-06-07T19:04:40.913268Z","shell.execute_reply.started":"2025-06-07T19:04:40.891597Z","shell.execute_reply":"2025-06-07T19:04:40.912587Z"},"trusted":true},"outputs":[],"execution_count":154},{"cell_type":"code","source":"class BNN_Classification():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.n_samples = parameters['train_samples']\n        self.test_samples = parameters['test_samples']\n        self.x_shape = parameters['x_shape']\n        self.classes = parameters['classes']\n        self.mu_init = parameters['mu_init']\n        self.rho_init = parameters['rho_init']\n        self.prior_init = parameters['prior_init']\n        self.best_acc = 0.\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.classes,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment,\n            'mu_init': self.mu_init,\n            'rho_init': self.rho_init,\n            'prior_init': self.prior_init,\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n        # print(f'Classification Task {self.label} Parameters: ')\n        # print(f'number of samples: {self.n_samples}')\n        # print(\"BNN Parameters: \")\n        # print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for idx, (x, y) in enumerate(tqdm(train_data)):\n            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)            \n            net_loss = self.loss_info[0]\n            net_loss.backward()\n            self.optimiser.step()\n\n    def predict(self, X):\n        probs = torch.zeros(size=[self.batch_size, self.classes]).to(device)\n        for _ in torch.arange(self.test_samples):\n            out = torch.nn.Softmax(dim=1)(self.net(X))\n            probs = probs + out / self.test_samples\n        preds = torch.argmax(probs, dim=1)\n        return preds, probs\n\n    def evaluate(self, test_loader):\n        self.net.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for data in tqdm(test_loader):\n                X, y = data\n                X, y = X.to(device), y.to(device)\n                preds, _ = self.predict(X)\n                total += self.batch_size\n                correct += (preds == y).sum().item()\n        self.acc = correct / total\n        print(f'{self.label} validation accuracy: {self.acc}')  \n        return self.acc\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-07T19:04:40.914796Z","iopub.execute_input":"2025-06-07T19:04:40.915000Z","iopub.status.idle":"2025-06-07T19:04:40.938101Z","shell.execute_reply.started":"2025-06-07T19:04:40.914985Z","shell.execute_reply":"2025-06-07T19:04:40.937564Z"},"trusted":true},"outputs":[],"execution_count":155},{"cell_type":"code","source":"class MLP_Classification():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.x_shape = parameters['x_shape']\n        self.classes = parameters['classes']\n        self.best_acc = 0.\n        self.dropout = parameters['dropout']\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.classes,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment,\n            'dropout': self.dropout,\n        }\n        self.net = MLP_Dropout(model_params).to(device)  \n        self.optimiser = torch.optim.SGD(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n\n    def train_step(self, train_data):\n        self.net.train()\n        for _, (x, y) in enumerate(tqdm(train_data)):\n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = torch.nn.functional.cross_entropy(self.net(x), y, reduction='sum')\n            self.loss_info.backward()\n            self.optimiser.step()\n\n    def predict(self, X):\n        probs = torch.nn.Softmax(dim=1)(self.net(X))\n        preds = torch.argmax(probs, dim=1)\n        return preds, probs\n\n    def evaluate(self, test_loader):\n        self.net.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for data in tqdm(test_loader):\n                X, y = data\n                X, y = X.to(device), y.to(device)\n                preds, _ = self.predict(X)\n                total += self.batch_size\n                correct += (preds == y).sum().item()\n        self.acc = correct / total\n        print(f'{self.label} validation accuracy: {self.acc}') \n        return self.acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:40.938708Z","iopub.execute_input":"2025-06-07T19:04:40.938908Z","iopub.status.idle":"2025-06-07T19:04:40.961552Z","shell.execute_reply.started":"2025-06-07T19:04:40.938893Z","shell.execute_reply":"2025-06-07T19:04:40.960921Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import SubsetRandomSampler\nimport matplotlib.pyplot as plt\n\ndef class_trainer():\n    config = ClassConfig\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x * 255 / 126.),\n    ])\n\n    train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n    test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n\n    # valid_size = 1 / 6\n    # num_train = len(train_data)\n    # indices = list(range(num_train))\n    # split = int(valid_size * num_train)\n    # train_idx, valid_idx = indices[split:], indices[:split]\n\n    # train_sampler = SubsetRandomSampler(train_idx)\n    # valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=config.batch_size, shuffle=True, drop_last=True)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=config.batch_size, shuffle=False, drop_last=True)\n\n    params = {\n        'lr': config.lr,\n        'hidden_units': config.hidden_units,\n        'experiment': config.experiment,\n        'dropout': config.dropout,\n        'batch_size': config.batch_size,\n        'epochs': config.epochs,\n        'x_shape': config.x_shape,\n        'classes': config.classes,\n        'num_batches': len(train_loader),\n        'train_samples': config.train_samples,\n        'test_samples': config.test_samples,\n        'mu_init': config.mu_init,\n        'rho_init': config.rho_init,\n        'prior_init': config.prior_init,\n    }\n\n    # Instantiate both models\n    bnn_model = BNN_Classification('bnn_classification', {**params})\n    mlp_model = MLP_Classification('mlp_classification', {**params})\n\n    bnn_test_errors = []\n    mlp_test_errors = []\n\n    epochs = config.epochs\n    for epoch in range(epochs):\n        print(f'--- Epoch {epoch+1}/{epochs} ---')\n\n        # Train both models\n        bnn_model.train_step(train_loader)\n        mlp_model.train_step(train_loader)\n\n        # Evaluate on test set\n        bnn_test_acc = bnn_model.evaluate(test_loader)\n        mlp_test_acc = mlp_model.evaluate(test_loader)\n\n        bnn_test_error = 100 * (1 - bnn_test_acc)\n        mlp_test_error = 100 * (1 - mlp_test_acc)\n\n        bnn_test_errors.append(bnn_test_error)\n        mlp_test_errors.append(mlp_test_error)\n\n        print(f'BNN Test Error: {bnn_test_error:.2f}%')\n        print(f'MLP Test Error: {mlp_test_error:.2f}%')\n\n        # Learning rate scheduler step if defined\n        bnn_model.scheduler.step()\n        mlp_model.scheduler.step()\n\n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.plot(bnn_test_errors, label='BNN Test Error')\n    plt.plot(mlp_test_errors, label='MLP Test Error')\n    plt.xlabel('Epoch')\n    plt.ylabel('Test Error (%)')\n    plt.title('BNN vs MLP Test Error on MNIST')\n    plt.legend()\n    plt.grid(True)\n    \n    # Set x-axis ticks every 5 epochs\n    epochs = len(bnn_test_errors)\n    plt.xticks(ticks=range(0, epochs, 5))\n    \n    plt.tight_layout()\n    plt.show()\n\n\n#class_trainer()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:40.962288Z","iopub.execute_input":"2025-06-07T19:04:40.962528Z","iopub.status.idle":"2025-06-07T19:04:40.983574Z","shell.execute_reply.started":"2025-06-07T19:04:40.962508Z","shell.execute_reply":"2025-06-07T19:04:40.982922Z"},"trusted":true},"outputs":[],"execution_count":157},{"cell_type":"code","source":"# search_config = {\n#     'batch_size': [128],\n#     'lr': [1e-3, 1e-4],\n#     'epochs': [10],\n#     'hidden_units': [1200],\n#     'experiment': ['classification'],\n#     'dropout': [False],\n#     'train_samples': [1, 2, 5],\n#     'test_samples': [10],\n#     'x_shape': [28 * 28],\n#     'classes': [10],\n#     'mu_init': [[-0.2, 0.2]],\n#     'rho_init': [[-5, -4]],\n#     'prior_init': [\n#         [0.25, -0, -6],\n#         [0.25, -0, -7], \n#         [0.25, -1, -6], \n#         [0.25, -1, -7], \n#         [0.75, -0, -6],\n#         [0.75, -0, -7], \n#         [0.75, -1, -6], \n#         [0.75, -1, -7],       \n#     ]\n# }\n\n# # search_config = {\n# #     'batch_size': [128],\n# #     'lr': [1e-3, 1e-4],\n# #     'epochs': [1], #10\n# #     'hidden_units': [1200],\n# #     'experiment': ['classification'],\n# #     'dropout': [False],\n# #     'train_samples': [1],\n# #     'test_samples': [10],\n# #     'x_shape': [28 * 28],\n# #     'classes': [10],\n# #     'mu_init': [[-0.2, 0.2]],\n# #     'rho_init': [[-5, -4]],\n# #     'prior_init': [\n# #         [0.25, -0, -6],        \n# #     ]\n# # }\n\n\n# import itertools\n# from copy import deepcopy\n\n# def generate_param_combinations(param_grid):\n#     keys = list(param_grid.keys())\n#     values = list(param_grid.values())\n#     for combo in itertools.product(*values):\n#         yield dict(zip(keys, combo))\n\n\n# def class_trainer(config):\n    \n#     transform = transforms.Compose([\n#             transforms.ToTensor(),\n#             transforms.Lambda(lambda x: x * 255 / 126.),  # divide as in paper, * 255 gives better results\n#         ])\n\n#     train_data = datasets.MNIST(\n#             root='data',\n#             train=True,\n#             download=True,\n#             transform=transform)\n#     # test_data = datasets.MNIST(\n#     #         root='data',\n#     #         train=False,\n#     #         download=True,\n#     #         transform=transform)\n\n#     valid_size = 1 / 6\n\n#     num_train = len(train_data)\n#     indices = list(range(num_train))\n#     split = int(valid_size * num_train)\n#     train_idx, valid_idx = indices[split:], indices[:split]\n\n#     train_sampler = SubsetRandomSampler(train_idx)\n#     valid_sampler = SubsetRandomSampler(valid_idx)\n\n\n#     train_loader = torch.utils.data.DataLoader(\n#             train_data,\n#             batch_size=config['batch_size'],\n#             sampler=train_sampler,\n#             drop_last=True)\n#     valid_loader = torch.utils.data.DataLoader(\n#             train_data,\n#             batch_size=config['batch_size'],\n#             sampler=valid_sampler,\n#             drop_last=True)\n#     # test_loader = torch.utils.data.DataLoader(\n#     #         test_data,\n#     #         batch_size=config.batch_size,\n#     #         shuffle=False,\n#     #         drop_last=True)\n\n#     params = deepcopy(config)\n#     params['num_batches'] = len(train_loader)\n\n#     model = BNN_Classification('bnn_classification', {**params})\n#     #model = MLP_Classification('mlp_classification', {**params})\n    \n#     epochs = config['epochs']\n#     for epoch in range(epochs):\n#             print(f'Epoch {epoch+1}/{epochs}')\n#             model.train_step(train_loader)\n#             valid_acc = model.evaluate(valid_loader)\n#             print('Valid Error', round(100 * (1 - valid_acc), 3), '%',)\n#             model.scheduler.step()\n#             if model.acc > model.best_acc:\n#                 model.best_acc = model.acc\n                \n\n#     return model.best_acc, model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:40.984320Z","iopub.execute_input":"2025-06-07T19:04:40.984568Z","iopub.status.idle":"2025-06-07T19:04:41.009083Z","shell.execute_reply.started":"2025-06-07T19:04:40.984546Z","shell.execute_reply":"2025-06-07T19:04:41.008417Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"# best_val_acc = 0.0\n# best_config = None\n# best_model = None\n\n# for config in generate_param_combinations(search_config):\n#     print(f\"Trying config: {config}\")\n#     val_acc, model = class_trainer(config)\n\n#     if val_acc > best_val_acc:\n#         best_val_acc = val_acc\n#         best_config = deepcopy(config)\n#         best_model = model\n\n# print(\"Best Config:\")\n# print(best_config)\n# print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.009807Z","iopub.execute_input":"2025-06-07T19:04:41.010060Z","iopub.status.idle":"2025-06-07T19:04:41.030070Z","shell.execute_reply.started":"2025-06-07T19:04:41.010044Z","shell.execute_reply":"2025-06-07T19:04:41.029564Z"}},"outputs":[],"execution_count":159},{"cell_type":"code","source":"# from tqdm import tqdm\n# import numpy as np\n# import torch\n# from torch.utils.data import SubsetRandomSampler\n# from torchvision import datasets, transforms\n# from itertools import product\n\n# def class_trainer(config):\n\n#     transform = transforms.Compose([\n#         transforms.ToTensor(),\n#         transforms.Lambda(lambda x: x * 255. / 126.),\n#     ])\n\n#     train_data = datasets.MNIST(\n#         root='data',\n#         train=True,\n#         download=True,\n#         transform=transform)\n#     test_data = datasets.MNIST(\n#         root='data',\n#         train=False,\n#         download=True,\n#         transform=transform)\n\n#     valid_size = 1 / 6\n#     num_train = len(train_data)\n#     indices = list(range(num_train))\n#     split = int(valid_size * num_train)\n#     train_idx, valid_idx = indices[split:], indices[:split]\n\n#     train_sampler = SubsetRandomSampler(train_idx)\n#     valid_sampler = SubsetRandomSampler(valid_idx)\n\n#     train_loader = torch.utils.data.DataLoader(\n#         train_data,\n#         batch_size=config['batch_size'],\n#         sampler=train_sampler,\n#         drop_last=True)\n#     valid_loader = torch.utils.data.DataLoader(\n#         train_data,\n#         batch_size=config['batch_size'],\n#         sampler=valid_sampler,\n#         drop_last=True)\n#     test_loader = torch.utils.data.DataLoader(\n#         test_data,\n#         batch_size=config['batch_size'],\n#         shuffle=False,\n#         drop_last=True)\n\n#     params = {\n#         'lr': config['lr'],\n#         'hidden_units': config['hidden_units'],\n#         'experiment': config['experiment'],\n#         'batch_size': config['batch_size'],\n#         'epochs': config['epochs'],\n#         'x_shape': config['x_shape'],\n#         'classes': config['classes'],\n#         'num_batches': len(train_loader),\n#         'train_samples': config['train_samples'],\n#         'test_samples': config['test_samples'],\n#         'mu_init': config['mu_init'],\n#         'rho_init': config['rho_init'],\n#         'prior_init': config['prior_init'],\n#     }\n\n#     model = BNN_Classification('bnn_classification', {**params, 'dropout': False})\n\n#     best_val_acc = 0\n#     for epoch in range(config['epochs']):\n#         print(f'Epoch {epoch + 1}/{config[\"epochs\"]}')\n#         model.train_step(train_loader)\n#         valid_acc = model.evaluate(valid_loader)\n#         print('Valid Error', round(100 * (1 - valid_acc), 3), '%')\n#         model.scheduler.step()\n#         if model.acc > model.best_acc:\n#             model.best_acc = model.acc\n#             best_val_acc = valid_acc\n#             # torch.save(model.net.state_dict(), \"best_model.pt\")\n\n#     return best_val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.030788Z","iopub.execute_input":"2025-06-07T19:04:41.031226Z","iopub.status.idle":"2025-06-07T19:04:41.046773Z","shell.execute_reply.started":"2025-06-07T19:04:41.031204Z","shell.execute_reply":"2025-06-07T19:04:41.046270Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"# def run_grid_search():\n#     # Define the grid\n#     param_grid = {\n#         'lr': [1e-3, 1e-4],\n#         'batch_size': [64, 128],\n#         'hidden_units': [128, 256],\n#     }\n\n#     # Fixed config options\n#     base_config = {\n#         'experiment': 'grid_search',\n#         'epochs': 5,\n#         'x_shape': (1, 28, 28),\n#         'classes': 10,\n#         'train_samples': 60000,\n#         'test_samples': 10000,\n#         'mu_init': 0,\n#         'rho_init': -3,\n#         'prior_init': 0,\n#     }\n\n#     best_acc = 0\n#     best_params = None\n\n#     # Iterate over all combinations\n#     for lr, batch_size, hidden_units in product(param_grid['lr'], param_grid['batch_size'], param_grid['hidden_units']):\n#         config = {\n#             **base_config,\n#             'lr': lr,\n#             'batch_size': batch_size,\n#             'hidden_units': hidden_units\n#         }\n\n#         print(f\"\\nRunning with config: {config}\")\n#         acc = class_trainer(config)\n\n#         if acc > best_acc:\n#             best_acc = acc\n#             best_params = config\n\n#     print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n#     print(f\"Best Config: {best_params}\")\n\n# run_grid_search()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.048555Z","iopub.execute_input":"2025-06-07T19:04:41.048849Z","iopub.status.idle":"2025-06-07T19:04:41.068078Z","shell.execute_reply.started":"2025-06-07T19:04:41.048806Z","shell.execute_reply":"2025-06-07T19:04:41.067387Z"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":"class BNN_Regression():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.n_samples = parameters['train_samples']\n        self.test_samples = parameters['test_samples']\n        self.x_shape = parameters['x_shape']\n        self.y_shape = parameters['y_shape']\n        self.noise_tol = parameters['noise_tolerance']\n        self.lr = parameters['lr']\n        self.best_loss = np.inf\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.y_shape,\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment'],\n            'mu_init': parameters['mu_init'],\n            'rho_init': parameters['rho_init'],\n            'prior_init': parameters['prior_init']\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=500, gamma=0.5)\n        # print(f'Regression Task {self.label} Parameters: ')\n        # print(f'number of samples: {self.n_samples}, noise tolerance: {self.noise_tol}')\n        print(\"BNN Parameters: \")\n        print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, mu_init: {parameters[\"mu_init\"]}, rho_init: {parameters[\"rho_init\"]}, prior_init: {parameters[\"prior_init\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for idx, (x, y) in enumerate(train_data):\n            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples, sigma=self.noise_tol)\n            net_loss = self.loss_info[0]\n            net_loss.backward()\n            self.optimiser.step()\n        self.epoch_loss = net_loss.item()\n\n    def evaluate(self, x_test):\n        self.net.eval()\n        with torch.no_grad():\n            y_test = np.zeros((self.test_samples, x_test.shape[0]))\n            for s in range(self.test_samples):\n                tmp = self.net(x_test.to(device)).detach().cpu().numpy()\n                y_test[s,:] = tmp.reshape(-1)\n            return y_test","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:41.068689Z","iopub.execute_input":"2025-06-07T19:04:41.068895Z","iopub.status.idle":"2025-06-07T19:04:41.087127Z","shell.execute_reply.started":"2025-06-07T19:04:41.068881Z","shell.execute_reply":"2025-06-07T19:04:41.086571Z"},"trusted":true},"outputs":[],"execution_count":162},{"cell_type":"code","source":"class MLP_Regression():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.x_shape = parameters['x_shape']\n        self.y_shape = parameters['y_shape']\n        self.best_loss = np.inf\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.y_shape,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment\n        }\n        self.net = MLP(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(\"MLP Parameters: \")\n        print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for _, (x, y) in enumerate(train_data):\n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = torch.nn.functional.mse_loss(self.net(x), y, reduction='sum')\n            self.loss_info.backward()\n            self.optimiser.step()\n\n        self.epoch_loss = self.loss_info.item()\n\n    def evaluate(self, x_test):\n        self.net.eval()\n        with torch.no_grad():\n            y_test = self.net(x_test.to(device)).detach().cpu().numpy()\n            return y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.087862Z","iopub.execute_input":"2025-06-07T19:04:41.088102Z","iopub.status.idle":"2025-06-07T19:04:41.114324Z","shell.execute_reply.started":"2025-06-07T19:04:41.088081Z","shell.execute_reply":"2025-06-07T19:04:41.113852Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"# def reg_trainer():\n#     config = RegConfig\n#     X, Y = create_data_reg(train_size=config.train_size)\n#     train_loader = PrepareData(X, Y)\n#     train_loader = DataLoader(train_loader, batch_size=config.batch_size, shuffle=True)\n\n#     params = {\n#         'lr': config.lr,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'batch_size': config.batch_size,\n#         'num_batches': len(train_loader),\n#         'x_shape': X.shape[1],\n#         'y_shape': Y.shape[1],\n#         'train_samples': config.train_samples,\n#         'test_samples': config.test_samples,\n#         'noise_tolerance': config.noise_tolerance,\n#         'mu_init': config.mu_init,\n#         'rho_init': config.rho_init,\n#         'prior_init': config.prior_init,\n#     }\n\n#     model = BNN_Regression('bnn_regression', {**params})\n#     #model = MLP_Regression('mlp_regression', {**params})\n\n#     epochs = config.epochs\n#     print(f\"Initialising training on {device}...\")\n\n#     # training loop\n#     for epoch in tqdm(range(epochs)):\n    \n#         model.train_step(train_loader)\n#         model.scheduler.step()\n#         # save best model\n#         if model.epoch_loss < model.best_loss:\n#             model.best_loss = model.epoch_loss\n#             # torch.save(model.net.state_dict(), model.save_model_path)\n\n#     # evaluate\n#     print(\"Evaluating and generating plots...\")\n#     x_test = torch.linspace(-2., 2, config.num_test_points).reshape(-1, 1)\n    \n#     # model.net.load_state_dict(torch.load(model.save_model_path, map_location=torch.device(device)))\n#     y_test = model.evaluate(x_test)\n    \n#     #create_regression_plot(x_test.cpu().numpy(), y_test.reshape(1, -1), train_loader) #per mlp regression\n   \n#     create_regression_plot(x_test.cpu().numpy(), y_test, train_loader)\n\n\n# reg_trainer()","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:41.114906Z","iopub.execute_input":"2025-06-07T19:04:41.115091Z","iopub.status.idle":"2025-06-07T19:04:41.137019Z","shell.execute_reply.started":"2025-06-07T19:04:41.115077Z","shell.execute_reply":"2025-06-07T19:04:41.136486Z"},"trusted":true},"outputs":[],"execution_count":164},{"cell_type":"code","source":"# def reg_trainer():\n#     config = RegConfig\n#     X, Y = create_data_reg(train_size=config.train_size)\n#     train_loader = PrepareData(X, Y)\n#     train_loader = DataLoader(train_loader, batch_size=config.batch_size, shuffle=True)\n\n#     params = {\n#         'lr': config.lr,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'batch_size': config.batch_size,\n#         'num_batches': len(train_loader),\n#         'x_shape': X.shape[1],\n#         'y_shape': Y.shape[1],\n#         'train_samples': config.train_samples,\n#         'test_samples': config.test_samples,\n#         'noise_tolerance': config.noise_tolerance,\n#         'mu_init': config.mu_init,\n#         'rho_init': config.rho_init,\n#         'prior_init': config.prior_init,\n#     }\n\n#     #model = BNN_Regression('bnn_regression', {**params})\n#     model = MLP_Regression('mlp_regression', {**params})\n\n#     epochs = config.epochs\n#     print(f\"Initialising training on {device}...\")\n\n#     # training loop\n#     for epoch in tqdm(range(epochs)):\n    \n#         model.train_step(train_loader)\n#         model.scheduler.step()\n#         # save best model\n#         if model.epoch_loss < model.best_loss:\n#             model.best_loss = model.epoch_loss\n#             # torch.save(model.net.state_dict(), model.save_model_path)\n\n#     # evaluate\n#     print(\"Evaluating and generating plots...\")\n#     x_test = torch.linspace(-2., 2, config.num_test_points).reshape(-1, 1)\n    \n#     # model.net.load_state_dict(torch.load(model.save_model_path, map_location=torch.device(device)))\n#     y_test = model.evaluate(x_test)\n    \n#     create_regression_plot(x_test.cpu().numpy(), y_test.reshape(1, -1), train_loader) #per mlp regression\n   \n#     #create_regression_plot(x_test.cpu().numpy(), y_test, train_loader)\n\n\n# reg_trainer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.137657Z","iopub.execute_input":"2025-06-07T19:04:41.138304Z","iopub.status.idle":"2025-06-07T19:04:41.160453Z","shell.execute_reply.started":"2025-06-07T19:04:41.138282Z","shell.execute_reply":"2025-06-07T19:04:41.159981Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"'''\nDefines base class for contextual bandits\n'''\nimport torch\nimport numpy as np\n\n\n\nclass Bandit():\n    def __init__(self, label, bandit_params, x, y):\n        self.n_samples = bandit_params['n_samples']\n        self.buffer_size = bandit_params['buffer_size']\n        self.batch_size = bandit_params['batch_size']\n        self.num_batches = bandit_params['num_batches']\n        self.lr = bandit_params['lr']\n        self.epsilon = bandit_params['epsilon']\n        self.cumulative_regrets = [0]\n        self.buffer_x, self.buffer_y = [], []\n        self.x, self.y = x, y\n        self.label = label\n        self.init_net(bandit_params)\n        self.tp, self.tn, self.fp, self.fn = 0, 0, 0, 0\n\n    def get_agent_reward(self, eaten, edible):\n        if not eaten:\n            return 0\n        if eaten and edible:\n            return 5\n        elif eaten and not edible:\n            return 5 if np.random.rand() > 0.5 else -35\n\n    def get_oracle_reward(self, edible):\n        return 5*edible \n\n    def take_action(self, mushroom):\n        context, edible = self.x[mushroom], self.y[mushroom]\n        eat_tuple = torch.FloatTensor(np.concatenate((context, [1, 0]))).unsqueeze(0).to(device)\n        reject_tuple = torch.FloatTensor(np.concatenate((context, [0, 1]))).unsqueeze(0).to(device)\n\n        # evaluate reward for actions\n        with torch.no_grad():\n            self.net.eval()\n            reward_eat = sum([self.net(eat_tuple) for _ in range(self.n_samples)]).item()\n            reward_reject = sum([self.net(reject_tuple) for _ in range(self.n_samples)]).item()\n\n        eat = reward_eat > reward_reject\n        # epsilon-greedy agent\n        if np.random.rand() < self.epsilon:\n            eat = (np.random.rand() < 0.5)\n        agent_reward = self.get_agent_reward(eat, edible)\n\n        # record bandit action\n        if edible and eat:\n            self.tp += 1\n        elif edible and not eat:\n            self.fn += 1\n        elif not edible and eat:\n            self.fp += 1\n        else:\n            self.tn += 1\n\n        # record context, action, reward\n        action = torch.Tensor([1, 0] if eat else [0, 1])\n        self.buffer_x.append(np.concatenate((context, action)))\n        self.buffer_y.append(agent_reward)\n\n        # calculate regret\n        regret = self.get_oracle_reward(edible) - agent_reward\n        self.cumulative_regrets.append(self.cumulative_regrets[-1]+regret)\n\n    def update(self, mushroom):\n        self.take_action(mushroom)\n        l = len(self.buffer_x)\n\n        if l <= self.batch_size:\n            idx_pool = int(self.batch_size//l + 1)*list(range(l))\n            idx_pool = np.random.permutation(idx_pool[-self.batch_size:])\n        elif l > self.batch_size and l < self.buffer_size:\n            idx_pool = int(l//self.batch_size)*self.batch_size\n            idx_pool = np.random.permutation(list(range(l))[-idx_pool:])\n        else:\n            idx_pool = np.random.permutation(list(range(l))[-self.buffer_size:])\n\n        context_pool = torch.Tensor([self.buffer_x[i] for i in idx_pool]).to(device)\n        value_pool = torch.Tensor([self.buffer_y[i] for i in idx_pool]).to(device)\n        \n        for i in range(0, len(idx_pool), self.batch_size):\n            self.loss_info = self.loss_step(context_pool[i:i+self.batch_size], value_pool[i:i+self.batch_size], i//self.batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T19:04:41.161077Z","iopub.execute_input":"2025-06-07T19:04:41.161319Z","iopub.status.idle":"2025-06-07T19:04:41.190015Z","shell.execute_reply.started":"2025-06-07T19:04:41.161296Z","shell.execute_reply":"2025-06-07T19:04:41.189474Z"}},"outputs":[],"execution_count":166},{"cell_type":"code","source":"'''\nTwo Contextual bandits \n1) BNN and Thompson Sampling -> BNN_Bandit\n2) MLP and epsilon-greedy poligy -> Greedy_Bandit\nBoth derived from base_bandit.py class\n'''\nimport torch\nimport numpy as np\n\nclass BNN_Bandit(Bandit):\n    def __init__(self, label, *args):\n        super().__init__(label, *args)\n\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x.shape[1]+2,\n            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment'],\n            'mu_init': parameters['mu_init'],\n            'rho_init': parameters['rho_init'],\n            'prior_init': parameters['prior_init']\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(f'Bandit {self.label} Parameters: ')\n        print(f'buffer_size: {self.buffer_size}, batch size: {self.batch_size}, number of samples: {self.n_samples}, epsilon: {self.epsilon}')\n        print(\"BNN Parameters: \")\n        print(f'input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def loss_step(self, x, y, batch_id):\n        beta = 2 ** (self.num_batches - (batch_id + 1)) / (2 ** self.num_batches - 1) \n        self.net.train()\n        self.net.zero_grad()\n        loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)\n        net_loss = loss_info[0]\n        net_loss.backward()\n        self.optimiser.step()\n        return loss_info\n\n\nclass Greedy_Bandit(Bandit):\n    def __init__(self, label, *args):\n        super().__init__(label, *args)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x.shape[1]+2,\n            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment']\n        }\n        self.net = MLP(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(f'Bandit {self.label} Parameters: ')\n        print(f'buffer_size: {self.buffer_size}, batch size: {self.batch_size}, number of samples: {self.n_samples}, epsilon: {self.epsilon}')\n        print(\"MLP Parameters: \")\n        print(f'input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def loss_step(self, x, y, batch_id):\n        self.net.train()\n        self.net.zero_grad()\n        net_loss = torch.nn.functional.mse_loss(self.net(x).squeeze(), y, reduction='sum')\n        net_loss.backward()\n        self.optimiser.step()\n        return net_loss","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:41.190624Z","iopub.execute_input":"2025-06-07T19:04:41.190850Z","iopub.status.idle":"2025-06-07T19:04:41.212410Z","shell.execute_reply.started":"2025-06-07T19:04:41.190826Z","shell.execute_reply":"2025-06-07T19:04:41.211668Z"},"trusted":true},"outputs":[],"execution_count":167},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef rl_trainer():\n    config = RLConfig\n    X, Y = read_data_rl(config.data_dir)\n\n    params = {\n        'buffer_size': config.buffer_size,\n        'batch_size': config.batch_size,\n        'num_batches': config.num_batches,\n        'lr': config.lr,\n        'hidden_units': config.hidden_units,\n        'experiment': config.experiment,\n        'mu_init': config.mu_init,\n        'rho_init': config.rho_init,\n        'prior_init': config.prior_init\n    }\n\n    bnn_bandit = BNN_Bandit('bnn_bandit', {**params, 'n_samples':2, 'epsilon':0}, X, Y)\n    greedy_bandit = Greedy_Bandit('greedy_bandit', {**params, 'n_samples':1, 'epsilon':0}, X, Y)\n    greedy_bandit_001 = Greedy_Bandit('greedy_bandit_001', {**params, 'n_samples':1, 'epsilon':0.01}, X, Y)\n    greedy_bandit_005 = Greedy_Bandit('greedy_bandit_005', {**params, 'n_samples':1, 'epsilon':0.05}, X, Y)\n    \n    \n    training_steps = config.training_steps\n    print(f\"Initialising training on {device}...\")\n    training_data_len = len(X)\n    for step in tqdm(range(training_steps)):\n        mushroom = np.random.randint(training_data_len)\n        bnn_bandit.update(mushroom)\n        greedy_bandit.update(mushroom)\n        greedy_bandit_001.update(mushroom)\n        greedy_bandit_005.update(mushroom)\n        \n        bnn_bandit.scheduler.step()\n        greedy_bandit.scheduler.step()\n        greedy_bandit_001.scheduler.step()\n        greedy_bandit_005.scheduler.step()\n\n    # Plot cumulative regret\n    plt.figure(figsize=(10, 6))\n    plt.plot(bnn_bandit.cumulative_regrets, label='BNN Cumulative Regret')\n    plt.plot(greedy_bandit.cumulative_regrets, label='Greedy Cumulative Regret')\n    plt.plot(greedy_bandit_001.cumulative_regrets, label='Greedy 0.01 Cumulative Regret')\n    plt.plot(greedy_bandit_005.cumulative_regrets, label='Greedy 0.05 Cumulative Regret')\n    plt.xlabel('Steps')\n    plt.ylabel('Cumulative Regret')\n    plt.title('BNN vs Greedy Cumulative Regret over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\nrl_trainer()","metadata":{"execution":{"iopub.status.busy":"2025-06-07T19:04:41.213276Z","iopub.execute_input":"2025-06-07T19:04:41.213469Z","iopub.status.idle":"2025-06-07T19:04:54.335204Z","shell.execute_reply.started":"2025-06-07T19:04:41.213455Z","shell.execute_reply":"2025-06-07T19:04:54.334127Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Bandit bnn_bandit Parameters: \nbuffer_size: 4096, batch size: 64, number of samples: 2, epsilon: 0\nBNN Parameters: \ninput shape: 119, hidden units: 100, output shape: 1, lr: 0.0001\nBandit greedy_bandit Parameters: \nbuffer_size: 4096, batch size: 64, number of samples: 1, epsilon: 0\nMLP Parameters: \ninput shape: 119, hidden units: 100, output shape: 1, lr: 0.0001\nBandit greedy_bandit_001 Parameters: \nbuffer_size: 4096, batch size: 64, number of samples: 1, epsilon: 0.01\nMLP Parameters: \ninput shape: 119, hidden units: 100, output shape: 1, lr: 0.0001\nBandit greedy_bandit_005 Parameters: \nbuffer_size: 4096, batch size: 64, number of samples: 1, epsilon: 0.05\nMLP Parameters: \ninput shape: 119, hidden units: 100, output shape: 1, lr: 0.0001\nInitialising training on cuda...\n","output_type":"stream"},{"name":"stderr","text":" 31%|ââââ      | 313/1000 [00:12<00:28, 24.11it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1476279589.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mrl_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/1476279589.py\u001b[0m in \u001b[0;36mrl_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmushroom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mbnn_bandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mgreedy_bandit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mgreedy_bandit_001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmushroom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2419875905.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, mushroom)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/3646314314.py\u001b[0m in \u001b[0;36mloss_step\u001b[0;34m(self, x, y, batch_id)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mnet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2189555317.py\u001b[0m in \u001b[0;36msample_elbo\u001b[0;34m(self, x, target, beta, samples, sigma)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mlog_prior\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_variational_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2189555317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2189555317.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBayesianNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":168}]}