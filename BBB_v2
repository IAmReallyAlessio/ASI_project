{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12061096,"sourceType":"datasetVersion","datasetId":7591461}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport math\nfrom torch import nn\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:50:33.731071Z","iopub.execute_input":"2025-06-05T09:50:33.731421Z","iopub.status.idle":"2025-06-05T09:50:33.736256Z","shell.execute_reply.started":"2025-06-05T09:50:33.731394Z","shell.execute_reply":"2025-06-05T09:50:33.735584Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"# define class for scale mixture gaussian prior\nclass ScaleMixtureGaussian:                               \n    def __init__(self, mixture_weight, stddev_1, stddev_2):\n        super().__init__()\n        # mixture_weight is the weight for the first gaussian\n        self.mixture_weight = mixture_weight\n        # stddev_1 and stddev_2 are the standard deviations for the two gaussians\n        self.stddev_1 = stddev_1\n        self.stddev_2 = stddev_2\n        # create two normal distributions with the specified standard deviations\n        self.gaussian1 = torch.distributions.Normal(0,stddev_1)\n        self.gaussian2 = torch.distributions.Normal(0,stddev_2)\n\n\n    def log_prob(self, x):\n        prob1 = torch.exp(self.gaussian1.log_prob(x))\n        prob2 = torch.exp(self.gaussian2.log_prob(x))\n        return (torch.log(self.mixture_weight * prob1 + (1-self.mixture_weight) * prob2)).sum()\n    \n# define class for gaussian node\nclass GaussianNode:\n    def __init__(self, mean, rho_param):\n        super().__init__()\n        self.mean = mean\n        self.rho_param = rho_param\n        self.normal = torch.distributions.Normal(0,1)\n    \n    # Calculate the standard deviation from the rho parameter\n    def sigma(self):\n        return torch.log1p(torch.exp(self.rho_param))\n\n    # Sample from the Gaussian node\n    def sample(self):\n        epsilon = self.normal.sample(self.rho_param.size()).cuda()\n        return self.mean + self.sigma() * epsilon\n    \n    # Calculate the KL divergence between the prior and the variational posterior\n    def log_prob(self, x):\n        return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma()) - ((x - self.mean) ** 2) / (2 * self.sigma() ** 2)).sum()\n\nclass BayesianLinear(nn.Module):\n    def __init__(self, in_features, out_features, mu_init, rho_init, prior_init):\n        super().__init__()\n\n        # Initialize the parameters for the weights and biases\n        self.weight_mean = nn.Parameter(torch.empty(out_features, in_features).uniform_(*mu_init))\n        self.weight_rho_param = nn.Parameter(torch.empty(out_features, in_features).uniform_(*rho_init))\n        self.weight = GaussianNode(self.weight_mean, self.weight_rho_param)\n\n        self.bias_mean = nn.Parameter(torch.empty(out_features).uniform_(*mu_init))\n        self.bias_rho_param = nn.Parameter(torch.empty(out_features).uniform_(*rho_init))\n        self.bias = GaussianNode(self.bias_mean, self.bias_rho_param)\n        \n        self.weight_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n        self.bias_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n\n        self.log_prior = 0\n        self.log_variational_posterior = 0\n\n    def forward(self, x):\n        weight = self.weight.sample()\n        bias = self.bias.sample()\n\n        return nn.functional.linear(x, weight, bias)\n\nclass BayesianNetwork(nn.Module):\n    def __init__(self, model_params):\n        super().__init__()\n        self.input_shape = model_params['input_shape']\n        self.classes = model_params['classes']\n        self.batch_size = model_params['batch_size']\n        self.hidden_units = model_params['hidden_units']\n        self.experiment = model_params['experiment']\n        self.mu_init = model_params['mu_init']\n        self.rho_init = model_params['rho_init']\n        self.prior_init = model_params['prior_init']\n\n        self.fc1 = BayesianLinear(self.input_shape, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n        self.fc1_activation = nn.ReLU()\n        self.fc2 = BayesianLinear(self.hidden_units, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n        self.fc2_activation = nn.ReLU()\n        self.fc3 = BayesianLinear(self.hidden_units, self.classes, self.mu_init, self.rho_init, self.prior_init)\n    \n    def forward(self, x):\n        if self.experiment == 'classification':\n            x = x.view(-1, self.input_shape) # Flatten images\n        x = self.fc1_activation(self.fc1(x))\n        x = self.fc2_activation(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def log_prior(self):\n        return self.fc1.log_prior + self.fc2.log_prior + self.fc3.log_prior\n    \n    def log_variational_posterior(self):\n        return self.fc1.log_variational_posterior + self.fc2.log_variational_posterior + self.fc3.log_variational_posterior\n\n\n    def get_nll(self, outputs, target, sigma=1.):\n        if self.experiment == 'regression': #  -(.5 * (target - outputs) ** 2).sum()\n            nll = -torch.distributions.Normal(outputs, sigma).log_prob(target).sum()\n        elif self.experiment == 'classification':\n            nll = nn.CrossEntropyLoss(reduction='sum')(outputs, target)\n        return nll\n\n    def sample_elbo(self, x, target, beta, samples, sigma=1.):\n        log_prior = torch.zeros(1).to(device)\n        log_variational_posterior = torch.zeros(1).to(device)\n        negative_log_likelihood = torch.zeros(1).to(device)\n\n        for i in range(samples):\n            output = self.forward(x)\n            log_prior += self.log_prior()\n            log_variational_posterior += self.log_variational_posterior()\n            negative_log_likelihood += self.get_nll(output, target, sigma)\n\n        log_prior = beta*(log_prior / samples)\n        log_variational_posterior = beta*(log_variational_posterior / samples) \n        negative_log_likelihood = negative_log_likelihood / samples\n        loss = log_variational_posterior - log_prior + negative_log_likelihood\n        return loss, log_prior, log_variational_posterior, negative_log_likelihood    \n","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.739245Z","iopub.execute_input":"2025-06-05T09:50:33.739515Z","iopub.status.idle":"2025-06-05T09:50:33.761134Z","shell.execute_reply.started":"2025-06-05T09:50:33.739499Z","shell.execute_reply":"2025-06-05T09:50:33.760656Z"},"trusted":true},"outputs":[],"execution_count":133},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, model_params):\n        super().__init__()\n        self.input_shape = model_params['input_shape']\n        self.classes = model_params['classes']\n        self.batch_size = model_params['batch_size']\n        self.hidden_units = model_params['hidden_units']\n        self.experiment = model_params['experiment']\n\n        self.net = nn.Sequential(\n            nn.Linear(self.input_shape, self.hidden_units),\n            nn.ReLU(),\n            nn.Linear(self.hidden_units, self.hidden_units),\n            nn.ReLU(),\n            nn.Linear(self.hidden_units, self.classes))\n    \n    def forward(self, x):\n        if self.experiment == 'classification':\n            x = x.view(-1, self.input_shape) # Flatten images\n        \n        x = self.net(x)\n        return x\n\n# class MLP_Dropout(nn.Module):\n#     def __init__(self, model_params):\n#         super().__init__()\n#         self.input_shape = model_params['input_shape']\n#         self.classes = model_params['classes']\n#         self.batch_size = model_params['batch_size']\n#         self.hidden_units = model_params['hidden_units']\n#         self.experiment = model_params['experiment']\n\n#         self.net = nn.Sequential(\n#             nn.Linear(self.input_shape, self.hidden_units),\n#             nn.ReLU(),\n#             nn.Dropout(0.5),\n#             nn.Linear(self.hidden_units, self.hidden_units),\n#             nn.ReLU(),\n#             nn.Dropout(0.5),\n#             nn.Linear(self.hidden_units, self.classes))\n    \n#     def forward(self, x):\n#         if self.experiment == 'classification':\n#             x = x.view(-1, self.input_shape) # Flatten images\n       \n#         x = self.net(x)\n#         return x\n\n#     def enable_dropout(self):\n#         ''' Enable the dropout layers during test-time '''\n#         for m in self.modules():\n#             if m.__class__.__name__.startswith('Dropout'):\n#                 m.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:50:33.761914Z","iopub.execute_input":"2025-06-05T09:50:33.762133Z","iopub.status.idle":"2025-06-05T09:50:33.785065Z","shell.execute_reply.started":"2025-06-05T09:50:33.762111Z","shell.execute_reply":"2025-06-05T09:50:33.784494Z"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":"class RegConfig:\n    # save_dir = './saved_models'\n    train_size = 1024\n    batch_size = 128\n    lr = 1e-3\n    epochs = 100 #1000\n    train_samples = 5                   # number of train samples for MC gradients\n    test_samples = 10                   # number of test samples for MC averaging\n    num_test_points = 400               # number of test points\n    experiment = 'regression'\n    hidden_units = 400                  # number of hidden units\n    noise_tolerance = .1                # log likelihood sigma\n    mu_init = [-0.2, 0.2]               # range for mean \n    rho_init = [-5, -4]                 # range for rho_param\n    prior_init = [0.5, -0, -6]        # mixture weight, log(stddev_1), log(stddev_2)\n   \n\nclass RLConfig:\n    data_dir = '/kaggle/input/mushroom/agaricus-lepiota.data' \n    batch_size = 64\n    num_batches = 64\n    buffer_size = batch_size * num_batches  # buffer to track latest batch of mushrooms\n    lr = 1e-4\n    training_steps = 5000 # 50000\n    experiment = 'regression'\n    hidden_units = 100                      # number of hidden units\n    mu_init = [-0.2, 0.2]                   # range for mean \n    rho_init = [-5, -4]                     # range for rho_param\n    prior_init = [0.5, -0, -6]              # mixture weight, log(stddev_1), log(stddev_2)\n\nclass ClassConfig:\n    batch_size = 128\n    lr = 1e-4\n    epochs = 600\n    hidden_units = 1200\n    experiment = 'classification'\n    dropout = False\n    train_samples = 2\n    test_samples = 10\n    x_shape = 28 * 28                       # x shape\n    classes = 10                            # number of output classes\n    mu_init = [-0.2, 0.2]                   # range for mean \n    rho_init = [-5, -4]                     # range for rho_param\n    prior_init = [0.5, -0, -8]             # mixture weight, log(stddev_1), log(stddev_2)","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.839815Z","iopub.execute_input":"2025-06-05T09:50:33.840012Z","iopub.status.idle":"2025-06-05T09:50:33.846541Z","shell.execute_reply.started":"2025-06-05T09:50:33.839998Z","shell.execute_reply":"2025-06-05T09:50:33.845933Z"},"trusted":true},"outputs":[],"execution_count":135},{"cell_type":"code","source":"class PrepareData(Dataset):\n    def __init__(self, X, y):\n        if not torch.is_tensor(X):\n            self.X = torch.from_numpy(X)\n        else:\n            self.X = X\n        if not torch.is_tensor(y):\n            self.y = torch.from_numpy(y)\n        else:\n            self.y = y # vedere\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndef read_data_rl(data_dir):\n    df = pd.read_csv(data_dir, sep=',', header=None)\n    df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n    X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n    Y = df['class']\n\n    # transform to one-hot encoding\n    label_encoder = preprocessing.LabelEncoder()\n    label_encoder.fit(Y)\n    Y_encoded = label_encoder.transform(Y)\n    oh_X = X.copy()\n    for feature in X.columns:\n        label_encoder.fit(X[feature])\n        oh_X[feature] = label_encoder.transform(X[feature])\n\n    oh_encoder = preprocessing.OneHotEncoder()\n    oh_encoder.fit(oh_X)\n    oh_X = oh_encoder.transform(oh_X).toarray()\n\n    return oh_X, Y_encoded\n\ndef create_data_reg(train_size):\n    np.random.seed(0)\n    xs = np.random.uniform(low=0., high=0.6, size=train_size)\n    \n    eps = np.random.normal(loc=0., scale=0.02, size=[train_size])\n\n    ys = xs + 0.3 * np.sin(2*np.pi * (xs + eps)) + 0.3 * np.sin(4*np.pi * (xs + eps)) + eps\n\n    xs = torch.from_numpy(xs).reshape(-1,1).float()\n    ys = torch.from_numpy(ys).reshape(-1,1).float()\n\n    return xs, ys","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.847494Z","iopub.execute_input":"2025-06-05T09:50:33.847727Z","iopub.status.idle":"2025-06-05T09:50:33.868902Z","shell.execute_reply.started":"2025-06-05T09:50:33.847707Z","shell.execute_reply":"2025-06-05T09:50:33.868195Z"},"trusted":true},"outputs":[],"execution_count":136},{"cell_type":"code","source":"# def load_bnn_class_model(saved_model):\n#     config = ClassConfig\n\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'mu_init': config.mu_init,\n#         'rho_init': config.rho_init,\n#         'prior_init': config.prior_init\n#     }\n#     model = BayesianNetwork(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()\n\n# def load_mlp_class_model(saved_model):\n#     config = ClassConfig\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#     }\n#     model = MLP(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()\n\n# def load_dropout_class_model(saved_model):\n#     config = ClassConfig\n#     model_params = {\n#         'input_shape': config.x_shape,\n#         'classes': config.classes,\n#         'batch_size': config.batch_size,\n#         'hidden_units': config.hidden_units,\n#         'experiment': config.experiment,\n#         'dropout': True\n#     }\n#     model = MLP_Dropout(model_params)\n#     model.load_state_dict(torch.load(saved_model))\n\n#     return model.eval()","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.869661Z","iopub.execute_input":"2025-06-05T09:50:33.869845Z","iopub.status.idle":"2025-06-05T09:50:33.885991Z","shell.execute_reply.started":"2025-06-05T09:50:33.869834Z","shell.execute_reply":"2025-06-05T09:50:33.885315Z"},"trusted":true},"outputs":[],"execution_count":137},{"cell_type":"code","source":"def create_regression_plot(X_test, y_test, train_ds):\n    fig = plt.figure(figsize=(9, 6))\n    plt.plot(X_test, np.median(y_test, axis=0), label='Median Posterior Predictive')\n    \n    # Range\n    plt.fill_between(\n        X_test.reshape(-1), \n        np.percentile(y_test, 0, axis=0), \n        np.percentile(y_test, 100, axis=0), \n        alpha = 0.2, color='orange', label='Range') #color='blue',\n    \n    # interquartile range\n    plt.fill_between(\n        X_test.reshape(-1), \n        np.percentile(y_test, 25, axis=0), \n        np.percentile(y_test, 75, axis=0), \n        alpha = 0.4,  label='Interquartile Range') #color='red',\n    \n    plt.scatter(train_ds.dataset.X, train_ds.dataset.y, label='Training data', marker='x', alpha=0.5, color='k', s=2)\n    plt.yticks(fontsize=20)\n    plt.xticks(fontsize=20)\n    plt.ylim([-1.5, 1.5])\n    plt.xlim([-0.6, 1.4])\n\n   ","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.887685Z","iopub.execute_input":"2025-06-05T09:50:33.887866Z","iopub.status.idle":"2025-06-05T09:50:33.905812Z","shell.execute_reply.started":"2025-06-05T09:50:33.887853Z","shell.execute_reply":"2025-06-05T09:50:33.905279Z"},"trusted":true},"outputs":[],"execution_count":138},{"cell_type":"code","source":"class BNN_Classification():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.n_samples = parameters['train_samples']\n        self.test_samples = parameters['test_samples']\n        self.x_shape = parameters['x_shape']\n        self.classes = parameters['classes']\n        self.mu_init = parameters['mu_init']\n        self.rho_init = parameters['rho_init']\n        self.prior_init = parameters['prior_init']\n        self.best_acc = 0.\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.classes,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment,\n            'mu_init': self.mu_init,\n            'rho_init': self.rho_init,\n            'prior_init': self.prior_init,\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n        # print(f'Classification Task {self.label} Parameters: ')\n        # print(f'number of samples: {self.n_samples}')\n        # print(\"BNN Parameters: \")\n        # print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for idx, (x, y) in enumerate(tqdm(train_data)):\n            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)            \n            net_loss = self.loss_info[0]\n            net_loss.backward()\n            self.optimiser.step()\n\n    def predict(self, X):\n        probs = torch.zeros(size=[self.batch_size, self.classes]).to(device)\n        for _ in torch.arange(self.test_samples):\n            out = torch.nn.Softmax(dim=1)(self.net(X))\n            probs = probs + out / self.test_samples\n        preds = torch.argmax(probs, dim=1)\n        return preds, probs\n\n    def evaluate(self, test_loader):\n        self.net.eval()\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for data in tqdm(test_loader):\n                X, y = data\n                X, y = X.to(device), y.to(device)\n                preds, _ = self.predict(X)\n                total += self.batch_size\n                correct += (preds == y).sum().item()\n        self.acc = correct / total\n        print(f'validation accuracy: {self.acc}')  \n        return self.acc","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-05T09:50:33.906493Z","iopub.execute_input":"2025-06-05T09:50:33.906648Z","iopub.status.idle":"2025-06-05T09:50:33.925192Z","shell.execute_reply.started":"2025-06-05T09:50:33.906636Z","shell.execute_reply":"2025-06-05T09:50:33.924579Z"},"trusted":true},"outputs":[],"execution_count":139},{"cell_type":"code","source":"class MLP_Classification():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.x_shape = parameters['x_shape']\n        self.classes = parameters['classes']\n        self.best_acc = 0.\n        self.dropout = parameters['dropout']\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.classes,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment,\n            'dropout': self.dropout,\n        }\n        if self.dropout:\n            self.net = MLP_Dropout(model_params).to(device)\n            print('MLP Dropout Parameters: ')\n            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n        else:\n            self.net = MLP(model_params).to(device)\n            print('MLP Parameters: ')\n            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n        self.optimiser = torch.optim.SGD(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n\n    def train_step(self, train_data):\n        self.net.train()\n        for _, (x, y) in enumerate(tqdm(train_data)):\n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = torch.nn.functional.cross_entropy(self.net(x), y, reduction='sum')\n            self.loss_info.backward()\n            self.optimiser.step()\n\n    def predict(self, X):\n        probs = torch.nn.Softmax(dim=1)(self.net(X))\n        preds = torch.argmax(probs, dim=1)\n        return preds, probs\n\n    def evaluate(self, test_loader):\n        self.net.eval()\n        print('Evaluating on validation data')\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for data in tqdm(test_loader):\n                X, y = data\n                X, y = X.to(device), y.to(device)\n                preds, _ = self.predict(X)\n                total += self.batch_size\n                correct += (preds == y).sum().item()\n        self.acc = correct / total\n        print(f'{self.label} validation accuracy: {self.acc}') \n        return self.acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:50:33.925859Z","iopub.execute_input":"2025-06-05T09:50:33.926070Z","iopub.status.idle":"2025-06-05T09:50:33.946667Z","shell.execute_reply.started":"2025-06-05T09:50:33.926053Z","shell.execute_reply":"2025-06-05T09:50:33.946060Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"def class_trainer():\n    config = ClassConfig\n    \n    transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x * 255 / 126.),  # divide as in paper, * 255 gives better results\n        ])\n\n    train_data = datasets.MNIST(\n            root='data',\n            train=True,\n            download=True,\n            transform=transform)\n    test_data = datasets.MNIST(\n            root='data',\n            train=False,\n            download=True,\n            transform=transform)\n\n    valid_size = 1 / 6\n\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split = int(valid_size * num_train)\n    train_idx, valid_idx = indices[split:], indices[:split]\n\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n\n    train_loader = torch.utils.data.DataLoader(\n            train_data,\n            batch_size=config.batch_size,\n            sampler=train_sampler,\n            drop_last=True)\n    valid_loader = torch.utils.data.DataLoader(\n            train_data,\n            batch_size=config.batch_size,\n            sampler=valid_sampler,\n            drop_last=True)\n    test_loader = torch.utils.data.DataLoader(\n            test_data,\n            batch_size=config.batch_size,\n            shuffle=False,\n            drop_last=True)\n\n    params = {\n        'lr': config.lr,\n        'hidden_units': config.hidden_units,\n        'experiment': config.experiment,\n        'dropout': config.dropout,\n        'batch_size': config.batch_size,\n        'epochs': config.epochs,\n        'x_shape': config.x_shape,\n        'classes': config.classes,\n        'num_batches': len(train_loader),\n        'train_samples': config.train_samples,\n        'test_samples': config.test_samples,\n        'mu_init': config.mu_init,\n        'rho_init': config.rho_init,\n        'prior_init': config.prior_init,\n    }\n\n    model = BNN_Classification('bnn_classification', {**params})\n    #model = MLP_Classification('mlp_classification', {**params})\n    \n    epochs = config.epochs\n    for epoch in range(epochs):\n            print(f'Epoch {epoch+1}/{epochs}')\n            model.train_step(train_loader)\n            valid_acc = model.evaluate(valid_loader)\n            # model.evaluate(test_loader)\n            print('Valid Error', round(100 * (1 - valid_acc), 3), '%',)\n            # model.log_progress(epoch)\n            model.scheduler.step()\n            if model.acc > model.best_acc:\n                model.best_acc = model.acc\n                # torch.save(model.net.state_dict(), model.save_model_path)\n\n\n\nclass_trainer()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:50:33.947296Z","iopub.execute_input":"2025-06-05T09:50:33.947512Z","iopub.status.idle":"2025-06-05T09:57:28.171266Z","shell.execute_reply.started":"2025-06-05T09:50:33.947494Z","shell.execute_reply":"2025-06-05T09:57:28.170058Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:23<00:00, 16.86it/s]\n100%|██████████| 78/78 [00:17<00:00,  4.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9121594551282052\nValid Error 8.784 %\nEpoch 2/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:23<00:00, 16.63it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9363982371794872\nValid Error 6.36 %\nEpoch 3/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.01it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9447115384615384\nValid Error 5.529 %\nEpoch 4/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.35it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9512219551282052\nValid Error 4.878 %\nEpoch 5/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.05it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9572315705128205\nValid Error 4.277 %\nEpoch 6/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.00it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9618389423076923\nValid Error 3.816 %\nEpoch 7/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.00it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.961738782051282\nValid Error 3.826 %\nEpoch 8/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 17.14it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9637419871794872\nValid Error 3.626 %\nEpoch 9/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:23<00:00, 16.75it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9639423076923077\nValid Error 3.606 %\nEpoch 10/600\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 390/390 [00:22<00:00, 16.97it/s]\n100%|██████████| 78/78 [00:16<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"validation accuracy: 0.9649439102564102\nValid Error 3.506 %\nEpoch 11/600\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 304/390 [00:17<00:04, 17.28it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2378533532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mclass_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2378533532.py\u001b[0m in \u001b[0;36mclass_trainer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# model.evaluate(test_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2079799134.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2189555317.py\u001b[0m in \u001b[0;36msample_elbo\u001b[0;34m(self, x, target, beta, samples, sigma)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mlog_variational_posterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mnegative_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":141},{"cell_type":"code","source":"# from tqdm import tqdm\n# import numpy as np\n# import torch\n# from torch.utils.data import SubsetRandomSampler\n# from torchvision import datasets, transforms\n# from itertools import product\n\n# def class_trainer(config):\n\n#     transform = transforms.Compose([\n#         transforms.ToTensor(),\n#         transforms.Lambda(lambda x: x * 255. / 126.),\n#     ])\n\n#     train_data = datasets.MNIST(\n#         root='data',\n#         train=True,\n#         download=True,\n#         transform=transform)\n#     test_data = datasets.MNIST(\n#         root='data',\n#         train=False,\n#         download=True,\n#         transform=transform)\n\n#     valid_size = 1 / 6\n#     num_train = len(train_data)\n#     indices = list(range(num_train))\n#     split = int(valid_size * num_train)\n#     train_idx, valid_idx = indices[split:], indices[:split]\n\n#     train_sampler = SubsetRandomSampler(train_idx)\n#     valid_sampler = SubsetRandomSampler(valid_idx)\n\n#     train_loader = torch.utils.data.DataLoader(\n#         train_data,\n#         batch_size=config['batch_size'],\n#         sampler=train_sampler,\n#         drop_last=True)\n#     valid_loader = torch.utils.data.DataLoader(\n#         train_data,\n#         batch_size=config['batch_size'],\n#         sampler=valid_sampler,\n#         drop_last=True)\n#     test_loader = torch.utils.data.DataLoader(\n#         test_data,\n#         batch_size=config['batch_size'],\n#         shuffle=False,\n#         drop_last=True)\n\n#     params = {\n#         'lr': config['lr'],\n#         'hidden_units': config['hidden_units'],\n#         'experiment': config['experiment'],\n#         'batch_size': config['batch_size'],\n#         'epochs': config['epochs'],\n#         'x_shape': config['x_shape'],\n#         'classes': config['classes'],\n#         'num_batches': len(train_loader),\n#         'train_samples': config['train_samples'],\n#         'test_samples': config['test_samples'],\n#         'mu_init': config['mu_init'],\n#         'rho_init': config['rho_init'],\n#         'prior_init': config['prior_init'],\n#     }\n\n#     model = BNN_Classification('bnn_classification', {**params, 'dropout': False})\n\n#     best_val_acc = 0\n#     for epoch in range(config['epochs']):\n#         print(f'Epoch {epoch + 1}/{config[\"epochs\"]}')\n#         model.train_step(train_loader)\n#         valid_acc = model.evaluate(valid_loader)\n#         print('Valid Error', round(100 * (1 - valid_acc), 3), '%')\n#         model.scheduler.step()\n#         if model.acc > model.best_acc:\n#             model.best_acc = model.acc\n#             best_val_acc = valid_acc\n#             # torch.save(model.net.state_dict(), \"best_model.pt\")\n\n#     return best_val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:57:28.172012Z","iopub.status.idle":"2025-06-05T09:57:28.172305Z","shell.execute_reply.started":"2025-06-05T09:57:28.172141Z","shell.execute_reply":"2025-06-05T09:57:28.172157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def run_grid_search():\n#     # Define the grid\n#     param_grid = {\n#         'lr': [1e-3, 1e-4],\n#         'batch_size': [64, 128],\n#         'hidden_units': [128, 256],\n#     }\n\n#     # Fixed config options\n#     base_config = {\n#         'experiment': 'grid_search',\n#         'epochs': 5,\n#         'x_shape': (1, 28, 28),\n#         'classes': 10,\n#         'train_samples': 60000,\n#         'test_samples': 10000,\n#         'mu_init': 0,\n#         'rho_init': -3,\n#         'prior_init': 0,\n#     }\n\n#     best_acc = 0\n#     best_params = None\n\n#     # Iterate over all combinations\n#     for lr, batch_size, hidden_units in product(param_grid['lr'], param_grid['batch_size'], param_grid['hidden_units']):\n#         config = {\n#             **base_config,\n#             'lr': lr,\n#             'batch_size': batch_size,\n#             'hidden_units': hidden_units\n#         }\n\n#         print(f\"\\nRunning with config: {config}\")\n#         acc = class_trainer(config)\n\n#         if acc > best_acc:\n#             best_acc = acc\n#             best_params = config\n\n#     print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n#     print(f\"Best Config: {best_params}\")\n\n# run_grid_search()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:57:28.173336Z","iopub.status.idle":"2025-06-05T09:57:28.173661Z","shell.execute_reply.started":"2025-06-05T09:57:28.173503Z","shell.execute_reply":"2025-06-05T09:57:28.173517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BNN_Regression():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.n_samples = parameters['train_samples']\n        self.test_samples = parameters['test_samples']\n        self.x_shape = parameters['x_shape']\n        self.y_shape = parameters['y_shape']\n        self.noise_tol = parameters['noise_tolerance']\n        self.lr = parameters['lr']\n        self.best_loss = np.inf\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.y_shape,\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment'],\n            'mu_init': parameters['mu_init'],\n            'rho_init': parameters['rho_init'],\n            'prior_init': parameters['prior_init']\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=500, gamma=0.5)\n        # print(f'Regression Task {self.label} Parameters: ')\n        # print(f'number of samples: {self.n_samples}, noise tolerance: {self.noise_tol}')\n        print(\"BNN Parameters: \")\n        print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, mu_init: {parameters[\"mu_init\"]}, rho_init: {parameters[\"rho_init\"]}, prior_init: {parameters[\"prior_init\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for idx, (x, y) in enumerate(train_data):\n            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples, sigma=self.noise_tol)\n            net_loss = self.loss_info[0]\n            net_loss.backward()\n            self.optimiser.step()\n        self.epoch_loss = net_loss.item()\n\n    def evaluate(self, x_test):\n        self.net.eval()\n        with torch.no_grad():\n            y_test = np.zeros((self.test_samples, x_test.shape[0]))\n            for s in range(self.test_samples):\n                tmp = self.net(x_test.to(device)).detach().cpu().numpy()\n                y_test[s,:] = tmp.reshape(-1)\n            return y_test","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:57:28.174749Z","iopub.status.idle":"2025-06-05T09:57:28.175063Z","shell.execute_reply.started":"2025-06-05T09:57:28.174887Z","shell.execute_reply":"2025-06-05T09:57:28.174902Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP_Regression():\n    def __init__(self, label, parameters):\n        super().__init__()\n        self.label = label\n        self.lr = parameters['lr']\n        self.hidden_units = parameters['hidden_units']\n        self.experiment = parameters['experiment']\n        self.batch_size = parameters['batch_size']\n        self.num_batches = parameters['num_batches']\n        self.x_shape = parameters['x_shape']\n        self.y_shape = parameters['y_shape']\n        self.best_loss = np.inf\n        self.init_net(parameters)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x_shape,\n            'classes': self.y_shape,\n            'batch_size': self.batch_size,\n            'hidden_units': self.hidden_units,\n            'experiment': self.experiment\n        }\n        self.net = MLP(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(\"MLP Parameters: \")\n        print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def train_step(self, train_data):\n        self.net.train()\n        for _, (x, y) in enumerate(train_data):\n            x, y = x.to(device), y.to(device)\n            self.net.zero_grad()\n            self.loss_info = torch.nn.functional.mse_loss(self.net(x), y, reduction='sum')\n            self.loss_info.backward()\n            self.optimiser.step()\n\n        self.epoch_loss = self.loss_info.item()\n\n    def evaluate(self, x_test):\n        self.net.eval()\n        with torch.no_grad():\n            y_test = self.net(x_test.to(device)).detach().cpu().numpy()\n            return y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:57:28.176661Z","iopub.status.idle":"2025-06-05T09:57:28.176900Z","shell.execute_reply.started":"2025-06-05T09:57:28.176793Z","shell.execute_reply":"2025-06-05T09:57:28.176803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def reg_trainer():\n    config = RegConfig\n    X, Y = create_data_reg(train_size=config.train_size)\n    train_loader = PrepareData(X, Y)\n    train_loader = DataLoader(train_loader, batch_size=config.batch_size, shuffle=True)\n\n    params = {\n        'lr': config.lr,\n        'hidden_units': config.hidden_units,\n        'experiment': config.experiment,\n        'batch_size': config.batch_size,\n        'num_batches': len(train_loader),\n        'x_shape': X.shape[1],\n        'y_shape': Y.shape[1],\n        'train_samples': config.train_samples,\n        'test_samples': config.test_samples,\n        'noise_tolerance': config.noise_tolerance,\n        'mu_init': config.mu_init,\n        'rho_init': config.rho_init,\n        'prior_init': config.prior_init,\n    }\n\n    model = BNN_Regression('bnn_regression', {**params})\n    #model = MLP_Regression('mlp_regression', {**params})\n\n    epochs = config.epochs\n    print(f\"Initialising training on {device}...\")\n\n    # training loop\n    for epoch in tqdm(range(epochs)):\n    \n        model.train_step(train_loader)\n        model.scheduler.step()\n        # save best model\n        if model.epoch_loss < model.best_loss:\n            model.best_loss = model.epoch_loss\n            # torch.save(model.net.state_dict(), model.save_model_path)\n\n    # evaluate\n    print(\"Evaluating and generating plots...\")\n    x_test = torch.linspace(-2., 2, config.num_test_points).reshape(-1, 1)\n    \n    # model.net.load_state_dict(torch.load(model.save_model_path, map_location=torch.device(device)))\n    y_test = model.evaluate(x_test)\n    \n    #create_regression_plot(x_test.cpu().numpy(), y_test.reshape(1, -1), train_loader) #per mlp regression\n   \n    create_regression_plot(x_test.cpu().numpy(), y_test, train_loader)\n\n\n#reg_trainer()","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:57:28.177867Z","iopub.status.idle":"2025-06-05T09:57:28.178147Z","shell.execute_reply.started":"2025-06-05T09:57:28.178029Z","shell.execute_reply":"2025-06-05T09:57:28.178042Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Bandit():\n    def __init__(self, label, bandit_params, x, y):\n        self.n_samples = bandit_params['n_samples']\n        self.buffer_size = bandit_params['buffer_size']\n        self.batch_size = bandit_params['batch_size']\n        self.num_batches = bandit_params['num_batches']\n        self.lr = bandit_params['lr']\n        self.epsilon = bandit_params['epsilon']\n        self.cumulative_regrets = [0]\n        self.buffer_x, self.buffer_y = [], []\n        self.x, self.y = x, y\n        self.label = label\n        self.init_net(bandit_params)\n        self.tp, self.tn, self.fp, self.fn = 0, 0, 0, 0\n\n    def get_agent_reward(self, eaten, edible):\n        if not eaten:\n            return 0\n        if eaten and edible:\n            return 5\n        elif eaten and not edible:\n            return 5 if np.random.rand() > 0.5 else -35\n\n    def get_oracle_reward(self, edible):\n        return 5*edible \n\n    def take_action(self, mushroom):\n        context, edible = self.x[mushroom], self.y[mushroom]\n        eat_tuple = torch.FloatTensor(np.concatenate((context, [1, 0]))).unsqueeze(0).to(device)\n        reject_tuple = torch.FloatTensor(np.concatenate((context, [0, 1]))).unsqueeze(0).to(device)\n\n        # evaluate reward for actions\n        with torch.no_grad():\n            self.net.eval()\n            reward_eat = sum([self.net(eat_tuple) for _ in range(self.n_samples)]).item()\n            reward_reject = sum([self.net(reject_tuple) for _ in range(self.n_samples)]).item()\n\n        eat = reward_eat > reward_reject\n        # epsilon-greedy agent\n        if np.random.rand() < self.epsilon:\n            eat = (np.random.rand() < 0.5)\n        agent_reward = self.get_agent_reward(eat, edible)\n\n        # record bandit action\n        if edible and eat:\n            self.tp += 1\n        elif edible and not eat:\n            self.fn += 1\n        elif not edible and eat:\n            self.fp += 1\n        else:\n            self.tn += 1\n\n        # record context, action, reward\n        action = torch.Tensor([1, 0] if eat else [0, 1])\n        self.buffer_x.append(np.concatenate((context, action)))\n        self.buffer_y.append(agent_reward)\n\n        # calculate regret\n        regret = self.get_oracle_reward(edible) - agent_reward\n        self.cumulative_regrets.append(self.cumulative_regrets[-1]+regret)\n\n    def update(self, mushroom):\n        self.take_action(mushroom)\n        l = len(self.buffer_x)\n\n        if l <= self.batch_size:\n            idx_pool = int(self.batch_size//l + 1)*list(range(l))\n            idx_pool = np.random.permutation(idx_pool[-self.batch_size:])\n        elif l > self.batch_size and l < self.buffer_size:\n            idx_pool = int(l//self.batch_size)*self.batch_size\n            idx_pool = np.random.permutation(list(range(l))[-idx_pool:])\n        else:\n            idx_pool = np.random.permutation(list(range(l))[-self.buffer_size:])\n\n        context_pool = torch.Tensor([self.buffer_x[i] for i in idx_pool]).to(device)\n        value_pool = torch.Tensor([self.buffer_y[i] for i in idx_pool]).to(device)\n        \n        for i in range(0, len(idx_pool), self.batch_size):\n            self.loss_info = self.loss_step(context_pool[i:i+self.batch_size], value_pool[i:i+self.batch_size], i//self.batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T09:57:28.179101Z","iopub.status.idle":"2025-06-05T09:57:28.179400Z","shell.execute_reply.started":"2025-06-05T09:57:28.179263Z","shell.execute_reply":"2025-06-05T09:57:28.179275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BNN_Bandit(Bandit):\n    def __init__(self, label, *args):\n        super().__init__(label, *args)\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x.shape[1]+2,\n            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment'],\n            'mu_init': parameters['mu_init'],\n            'rho_init': parameters['rho_init'],\n            'prior_init': parameters['prior_init']\n        }\n        self.net = BayesianNetwork(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(\"BNN Parameters: \")\n        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def loss_step(self, x, y, batch_id):\n        beta = 2 ** (self.num_batches - (batch_id + 1)) / (2 ** self.num_batches - 1) \n        self.net.train()\n        self.net.zero_grad()\n        loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)\n        net_loss = loss_info[0]\n        net_loss.backward()\n        self.optimiser.step()\n        return loss_info\n\nclass Greedy_Bandit(Bandit):\n    def __init__(self, label, *args):\n        super().__init__(label, *args)\n        self.writer = SummaryWriter(comment=f\"_{label}_training\"),\n    \n    def init_net(self, parameters):\n        model_params = {\n            'input_shape': self.x.shape[1]+2,\n            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n            'batch_size': self.batch_size,\n            'hidden_units': parameters['hidden_units'],\n            'experiment': parameters['experiment']\n        }\n        self.net = MLP(model_params).to(device)\n        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n        print(f'Bandit {self.label} Parameters: ')\n        print(f'buffer_size: {self.buffer_size}, batch size: {self.batch_size}, number of samples: {self.n_samples}, epsilon: {self.epsilon}')\n        print(\"MLP Parameters: \")\n        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n\n    def loss_step(self, x, y, batch_id):\n        self.net.train()\n        self.net.zero_grad()\n        net_loss = torch.nn.functional.mse_loss(self.net(x).squeeze(), y, reduction='sum')\n        net_loss.backward()\n        self.optimiser.step()\n        return net_loss","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:57:28.180590Z","iopub.status.idle":"2025-06-05T09:57:28.180855Z","shell.execute_reply.started":"2025-06-05T09:57:28.180700Z","shell.execute_reply":"2025-06-05T09:57:28.180713Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef rl_trainer():\n    config = RLConfig\n    X, Y = read_data_rl(config.data_dir)\n\n    params = {\n        'buffer_size': config.buffer_size,\n        'batch_size': config.batch_size,\n        'num_batches': config.num_batches,\n        'lr': config.lr,\n        'hidden_units': config.hidden_units,\n        'experiment': config.experiment,\n        'mu_init': config.mu_init,\n        'rho_init': config.rho_init,\n        'prior_init': config.prior_init\n    }\n\n    bandit = BNN_Bandit('bnn_bandit', {**params, 'n_samples':2, 'epsilon':0}, X, Y)\n    \n    training_steps = config.training_steps\n    print(f\"Initialising training on {device}...\")\n    training_data_len = len(X)\n    for step in tqdm(range(training_steps)):\n        mushroom = np.random.randint(training_data_len)\n        bandit.update(mushroom)\n        bandit.scheduler.step()\n\n    # Plot cumulative regret\n    plt.figure(figsize=(10, 6))\n    plt.plot(bandit.cumulative_regrets, label='Cumulative Regret')\n    plt.xlabel('Steps')\n    plt.ylabel('Cumulative Regret')\n    plt.title('Cumulative Regret over Time')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n#rl_trainer()","metadata":{"execution":{"iopub.status.busy":"2025-06-05T09:57:28.182003Z","iopub.status.idle":"2025-06-05T09:57:28.182312Z","shell.execute_reply.started":"2025-06-05T09:57:28.182148Z","shell.execute_reply":"2025-06-05T09:57:28.182161Z"},"trusted":true},"outputs":[],"execution_count":null}]}