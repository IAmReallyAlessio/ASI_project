{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001cefb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x235eacdd490>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBB_HyperParameters(object):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        self.dataset = 'mnist'  \n",
    "\n",
    "        self.lr = 1e-4 #1e-3, 1e-4, 1e-5\n",
    "        self.momentum = 0.95\n",
    "        self.hidden_units = 1200\n",
    "        self.mixture = True\n",
    "        self.pi = 0.75 # 0.75, 0.5, 0.25\n",
    "        self.s1 = float(np.exp(-1)) # exp(0), exp(-1), exp(-2)\n",
    "        self.s2 = float(np.exp(-8)) # exp(-6), exp(-7), exp(-8)\n",
    "        self.rho_init = -8\n",
    "\n",
    "        self.max_epoch = 5\n",
    "        self.n_test_samples = 10\n",
    "        self.batch_size = 128\n",
    "    \n",
    "\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    return (1. / (torch.sqrt(torch.tensor(2. * np.pi)) * sigma)) * torch.exp(- (x - mu) ** 2 / (2. * sigma ** 2))\n",
    "\n",
    "\n",
    "def mixture_prior(input, pi, s1, s2):\n",
    "    p1 = pi * gaussian(input, 0., s1)\n",
    "    p2 = (1. - pi) * gaussian(input, 0., s2)\n",
    "    return torch.log(p1 + p2)\n",
    "\n",
    "\n",
    "def log_gaussian_rho(x, mu, rho):\n",
    "    return float(-0.5 * np.log(2 * np.pi)) - rho - (x - mu) ** 2 / (2 * torch.exp(rho) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f524febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBBLayer(nn.Module):\n",
    "    def __init__(self, n_input, n_output, hyper):\n",
    "        super(BBBLayer, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.s1 = hyper.s1\n",
    "        self.s2 = hyper.s2\n",
    "        self.pi = hyper.pi\n",
    "\n",
    "        # We initialise weigth_mu and bias_mu as for usual Linear layers in PyTorch\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(n_output, n_input).uniform_(-0.1, 0.1))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(n_output).uniform_(-0.1, 0.1))\n",
    "\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(n_output).uniform_(-0.1, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(n_output, n_input).uniform_(-0.1, 0.1))\n",
    "\n",
    "        self.log_prior = 0. \n",
    "        self.log_varpost = 0. \n",
    "\n",
    "    def forward(self, data, infer=False):\n",
    "        if infer:\n",
    "            output = F.linear(data, self.weight_mu, self.bias_mu)\n",
    "            return output\n",
    "\n",
    "        epsilon_W = Variable(torch.Tensor(self.n_output, self.n_input).normal_(0, 1).cuda())\n",
    "        epsilon_b = Variable(torch.Tensor(self.n_output).normal_(0, 1).cuda())\n",
    "        W = self.weight_mu + torch.log(1+torch.exp(self.weight_rho)) * epsilon_W\n",
    "        b = self.bias_mu + torch.log(1+torch.exp(self.bias_rho)) * epsilon_b\n",
    "\n",
    "        self.log_varpost = log_gaussian_rho(W, self.weight_mu, self.weight_rho).sum() + log_gaussian_rho(b, self.bias_mu, self.bias_rho).sum()\n",
    "        self.log_prior = mixture_prior(W, self.pi, self.s2, self.s1).sum() + mixture_prior(b, self.pi, self.s2, self.s1).sum()\n",
    "\n",
    "        output = F.linear(data, W, b)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fbaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBB(nn.Module):\n",
    "    def __init__(self, n_input, n_ouput, hyper):\n",
    "        super(BBB, self).__init__()\n",
    "\n",
    "        self.n_input = n_input\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.append(BBBLayer(n_input, hyper.hidden_units, hyper))\n",
    "        self.layers.append(BBBLayer(hyper.hidden_units, hyper.hidden_units, hyper))\n",
    "        self.layers.append(BBBLayer(hyper.hidden_units, n_ouput, hyper))\n",
    "\n",
    "    def forward(self, data, infer=False):\n",
    "        output = F.relu(self.layers[0](data.view(-1, self.n_input), infer))\n",
    "        output = F.relu(self.layers[1](output, infer))\n",
    "        output = F.softmax(self.layers[2](output, infer), dim=1)\n",
    "        return output\n",
    "\n",
    "    def get_prior_varpost(self):\n",
    "        log_prior = self.layers[0].log_prior + self.layers[1].log_prior + self.layers[2].log_prior\n",
    "        log_varpost = self.layers[0].log_varpost + self.layers[1].log_varpost + self.layers[2].log_varpost\n",
    "        return log_prior, log_varpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a4bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloSampling(model, data, target):\n",
    "    s_log_prior, s_log_varpost, s_log_likelihood = 0., 0., 0.\n",
    "    \n",
    "    output = torch.log(model(data))\n",
    "\n",
    "    sample_log_prior, sample_log_varpost = model.get_prior_varpost()\n",
    "    sample_log_likelihood = -F.nll_loss(output, target, reduction='sum')\n",
    "\n",
    "    s_log_prior += sample_log_prior \n",
    "    s_log_varpost += sample_log_varpost \n",
    "    s_log_likelihood += sample_log_likelihood\n",
    "\n",
    "    return s_log_prior, s_log_varpost, s_log_likelihood\n",
    "\n",
    "\n",
    "def ELBO(log_prior, log_varpost, l_likelihood, m):\n",
    "    kl = (1/m) * (log_varpost - log_prior)\n",
    "    return kl - l_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23d8f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader, train=True):\n",
    "    loss_sum = 0\n",
    "    kl_sum = 0\n",
    "    m = len(loader)\n",
    "\n",
    "    for batch_id, (data, target) in enumerate(loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_prior, log_varpost, l_likelihood = MonteCarloSampling(model, data, target)\n",
    "        loss = ELBO(log_prior, log_varpost, l_likelihood, m)\n",
    "        loss_sum += loss / len(loader)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            kl_sum += (1. / len(loader)) * (log_varpost - log_prior)\n",
    "    if train:\n",
    "        return loss_sum\n",
    "    else:\n",
    "        return kl_sum\n",
    "\n",
    "def evaluate(model, loader, infer=True, samples=1):\n",
    "    acc_sum = 0\n",
    "    for idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        if samples == 1:\n",
    "            output = model(data, infer=infer)\n",
    "        else:\n",
    "            output = model(data)\n",
    "            for i in range(samples - 1):\n",
    "                output += model(data)\n",
    "\n",
    "        predict = output.data.max(1)[1]\n",
    "        acc = predict.eq(target.data).cpu().sum().item()\n",
    "        acc_sum += acc\n",
    "    return acc_sum / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207692bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BBB_run(hyper, train_loader, valid_loader, test_loader, n_input, n_output):\n",
    "    \n",
    "    model = BBB(n_input, n_output, hyper).cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=hyper.lr, momentum=hyper.momentum)\n",
    "\n",
    "    train_losses = np.zeros(hyper.max_epoch)\n",
    "    valid_accs = np.zeros(hyper.max_epoch)\n",
    "    test_accs = np.zeros(hyper.max_epoch)\n",
    "\n",
    "    for epoch in range(hyper.max_epoch):\n",
    "        train_loss = train(model, optimizer, train_loader)\n",
    "        valid_acc = evaluate(model, valid_loader)\n",
    "        test_acc = evaluate(model, test_loader)\n",
    "\n",
    "        print('Epoch', epoch + 1, 'Loss', float(train_loss),\n",
    "              'Valid Error', round(100 * (1 - valid_acc / hyper.eval_batch_size), 3), '%',\n",
    "              'Test Error',  round(100 * (1 - test_acc / hyper.eval_batch_size), 3), '%')\n",
    "\n",
    "        valid_accs[epoch] = valid_acc\n",
    "        test_accs[epoch] = test_acc\n",
    "        train_losses[epoch] = train_loss\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b323e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x * 255. / 126.),  # divide as in paper\n",
    "        ])\n",
    "\n",
    "\n",
    "train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False, download=False, transform=transform)\n",
    "\n",
    "print(len(train_data), len(test_data))\n",
    "\n",
    "n_input = 28 * 28\n",
    "n_ouput = 10\n",
    "\n",
    "split_size = 1/6\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, validaton_set= torch.utils.data.random_split(train_data, [1 - split_size, split_size], generator=generator)\n",
    "\n",
    "print(len(train_set), len(validaton_set))\n",
    "\n",
    "\n",
    "hyper = BBB_HyperParameters()\n",
    "\n",
    "training_dataloader = DataLoader(train_set, batch_size=hyper.batch_size, shuffle=True, num_workers=1)\n",
    "validation_dataloader = DataLoader(validaton_set, batch_size=hyper.batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data, batch_size=hyper.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=hyper.batch_size,\n",
    "#         sampler=train_sampler,\n",
    "#         num_workers=1)\n",
    "#     valid_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=hyper.eval_batch_size,\n",
    "#         sampler=valid_sampler,\n",
    "#         num_workers=1)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_data,\n",
    "#         batch_size=hyper.eval_batch_size,\n",
    "#         num_workers=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
