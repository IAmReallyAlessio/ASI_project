{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"26a358d5","cell_type":"code","source":"import csv\nimport sys\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torchvision import datasets\nimport matplotlib.pyplot as plt\nplt.switch_backend('agg')\n\ntorch.manual_seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:21.737446Z","iopub.execute_input":"2025-05-19T15:34:21.738162Z","iopub.status.idle":"2025-05-19T15:34:21.749537Z","shell.execute_reply.started":"2025-05-19T15:34:21.738130Z","shell.execute_reply":"2025-05-19T15:34:21.748804Z"}},"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c4baf1dc610>"},"metadata":{}}],"execution_count":95},{"id":"b7e406cf","cell_type":"code","source":"class BBB_HyperParameters(object):\n\n    def __init__(self, ):\n        self.lr = 1e-4 #1e-3, 1e-4, 1e-5\n        self.momentum = 0.95\n        self.hidden_units = 16\n        self.pi = 0.75 # 0.75, 0.5, 0.25\n        self.s1 = float(np.exp(-1)) # exp(0), exp(-1), exp(-2)\n        self.s2 = float(np.exp(-8)) # exp(-6), exp(-7), exp(-8)\n        self.max_epoch = 600\n        self.batch_size = 200\n        self.num_batches = 10\n        self.test_batch_size = 50\n        self.test_samples = 10\n\n\ndef gaussian(x, mu, sigma):\n    return (1. / (torch.sqrt(torch.tensor(2. * np.pi)) * sigma)) * torch.exp(- (x - mu) ** 2 / (2. * sigma ** 2))\n\n\ndef mixture_prior(input, pi, s1, s2):\n    p1 = pi * gaussian(input, 0., s1)\n    p2 = (1. - pi) * gaussian(input, 0., s2)\n    return torch.log(p1 + p2)\n\n\ndef log_gaussian_rho(x, mu, rho):\n    return float(-0.5 * np.log(2 * np.pi)) - rho - (x - mu) ** 2 / (2 * torch.exp(rho) ** 2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:25.810844Z","iopub.execute_input":"2025-05-19T15:34:25.811504Z","iopub.status.idle":"2025-05-19T15:34:25.817324Z","shell.execute_reply.started":"2025-05-19T15:34:25.811480Z","shell.execute_reply":"2025-05-19T15:34:25.816648Z"}},"outputs":[],"execution_count":97},{"id":"bc2f5819","cell_type":"code","source":"class BBBLayer(nn.Module):\n    def __init__(self, n_input, n_output, hyper):\n        super(BBBLayer, self).__init__()\n        self.n_input = n_input\n        self.n_output = n_output\n\n        self.s1 = hyper.s1\n        self.s2 = hyper.s2\n        self.pi = hyper.pi\n\n        \n        self.weight_mu = nn.Parameter(torch.Tensor(n_output, n_input))\n        self.bias_mu = nn.Parameter(torch.Tensor(n_output))\n\n        torch.nn.init.kaiming_uniform_(self.weight_mu, nonlinearity='relu')\n        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight_mu)\n        bound = 1 / math.sqrt(fan_in)\n        torch.nn.init.uniform_(self.bias_mu, -bound, bound)\n\n        self.weight_rho = nn.Parameter(torch.Tensor(n_output, n_input).normal_(-10.0, 0.05))\n        self.bias_rho = nn.Parameter(torch.Tensor(n_output).normal_(-10.0, 0.05))\n\n        self.log_prior = 0. \n        self.log_varpost = 0. \n\n    def forward(self, data, infer=False):\n        if infer:\n            output = F.linear(data, self.weight_mu, self.bias_mu)\n            return output\n\n        epsilon_W = Variable(torch.Tensor(self.n_output, self.n_input).normal_(0., 1.).cuda())\n        epsilon_b = Variable(torch.Tensor(self.n_output).normal_(0., 1.).cuda())\n        W = self.weight_mu + torch.log(1+torch.exp(self.weight_rho)) * epsilon_W\n        b = self.bias_mu + torch.log(1+torch.exp(self.bias_rho)) * epsilon_b\n\n        self.log_varpost = log_gaussian_rho(W, self.weight_mu, self.weight_rho).sum() + log_gaussian_rho(b, self.bias_mu, self.bias_rho).sum()\n        self.log_prior = mixture_prior(W, self.pi, self.s2, self.s1).sum() + mixture_prior(b, self.pi, self.s2, self.s1).sum()\n\n        output = F.linear(data, W, b)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:27.652130Z","iopub.execute_input":"2025-05-19T15:34:27.652749Z","iopub.status.idle":"2025-05-19T15:34:27.661049Z","shell.execute_reply.started":"2025-05-19T15:34:27.652725Z","shell.execute_reply":"2025-05-19T15:34:27.660312Z"}},"outputs":[],"execution_count":98},{"id":"5005c594","cell_type":"code","source":"class BBB(nn.Module):\n    def __init__(self, n_input, n_output, hyper):\n        super(BBB, self).__init__()\n\n        self.n_input = n_input\n        self.layers = nn.ModuleList([])\n        self.layers.append(BBBLayer(n_input, hyper.hidden_units, hyper))\n        self.layers.append(BBBLayer(hyper.hidden_units, hyper.hidden_units, hyper))\n        #self.layers.append(BBBLayer(hyper.hidden_units, hyper.hidden_units, hyper))\n        self.layers.append(BBBLayer(hyper.hidden_units, n_output, hyper))\n\n    def forward(self, data, infer=False):\n        output = F.relu(self.layers[0](data.view(-1, self.n_input), infer))\n        output = F.relu(self.layers[1](output, infer))\n        #output = F.relu(self.layers[2](output, infer))\n        output = self.layers[2](output, infer)\n        return output\n\n    def get_prior_varpost(self):\n        log_prior = self.layers[0].log_prior + self.layers[1].log_prior + self.layers[2].log_prior #+ self.layers[3].log_prior\n        log_varpost = self.layers[0].log_varpost + self.layers[1].log_varpost + self.layers[2].log_varpost #+ self.layers[3].log_prior\n        return log_prior, log_varpost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:32.092347Z","iopub.execute_input":"2025-05-19T15:34:32.093066Z","iopub.status.idle":"2025-05-19T15:34:32.098878Z","shell.execute_reply.started":"2025-05-19T15:34:32.093042Z","shell.execute_reply":"2025-05-19T15:34:32.098191Z"}},"outputs":[],"execution_count":100},{"id":"4993ce98","cell_type":"code","source":"def MonteCarloSampling(model, data, target):\n    s_log_prior, s_log_varpost, s_log_likelihood = 0., 0., 0.\n\n    #print(model(data)[0])\n    \n    output = model(data)\n\n    #print(f\"Log-Output: {output}\")\n\n    sample_log_prior, sample_log_varpost = model.get_prior_varpost()\n\n    #print(f\"log_prior: {sample_log_prior}, log_varpost: {sample_log_varpost}\")\n\n    \n    sample_log_likelihood = -(0.5 * (target - output) ** 2).sum() \n\n    #print(f\"log_likelihood: {sample_log_likelihood}\")\n\n    s_log_prior += sample_log_prior \n    s_log_varpost += sample_log_varpost \n    s_log_likelihood += sample_log_likelihood\n\n    return s_log_prior, s_log_varpost, s_log_likelihood\n\n\ndef ELBO(log_prior, log_varpost, l_likelihood, pi_i):\n    kl = (1/pi_i) * (log_varpost - log_prior)\n    return kl - l_likelihood","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:56.257899Z","iopub.execute_input":"2025-05-19T15:34:56.258575Z","iopub.status.idle":"2025-05-19T15:34:56.263331Z","shell.execute_reply.started":"2025-05-19T15:34:56.258550Z","shell.execute_reply":"2025-05-19T15:34:56.262498Z"}},"outputs":[],"execution_count":103},{"id":"6c8f1c4a","cell_type":"code","source":"def train(model, optimizer, data, target, num_batches):\n    loss_sum = 0\n    m = num_batches\n\n    for i in range(m):\n        x = data[i].reshape((-1, 1)).cuda()\n        y = target[i].reshape((-1, 1)).cuda()\n        model.zero_grad()\n        \n        log_prior, log_varpost, l_likelihood = MonteCarloSampling(model, x, y)\n        #pi_i = (2 **(m - i + 1)) / (2 **(m) - 1)\n        pi_i = m\n\n        loss = ELBO(log_prior, log_varpost, l_likelihood, pi_i)\n        loss_sum += loss / m\n\n        loss.backward()\n        optimizer.step()\n        \n    return loss_sum\n   \n\ndef evaluate(model, loader, infer=True, samples=1):\n    acc_sum = 0\n    for idx, (data, target) in enumerate(loader):\n        data, target = data.cuda(), target.cuda()\n\n        if samples == 1:\n            output = model(data, infer=infer)\n        \n        predict = output.data.max(1)[1]\n        acc = predict.eq(target.data).cpu().sum().item()\n        acc_sum += acc\n    return acc_sum / len(loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:34:58.015487Z","iopub.execute_input":"2025-05-19T15:34:58.016049Z","iopub.status.idle":"2025-05-19T15:34:58.021743Z","shell.execute_reply.started":"2025-05-19T15:34:58.016027Z","shell.execute_reply":"2025-05-19T15:34:58.020971Z"}},"outputs":[],"execution_count":104},{"id":"9074f8b8","cell_type":"code","source":"def BBB_run(hyper, data, target, n_input, n_output, X_test):\n\n    \n    model = BBB(n_input, n_output, hyper).cuda()\n    optimizer = torch.optim.SGD(model.parameters(), lr=hyper.lr, momentum=hyper.momentum)\n\n    train_losses = np.zeros(hyper.max_epoch)\n    \n\n    for epoch in range(hyper.max_epoch):\n        train_loss = train(model, optimizer, data, target, hyper.num_batches)\n        \n        print('Epoch', epoch + 1, 'Loss', float(train_loss))\n        train_losses[epoch] = train_loss\n\n    outputs = torch.zeros(hyper.test_samples, hyper.test_batch_size, n_output).cuda()\n    for i in range(hyper.test_samples):\n        outputs[i] = model.forward(X_test)\n    pred_mean = outputs.mean(0).data.cpu().numpy().squeeze(1) #Compute mean prediction\n    pred_std = outputs.std(0).data.cpu().numpy().squeeze(1) #Compute standard deviation of prediction for each data point\n\n\n    return pred_mean, pred_std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:35:00.796471Z","iopub.execute_input":"2025-05-19T15:35:00.796803Z","iopub.status.idle":"2025-05-19T15:35:00.803209Z","shell.execute_reply.started":"2025-05-19T15:35:00.796779Z","shell.execute_reply":"2025-05-19T15:35:00.802362Z"}},"outputs":[],"execution_count":105},{"id":"e3814fca","cell_type":"code","source":"#Hyperparameter setting\n\nhyper = BBB_HyperParameters()\n\nprint('Generating Data set.')\n\n#Data Generation step\nif torch.cuda.is_available():\n    Var = lambda x, dtype=torch.cuda.FloatTensor: Variable(torch.from_numpy(x).type(dtype)) #converting data to tensor\nelse:\n    Var = lambda x, dtype=torch.FloatTensor: Variable(torch.from_numpy(x).type(dtype)) #converting data to tensor\n\nx = np.random.uniform(-0.1, 0.61, size=(hyper.num_batches, hyper.batch_size))\nnoise = np.random.normal(0, 0.02, size=(hyper.num_batches, hyper.batch_size)) #metric as mentioned in the paper\ny = x + 0.3*np.sin(2*np.pi*(x+noise)) + 0.3*np.sin(4*np.pi*(x+noise)) + noise\n\nx_test = np.linspace(-0.5, 1, hyper.test_batch_size)\ny_test = x_test + 0.3*np.sin(2*np.pi*x_test) + 0.3*np.sin(4*np.pi*x_test)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:35:02.860325Z","iopub.execute_input":"2025-05-19T15:35:02.860782Z","iopub.status.idle":"2025-05-19T15:35:02.868503Z","shell.execute_reply.started":"2025-05-19T15:35:02.860760Z","shell.execute_reply":"2025-05-19T15:35:02.867743Z"}},"outputs":[{"name":"stdout","text":"Generating Data set.\n","output_type":"stream"}],"execution_count":106},{"id":"8b8a0ec6","cell_type":"code","source":"def BBB_Regression(x,y,x_test,y_test):\n\n    print('BBB Training Begins!')\n\n    X = Var(x)\n    Y = Var(y)\n    X_test = Var(x_test)\n\n    pred_mean, pred_std = BBB_run(hyper, X, Y, 1, 1, X_test)\n\n    print('Training Ends!')\n\n\n    #Visualization\n    plt.fill_between(x_test, pred_mean - 3 * pred_std, pred_mean + 3 * pred_std,\n                        color='cornflowerblue', alpha=.5, label='+/- 3 std')\n    plt.scatter(x, y,marker='x', c='black', label='target')\n    plt.plot(x_test, pred_mean, c='red', label='Prediction')\n    plt.plot(x_test, y_test, c='grey', label='truth')\n    plt.legend()\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:35:04.807528Z","iopub.execute_input":"2025-05-19T15:35:04.808052Z","iopub.status.idle":"2025-05-19T15:35:04.813007Z","shell.execute_reply.started":"2025-05-19T15:35:04.808029Z","shell.execute_reply":"2025-05-19T15:35:04.812261Z"}},"outputs":[],"execution_count":107},{"id":"65256de1","cell_type":"code","source":"#Comparing to standard neural network\ndef NN_Regression(x,y,x_test,y_test):\n\n    print('SGD Training Begins!')\n\n    x = x.flatten()\n    X = Var(x)\n    X = torch.unsqueeze(X,1)\n    \n    y = y.flatten()\n    Y = Var(y)\n    Y = torch.unsqueeze(Y,1)\n    X_test = Var(x_test)\n    X_test = torch.unsqueeze(X_test,1)\n\n    class Net(torch.nn.Module):\n        def __init__(self, n_feature, n_hidden, n_output):\n            super(Net, self).__init__()\n            self.l1 = torch.nn.Linear(n_feature, n_hidden[0])   # hidden layer 1\n            self.l2 =  torch.nn.Linear(n_hidden[0], n_hidden[1])   # hidden layer 2\n            self.l3 =  torch.nn.Linear(n_hidden[1], n_hidden[2])   # hidden layer 3\n            self.predict = torch.nn.Linear(n_hidden[2], n_output)   # output layer\n\n        def forward(self, x):\n            x = F.relu(self.l1(x))      # activation function for hidden layer 1\n            x = F.relu(self.l2(x))      # activation function for hidden layer 2\n            x = F.relu(self.l3(x))      # activation function for hidden layer 3\n            x = self.predict(x)         # linear output\n            return x\n\n    net = Net(n_feature=1, n_hidden=[16,16,16], n_output=1)\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n    loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n\n    for epoch in range(2000):\n        if (epoch)%10 == 0:\n            print('Epoch: ', epoch)\n        prediction = net(X)     # input x and predict based on x\n        loss = loss_func(prediction, Y)     # must be (1. nn output, 2. target)\n        optimizer.zero_grad()   # clear gradients for next train\n        loss.backward()         # backpropagation, compute gradients\n        optimizer.step()        # apply gradients\n    \n    prediction = net(X_test)\n    \n    #Visualization\n    plt.scatter(x, y,marker='x', c='black', label='target')\n    plt.plot(x_test, prediction.detach().numpy(), c='red', label='Prediction')\n    plt.plot(x_test, y_test, c='grey', label='truth')\n    plt.legend()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:35:08.888065Z","iopub.execute_input":"2025-05-19T15:35:08.888566Z","iopub.status.idle":"2025-05-19T15:35:08.897168Z","shell.execute_reply.started":"2025-05-19T15:35:08.888542Z","shell.execute_reply":"2025-05-19T15:35:08.896614Z"}},"outputs":[],"execution_count":108},{"id":"bd4ace09","cell_type":"code","source":"BBB_Regression(x,y,x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:35:11.626103Z","iopub.execute_input":"2025-05-19T15:35:11.626373Z","iopub.status.idle":"2025-05-19T15:36:09.680288Z","shell.execute_reply.started":"2025-05-19T15:35:11.626342Z","shell.execute_reply":"2025-05-19T15:36:09.679737Z"}},"outputs":[{"name":"stdout","text":"BBB Training Begins!\nEpoch 1 Loss 368.9903564453125\nEpoch 2 Loss 356.34503173828125\nEpoch 3 Loss 353.4905090332031\nEpoch 4 Loss 352.1666564941406\nEpoch 5 Loss 349.9650573730469\nEpoch 6 Loss 350.67901611328125\nEpoch 7 Loss 350.3206481933594\nEpoch 8 Loss 349.6060791015625\nEpoch 9 Loss 348.5667419433594\nEpoch 10 Loss 348.74151611328125\nEpoch 11 Loss 347.3508605957031\nEpoch 12 Loss 346.37652587890625\nEpoch 13 Loss 344.8970642089844\nEpoch 14 Loss 344.9372253417969\nEpoch 15 Loss 344.7533264160156\nEpoch 16 Loss 344.1832580566406\nEpoch 17 Loss 344.4295654296875\nEpoch 18 Loss 343.0972900390625\nEpoch 19 Loss 342.1490173339844\nEpoch 20 Loss 341.17816162109375\nEpoch 21 Loss 340.5167236328125\nEpoch 22 Loss 340.17828369140625\nEpoch 23 Loss 339.2149658203125\nEpoch 24 Loss 337.9145812988281\nEpoch 25 Loss 337.1806335449219\nEpoch 26 Loss 337.24932861328125\nEpoch 27 Loss 336.8558349609375\nEpoch 28 Loss 337.1261291503906\nEpoch 29 Loss 335.79046630859375\nEpoch 30 Loss 335.26104736328125\nEpoch 31 Loss 335.2823791503906\nEpoch 32 Loss 334.5964050292969\nEpoch 33 Loss 334.1100769042969\nEpoch 34 Loss 333.9424133300781\nEpoch 35 Loss 333.8165588378906\nEpoch 36 Loss 332.8368835449219\nEpoch 37 Loss 333.3520202636719\nEpoch 38 Loss 332.58343505859375\nEpoch 39 Loss 332.3053894042969\nEpoch 40 Loss 331.21844482421875\nEpoch 41 Loss 331.6429138183594\nEpoch 42 Loss 331.8612976074219\nEpoch 43 Loss 331.1697082519531\nEpoch 44 Loss 329.144287109375\nEpoch 45 Loss 329.9214172363281\nEpoch 46 Loss 330.0665283203125\nEpoch 47 Loss 328.52801513671875\nEpoch 48 Loss 328.5994873046875\nEpoch 49 Loss 328.600341796875\nEpoch 50 Loss 328.3764343261719\nEpoch 51 Loss 327.82232666015625\nEpoch 52 Loss 327.8571472167969\nEpoch 53 Loss 326.9280700683594\nEpoch 54 Loss 326.82611083984375\nEpoch 55 Loss 326.3854064941406\nEpoch 56 Loss 327.88140869140625\nEpoch 57 Loss 328.2308654785156\nEpoch 58 Loss 329.0426940917969\nEpoch 59 Loss 328.6848449707031\nEpoch 60 Loss 327.74755859375\nEpoch 61 Loss 327.8497009277344\nEpoch 62 Loss 328.24151611328125\nEpoch 63 Loss 328.302978515625\nEpoch 64 Loss 328.4070129394531\nEpoch 65 Loss 327.9073181152344\nEpoch 66 Loss 328.00933837890625\nEpoch 67 Loss 326.86834716796875\nEpoch 68 Loss 327.0752258300781\nEpoch 69 Loss 327.1958923339844\nEpoch 70 Loss 326.57733154296875\nEpoch 71 Loss 325.8051452636719\nEpoch 72 Loss 325.7663879394531\nEpoch 73 Loss 326.1692810058594\nEpoch 74 Loss 326.3121337890625\nEpoch 75 Loss 329.3168640136719\nEpoch 76 Loss 328.66546630859375\nEpoch 77 Loss 328.9353332519531\nEpoch 78 Loss 327.9635009765625\nEpoch 79 Loss 327.30670166015625\nEpoch 80 Loss 326.9604797363281\nEpoch 81 Loss 326.21893310546875\nEpoch 82 Loss 325.13970947265625\nEpoch 83 Loss 325.01690673828125\nEpoch 84 Loss 324.9035949707031\nEpoch 85 Loss 324.1545104980469\nEpoch 86 Loss 324.2037353515625\nEpoch 87 Loss 323.74847412109375\nEpoch 88 Loss 323.7135009765625\nEpoch 89 Loss 323.7627868652344\nEpoch 90 Loss 322.6911926269531\nEpoch 91 Loss 322.8036193847656\nEpoch 92 Loss 322.2528991699219\nEpoch 93 Loss 322.3230895996094\nEpoch 94 Loss 323.09710693359375\nEpoch 95 Loss 322.48651123046875\nEpoch 96 Loss 322.4305419921875\nEpoch 97 Loss 322.1034240722656\nEpoch 98 Loss 321.5515441894531\nEpoch 99 Loss 321.4735107421875\nEpoch 100 Loss 321.9945983886719\nEpoch 101 Loss 321.42755126953125\nEpoch 102 Loss 322.4931945800781\nEpoch 103 Loss 321.0588073730469\nEpoch 104 Loss 321.3689270019531\nEpoch 105 Loss 321.56549072265625\nEpoch 106 Loss 321.10919189453125\nEpoch 107 Loss 321.1152038574219\nEpoch 108 Loss 320.760009765625\nEpoch 109 Loss 321.07000732421875\nEpoch 110 Loss 321.0256042480469\nEpoch 111 Loss 321.7351379394531\nEpoch 112 Loss 321.002685546875\nEpoch 113 Loss 321.1243896484375\nEpoch 114 Loss 320.7080078125\nEpoch 115 Loss 320.59417724609375\nEpoch 116 Loss 320.1678466796875\nEpoch 117 Loss 319.99151611328125\nEpoch 118 Loss 319.7142333984375\nEpoch 119 Loss 319.3412170410156\nEpoch 120 Loss 318.9461975097656\nEpoch 121 Loss 319.32232666015625\nEpoch 122 Loss 319.23858642578125\nEpoch 123 Loss 319.38275146484375\nEpoch 124 Loss 318.5208740234375\nEpoch 125 Loss 318.2747802734375\nEpoch 126 Loss 318.1609191894531\nEpoch 127 Loss 318.3316650390625\nEpoch 128 Loss 318.3450012207031\nEpoch 129 Loss 320.3299865722656\nEpoch 130 Loss 322.5318908691406\nEpoch 131 Loss 323.96710205078125\nEpoch 132 Loss 324.88507080078125\nEpoch 133 Loss 322.6370544433594\nEpoch 134 Loss 321.9387512207031\nEpoch 135 Loss 321.400390625\nEpoch 136 Loss 321.7814636230469\nEpoch 137 Loss 321.3545227050781\nEpoch 138 Loss 321.20501708984375\nEpoch 139 Loss 320.99566650390625\nEpoch 140 Loss 320.7822265625\nEpoch 141 Loss 320.38275146484375\nEpoch 142 Loss 320.32232666015625\nEpoch 143 Loss 319.933349609375\nEpoch 144 Loss 319.29827880859375\nEpoch 145 Loss 319.46380615234375\nEpoch 146 Loss 319.05810546875\nEpoch 147 Loss 319.3111267089844\nEpoch 148 Loss 318.5232849121094\nEpoch 149 Loss 319.1927795410156\nEpoch 150 Loss 318.4760437011719\nEpoch 151 Loss 317.9839172363281\nEpoch 152 Loss 318.01080322265625\nEpoch 153 Loss 317.08209228515625\nEpoch 154 Loss 318.5223388671875\nEpoch 155 Loss 318.07025146484375\nEpoch 156 Loss 317.59295654296875\nEpoch 157 Loss 317.9115295410156\nEpoch 158 Loss 317.5980224609375\nEpoch 159 Loss 317.4530029296875\nEpoch 160 Loss 317.38629150390625\nEpoch 161 Loss 317.2398376464844\nEpoch 162 Loss 317.0158996582031\nEpoch 163 Loss 317.0164489746094\nEpoch 164 Loss 316.6204528808594\nEpoch 165 Loss 316.9457092285156\nEpoch 166 Loss 317.0274963378906\nEpoch 167 Loss 317.151611328125\nEpoch 168 Loss 315.9399108886719\nEpoch 169 Loss 316.19976806640625\nEpoch 170 Loss 317.515625\nEpoch 171 Loss 317.002685546875\nEpoch 172 Loss 316.55853271484375\nEpoch 173 Loss 317.0223083496094\nEpoch 174 Loss 316.6185302734375\nEpoch 175 Loss 316.7933044433594\nEpoch 176 Loss 316.80303955078125\nEpoch 177 Loss 316.4768371582031\nEpoch 178 Loss 317.1888427734375\nEpoch 179 Loss 317.1213073730469\nEpoch 180 Loss 316.68463134765625\nEpoch 181 Loss 316.15264892578125\nEpoch 182 Loss 315.73443603515625\nEpoch 183 Loss 315.6578369140625\nEpoch 184 Loss 316.3136291503906\nEpoch 185 Loss 315.7352294921875\nEpoch 186 Loss 314.94024658203125\nEpoch 187 Loss 315.31414794921875\nEpoch 188 Loss 315.53802490234375\nEpoch 189 Loss 315.890869140625\nEpoch 190 Loss 314.3728942871094\nEpoch 191 Loss 313.844970703125\nEpoch 192 Loss 314.29296875\nEpoch 193 Loss 314.46624755859375\nEpoch 194 Loss 314.0711975097656\nEpoch 195 Loss 314.1323547363281\nEpoch 196 Loss 314.51300048828125\nEpoch 197 Loss 314.83734130859375\nEpoch 198 Loss 313.7100524902344\nEpoch 199 Loss 314.58197021484375\nEpoch 200 Loss 313.93475341796875\nEpoch 201 Loss 314.8686828613281\nEpoch 202 Loss 314.6860046386719\nEpoch 203 Loss 314.5598449707031\nEpoch 204 Loss 313.6591796875\nEpoch 205 Loss 313.37744140625\nEpoch 206 Loss 314.9128723144531\nEpoch 207 Loss 315.353759765625\nEpoch 208 Loss 314.6268615722656\nEpoch 209 Loss 314.8932189941406\nEpoch 210 Loss 314.69561767578125\nEpoch 211 Loss 314.6711120605469\nEpoch 212 Loss 314.24456787109375\nEpoch 213 Loss 314.9751281738281\nEpoch 214 Loss 315.1833190917969\nEpoch 215 Loss 314.9955139160156\nEpoch 216 Loss 315.443603515625\nEpoch 217 Loss 314.5812072753906\nEpoch 218 Loss 314.79876708984375\nEpoch 219 Loss 315.3682556152344\nEpoch 220 Loss 314.82763671875\nEpoch 221 Loss 314.9894104003906\nEpoch 222 Loss 315.3501892089844\nEpoch 223 Loss 315.5937805175781\nEpoch 224 Loss 315.8450012207031\nEpoch 225 Loss 315.64422607421875\nEpoch 226 Loss 316.5445861816406\nEpoch 227 Loss 316.07659912109375\nEpoch 228 Loss 316.829345703125\nEpoch 229 Loss 316.7225341796875\nEpoch 230 Loss 316.3222961425781\nEpoch 231 Loss 317.1899108886719\nEpoch 232 Loss 315.18621826171875\nEpoch 233 Loss 316.0213623046875\nEpoch 234 Loss 315.7123718261719\nEpoch 235 Loss 315.6258850097656\nEpoch 236 Loss 315.6533203125\nEpoch 237 Loss 314.4587097167969\nEpoch 238 Loss 314.5621643066406\nEpoch 239 Loss 314.33050537109375\nEpoch 240 Loss 314.93865966796875\nEpoch 241 Loss 314.9029846191406\nEpoch 242 Loss 315.2457275390625\nEpoch 243 Loss 314.8199768066406\nEpoch 244 Loss 315.4563293457031\nEpoch 245 Loss 315.95477294921875\nEpoch 246 Loss 315.3150634765625\nEpoch 247 Loss 316.13751220703125\nEpoch 248 Loss 315.098388671875\nEpoch 249 Loss 315.7772216796875\nEpoch 250 Loss 315.2835693359375\nEpoch 251 Loss 315.8533935546875\nEpoch 252 Loss 316.13421630859375\nEpoch 253 Loss 315.22967529296875\nEpoch 254 Loss 316.089111328125\nEpoch 255 Loss 317.42730712890625\nEpoch 256 Loss 318.5505676269531\nEpoch 257 Loss 318.8687438964844\nEpoch 258 Loss 317.41876220703125\nEpoch 259 Loss 317.6348571777344\nEpoch 260 Loss 317.7394104003906\nEpoch 261 Loss 318.1214904785156\nEpoch 262 Loss 319.68243408203125\nEpoch 263 Loss 320.6431579589844\nEpoch 264 Loss 320.6855163574219\nEpoch 265 Loss 321.0426330566406\nEpoch 266 Loss 321.9357604980469\nEpoch 267 Loss 322.5806884765625\nEpoch 268 Loss 322.97308349609375\nEpoch 269 Loss 324.1228942871094\nEpoch 270 Loss 324.0248107910156\nEpoch 271 Loss 324.0569763183594\nEpoch 272 Loss 324.77349853515625\nEpoch 273 Loss 324.0184020996094\nEpoch 274 Loss 324.11895751953125\nEpoch 275 Loss 322.84283447265625\nEpoch 276 Loss 323.4838562011719\nEpoch 277 Loss 322.0660095214844\nEpoch 278 Loss 321.36773681640625\nEpoch 279 Loss 321.5883483886719\nEpoch 280 Loss 321.5313415527344\nEpoch 281 Loss 321.0755310058594\nEpoch 282 Loss 321.03558349609375\nEpoch 283 Loss 321.0558166503906\nEpoch 284 Loss 320.5784912109375\nEpoch 285 Loss 319.92315673828125\nEpoch 286 Loss 320.20135498046875\nEpoch 287 Loss 320.06060791015625\nEpoch 288 Loss 320.1546936035156\nEpoch 289 Loss 320.0893249511719\nEpoch 290 Loss 320.072265625\nEpoch 291 Loss 318.72845458984375\nEpoch 292 Loss 319.32086181640625\nEpoch 293 Loss 318.2984619140625\nEpoch 294 Loss 317.2051696777344\nEpoch 295 Loss 317.935302734375\nEpoch 296 Loss 318.767578125\nEpoch 297 Loss 319.1646728515625\nEpoch 298 Loss 317.2584533691406\nEpoch 299 Loss 317.38031005859375\nEpoch 300 Loss 318.0343322753906\nEpoch 301 Loss 317.44342041015625\nEpoch 302 Loss 317.36273193359375\nEpoch 303 Loss 316.96075439453125\nEpoch 304 Loss 316.95184326171875\nEpoch 305 Loss 317.54046630859375\nEpoch 306 Loss 316.8049621582031\nEpoch 307 Loss 317.92578125\nEpoch 308 Loss 317.2734375\nEpoch 309 Loss 318.0569152832031\nEpoch 310 Loss 318.4925537109375\nEpoch 311 Loss 318.1253662109375\nEpoch 312 Loss 317.5365905761719\nEpoch 313 Loss 317.6346130371094\nEpoch 314 Loss 317.3643798828125\nEpoch 315 Loss 318.8237609863281\nEpoch 316 Loss 319.0332946777344\nEpoch 317 Loss 319.52215576171875\nEpoch 318 Loss 319.6295471191406\nEpoch 319 Loss 320.0936279296875\nEpoch 320 Loss 319.4273986816406\nEpoch 321 Loss 319.4561462402344\nEpoch 322 Loss 319.34515380859375\nEpoch 323 Loss 318.5892333984375\nEpoch 324 Loss 317.82403564453125\nEpoch 325 Loss 317.6318664550781\nEpoch 326 Loss 318.01031494140625\nEpoch 327 Loss 316.900634765625\nEpoch 328 Loss 317.2274475097656\nEpoch 329 Loss 316.7848205566406\nEpoch 330 Loss 317.01153564453125\nEpoch 331 Loss 317.9317321777344\nEpoch 332 Loss 317.8704528808594\nEpoch 333 Loss 318.1389465332031\nEpoch 334 Loss 318.2591552734375\nEpoch 335 Loss 317.9525146484375\nEpoch 336 Loss 319.9957580566406\nEpoch 337 Loss 319.6658630371094\nEpoch 338 Loss 320.7022705078125\nEpoch 339 Loss 321.2669677734375\nEpoch 340 Loss 320.78790283203125\nEpoch 341 Loss 322.7343444824219\nEpoch 342 Loss 322.8979797363281\nEpoch 343 Loss 323.2728271484375\nEpoch 344 Loss 322.9071044921875\nEpoch 345 Loss 322.9551696777344\nEpoch 346 Loss 322.3099365234375\nEpoch 347 Loss 322.64117431640625\nEpoch 348 Loss 322.3395690917969\nEpoch 349 Loss 322.79156494140625\nEpoch 350 Loss 324.10968017578125\nEpoch 351 Loss 325.28173828125\nEpoch 352 Loss 324.8641357421875\nEpoch 353 Loss 324.8233947753906\nEpoch 354 Loss 325.40753173828125\nEpoch 355 Loss 330.782958984375\nEpoch 356 Loss 332.68359375\nEpoch 357 Loss 332.7864990234375\nEpoch 358 Loss 335.1404113769531\nEpoch 359 Loss 334.6472473144531\nEpoch 360 Loss 334.98468017578125\nEpoch 361 Loss 333.0156555175781\nEpoch 362 Loss 333.5426940917969\nEpoch 363 Loss 333.42852783203125\nEpoch 364 Loss 332.888427734375\nEpoch 365 Loss 332.5408935546875\nEpoch 366 Loss 332.7684326171875\nEpoch 367 Loss 331.07525634765625\nEpoch 368 Loss 331.9031982421875\nEpoch 369 Loss 330.19189453125\nEpoch 370 Loss 329.438720703125\nEpoch 371 Loss 329.26416015625\nEpoch 372 Loss 328.56903076171875\nEpoch 373 Loss 328.4725341796875\nEpoch 374 Loss 328.49566650390625\nEpoch 375 Loss 326.65875244140625\nEpoch 376 Loss 326.4595642089844\nEpoch 377 Loss 326.1067810058594\nEpoch 378 Loss 325.02886962890625\nEpoch 379 Loss 325.4956359863281\nEpoch 380 Loss 323.9538269042969\nEpoch 381 Loss 323.571533203125\nEpoch 382 Loss 322.3935852050781\nEpoch 383 Loss 321.5761413574219\nEpoch 384 Loss 320.53570556640625\nEpoch 385 Loss 320.474853515625\nEpoch 386 Loss 319.3517761230469\nEpoch 387 Loss 318.8033752441406\nEpoch 388 Loss 318.51898193359375\nEpoch 389 Loss 317.81842041015625\nEpoch 390 Loss 316.9489440917969\nEpoch 391 Loss 316.6261901855469\nEpoch 392 Loss 315.98468017578125\nEpoch 393 Loss 315.7518310546875\nEpoch 394 Loss 314.407470703125\nEpoch 395 Loss 314.1007995605469\nEpoch 396 Loss 313.52593994140625\nEpoch 397 Loss 313.04718017578125\nEpoch 398 Loss 312.56951904296875\nEpoch 399 Loss 311.4269104003906\nEpoch 400 Loss 311.958984375\nEpoch 401 Loss 311.32537841796875\nEpoch 402 Loss 310.42083740234375\nEpoch 403 Loss 310.1136169433594\nEpoch 404 Loss 309.8300476074219\nEpoch 405 Loss 308.742431640625\nEpoch 406 Loss 308.8042297363281\nEpoch 407 Loss 308.5159912109375\nEpoch 408 Loss 307.18682861328125\nEpoch 409 Loss 307.3363037109375\nEpoch 410 Loss 306.6967468261719\nEpoch 411 Loss 306.6866760253906\nEpoch 412 Loss 307.3134765625\nEpoch 413 Loss 306.302734375\nEpoch 414 Loss 306.15655517578125\nEpoch 415 Loss 306.31866455078125\nEpoch 416 Loss 305.2055969238281\nEpoch 417 Loss 305.35894775390625\nEpoch 418 Loss 305.2974853515625\nEpoch 419 Loss 304.7506103515625\nEpoch 420 Loss 305.35601806640625\nEpoch 421 Loss 305.026611328125\nEpoch 422 Loss 304.77935791015625\nEpoch 423 Loss 303.78399658203125\nEpoch 424 Loss 303.35235595703125\nEpoch 425 Loss 304.1464538574219\nEpoch 426 Loss 303.3929138183594\nEpoch 427 Loss 302.62652587890625\nEpoch 428 Loss 302.49212646484375\nEpoch 429 Loss 303.63739013671875\nEpoch 430 Loss 302.76513671875\nEpoch 431 Loss 304.7672424316406\nEpoch 432 Loss 304.2879638671875\nEpoch 433 Loss 304.73236083984375\nEpoch 434 Loss 303.8612060546875\nEpoch 435 Loss 304.7065124511719\nEpoch 436 Loss 304.68829345703125\nEpoch 437 Loss 304.6064758300781\nEpoch 438 Loss 303.3144836425781\nEpoch 439 Loss 304.4789123535156\nEpoch 440 Loss 304.0215759277344\nEpoch 441 Loss 303.2569885253906\nEpoch 442 Loss 302.6348876953125\nEpoch 443 Loss 302.7835388183594\nEpoch 444 Loss 302.6543273925781\nEpoch 445 Loss 301.9183349609375\nEpoch 446 Loss 302.8461608886719\nEpoch 447 Loss 301.174560546875\nEpoch 448 Loss 302.5047302246094\nEpoch 449 Loss 301.052490234375\nEpoch 450 Loss 301.1110534667969\nEpoch 451 Loss 301.1473388671875\nEpoch 452 Loss 300.66741943359375\nEpoch 453 Loss 300.4184875488281\nEpoch 454 Loss 300.05584716796875\nEpoch 455 Loss 299.9234619140625\nEpoch 456 Loss 299.7345886230469\nEpoch 457 Loss 299.60723876953125\nEpoch 458 Loss 299.7271423339844\nEpoch 459 Loss 300.16314697265625\nEpoch 460 Loss 299.9930725097656\nEpoch 461 Loss 299.4956359863281\nEpoch 462 Loss 299.72174072265625\nEpoch 463 Loss 299.89892578125\nEpoch 464 Loss 299.970703125\nEpoch 465 Loss 299.75714111328125\nEpoch 466 Loss 299.04339599609375\nEpoch 467 Loss 299.10894775390625\nEpoch 468 Loss 299.86651611328125\nEpoch 469 Loss 299.3931884765625\nEpoch 470 Loss 299.31390380859375\nEpoch 471 Loss 298.5127868652344\nEpoch 472 Loss 298.53875732421875\nEpoch 473 Loss 298.5626220703125\nEpoch 474 Loss 298.14691162109375\nEpoch 475 Loss 298.48681640625\nEpoch 476 Loss 299.023681640625\nEpoch 477 Loss 298.4727783203125\nEpoch 478 Loss 299.5784606933594\nEpoch 479 Loss 299.65423583984375\nEpoch 480 Loss 300.0439147949219\nEpoch 481 Loss 299.5863037109375\nEpoch 482 Loss 300.3517150878906\nEpoch 483 Loss 299.6256103515625\nEpoch 484 Loss 299.8998718261719\nEpoch 485 Loss 299.524658203125\nEpoch 486 Loss 299.55401611328125\nEpoch 487 Loss 298.51605224609375\nEpoch 488 Loss 298.5234680175781\nEpoch 489 Loss 297.3345642089844\nEpoch 490 Loss 297.89715576171875\nEpoch 491 Loss 297.5341796875\nEpoch 492 Loss 298.4541015625\nEpoch 493 Loss 298.18695068359375\nEpoch 494 Loss 297.6482849121094\nEpoch 495 Loss 297.78387451171875\nEpoch 496 Loss 298.2945861816406\nEpoch 497 Loss 298.3917236328125\nEpoch 498 Loss 298.621826171875\nEpoch 499 Loss 298.8215026855469\nEpoch 500 Loss 299.2923278808594\nEpoch 501 Loss 298.7967224121094\nEpoch 502 Loss 299.4013977050781\nEpoch 503 Loss 299.5142517089844\nEpoch 504 Loss 300.10003662109375\nEpoch 505 Loss 299.443115234375\nEpoch 506 Loss 299.5953674316406\nEpoch 507 Loss 300.5032958984375\nEpoch 508 Loss 299.9772644042969\nEpoch 509 Loss 300.0822448730469\nEpoch 510 Loss 300.4891662597656\nEpoch 511 Loss 300.25360107421875\nEpoch 512 Loss 300.23773193359375\nEpoch 513 Loss 301.07379150390625\nEpoch 514 Loss 300.34234619140625\nEpoch 515 Loss 301.23541259765625\nEpoch 516 Loss 300.38323974609375\nEpoch 517 Loss 301.21087646484375\nEpoch 518 Loss 300.61883544921875\nEpoch 519 Loss 300.55401611328125\nEpoch 520 Loss 299.43829345703125\nEpoch 521 Loss 300.10882568359375\nEpoch 522 Loss 301.298583984375\nEpoch 523 Loss 300.7940368652344\nEpoch 524 Loss 301.0898742675781\nEpoch 525 Loss 301.4128723144531\nEpoch 526 Loss 301.0882568359375\nEpoch 527 Loss 302.01446533203125\nEpoch 528 Loss 301.5671691894531\nEpoch 529 Loss 301.90667724609375\nEpoch 530 Loss 300.7221374511719\nEpoch 531 Loss 301.4667053222656\nEpoch 532 Loss 301.0675048828125\nEpoch 533 Loss 300.5672607421875\nEpoch 534 Loss 300.5820007324219\nEpoch 535 Loss 300.5621337890625\nEpoch 536 Loss 300.7719421386719\nEpoch 537 Loss 300.5831298828125\nEpoch 538 Loss 299.9001770019531\nEpoch 539 Loss 300.56719970703125\nEpoch 540 Loss 299.9748229980469\nEpoch 541 Loss 299.17559814453125\nEpoch 542 Loss 300.1474304199219\nEpoch 543 Loss 299.5665283203125\nEpoch 544 Loss 298.6828918457031\nEpoch 545 Loss 297.90411376953125\nEpoch 546 Loss 297.9882507324219\nEpoch 547 Loss 297.3734436035156\nEpoch 548 Loss 297.2806396484375\nEpoch 549 Loss 297.2139587402344\nEpoch 550 Loss 296.0990905761719\nEpoch 551 Loss 296.806884765625\nEpoch 552 Loss 294.84112548828125\nEpoch 553 Loss 297.62884521484375\nEpoch 554 Loss 297.3980712890625\nEpoch 555 Loss 297.7393798828125\nEpoch 556 Loss 299.7929382324219\nEpoch 557 Loss 300.20172119140625\nEpoch 558 Loss 300.36712646484375\nEpoch 559 Loss 300.5127258300781\nEpoch 560 Loss 299.90045166015625\nEpoch 561 Loss 300.5321960449219\nEpoch 562 Loss 300.4199523925781\nEpoch 563 Loss 300.806640625\nEpoch 564 Loss 299.762451171875\nEpoch 565 Loss 300.2450256347656\nEpoch 566 Loss 300.2666015625\nEpoch 567 Loss 300.4134521484375\nEpoch 568 Loss 299.56005859375\nEpoch 569 Loss 299.9707946777344\nEpoch 570 Loss 299.2991638183594\nEpoch 571 Loss 298.0961608886719\nEpoch 572 Loss 297.9405822753906\nEpoch 573 Loss 297.9537658691406\nEpoch 574 Loss 297.08447265625\nEpoch 575 Loss 297.0447998046875\nEpoch 576 Loss 296.8520202636719\nEpoch 577 Loss 296.3031005859375\nEpoch 578 Loss 295.5439147949219\nEpoch 579 Loss 295.60906982421875\nEpoch 580 Loss 295.2200622558594\nEpoch 581 Loss 295.0567932128906\nEpoch 582 Loss 294.500244140625\nEpoch 583 Loss 293.826171875\nEpoch 584 Loss 293.9278869628906\nEpoch 585 Loss 293.08380126953125\nEpoch 586 Loss 293.7812194824219\nEpoch 587 Loss 293.47021484375\nEpoch 588 Loss 292.5782470703125\nEpoch 589 Loss 292.787841796875\nEpoch 590 Loss 291.7588806152344\nEpoch 591 Loss 292.4900817871094\nEpoch 592 Loss 292.2269592285156\nEpoch 593 Loss 292.9194641113281\nEpoch 594 Loss 291.6739501953125\nEpoch 595 Loss 292.1494445800781\nEpoch 596 Loss 291.35919189453125\nEpoch 597 Loss 292.13995361328125\nEpoch 598 Loss 291.72503662109375\nEpoch 599 Loss 291.18377685546875\nEpoch 600 Loss 291.2471618652344\nTraining Ends!\n","output_type":"stream"}],"execution_count":109},{"id":"f6a2b8ea","cell_type":"code","source":"NN_Regression(x,y,x_test,y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T15:01:52.437072Z","iopub.status.idle":"2025-05-19T15:01:52.437309Z","shell.execute_reply.started":"2025-05-19T15:01:52.437203Z","shell.execute_reply":"2025-05-19T15:01:52.437214Z"}},"outputs":[],"execution_count":null}]}