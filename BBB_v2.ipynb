{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef055309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:31.145415Z",
     "iopub.status.busy": "2025-06-05T12:14:31.145184Z",
     "iopub.status.idle": "2025-06-05T12:14:43.529317Z",
     "shell.execute_reply": "2025-06-05T12:14:43.528671Z"
    },
    "papermill": {
     "duration": 12.390858,
     "end_time": "2025-06-05T12:14:43.530711",
     "exception": false,
     "start_time": "2025-06-05T12:14:31.139853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27999a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.539450Z",
     "iopub.status.busy": "2025-06-05T12:14:43.539130Z",
     "iopub.status.idle": "2025-06-05T12:14:43.556397Z",
     "shell.execute_reply": "2025-06-05T12:14:43.555813Z"
    },
    "papermill": {
     "duration": 0.022701,
     "end_time": "2025-06-05T12:14:43.557381",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.534680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define class for scale mixture gaussian prior\n",
    "class ScaleMixtureGaussian:                               \n",
    "    def __init__(self, mixture_weight, stddev_1, stddev_2):\n",
    "        super().__init__()\n",
    "        # mixture_weight is the weight for the first gaussian\n",
    "        self.mixture_weight = mixture_weight\n",
    "        # stddev_1 and stddev_2 are the standard deviations for the two gaussians\n",
    "        self.stddev_1 = stddev_1\n",
    "        self.stddev_2 = stddev_2\n",
    "        # create two normal distributions with the specified standard deviations\n",
    "        self.gaussian1 = torch.distributions.Normal(0,stddev_1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,stddev_2)\n",
    "\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(x))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(x))\n",
    "        return (torch.log(self.mixture_weight * prob1 + (1-self.mixture_weight) * prob2)).sum()\n",
    "    \n",
    "# define class for gaussian node\n",
    "class GaussianNode:\n",
    "    def __init__(self, mean, rho_param):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.rho_param = rho_param\n",
    "        self.normal = torch.distributions.Normal(0,1)\n",
    "    \n",
    "    # Calculate the standard deviation from the rho parameter\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho_param))\n",
    "\n",
    "    # Sample from the Gaussian node\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.rho_param.size()).cuda()\n",
    "        return self.mean + self.sigma() * epsilon\n",
    "    \n",
    "    # Calculate the KL divergence between the prior and the variational posterior\n",
    "    def log_prob(self, x):\n",
    "        return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma()) - ((x - self.mean) ** 2) / (2 * self.sigma() ** 2)).sum()\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, mu_init, rho_init, prior_init):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the parameters for the weights and biases\n",
    "        self.weight_mean = nn.Parameter(torch.empty(out_features, in_features).uniform_(*mu_init))\n",
    "        self.weight_rho_param = nn.Parameter(torch.empty(out_features, in_features).uniform_(*rho_init))\n",
    "        self.weight = GaussianNode(self.weight_mean, self.weight_rho_param)\n",
    "\n",
    "        self.bias_mean = nn.Parameter(torch.empty(out_features).uniform_(*mu_init))\n",
    "        self.bias_rho_param = nn.Parameter(torch.empty(out_features).uniform_(*rho_init))\n",
    "        self.bias = GaussianNode(self.bias_mean, self.bias_rho_param)\n",
    "        \n",
    "        self.weight_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n",
    "        self.bias_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n",
    "\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight.sample()\n",
    "        bias = self.bias.sample()\n",
    "\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "        self.input_shape = model_params['input_shape']\n",
    "        self.classes = model_params['classes']\n",
    "        self.batch_size = model_params['batch_size']\n",
    "        self.hidden_units = model_params['hidden_units']\n",
    "        self.experiment = model_params['experiment']\n",
    "        self.mu_init = model_params['mu_init']\n",
    "        self.rho_init = model_params['rho_init']\n",
    "        self.prior_init = model_params['prior_init']\n",
    "\n",
    "        self.fc1 = BayesianLinear(self.input_shape, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n",
    "        self.fc1_activation = nn.ReLU()\n",
    "        self.fc2 = BayesianLinear(self.hidden_units, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n",
    "        self.fc2_activation = nn.ReLU()\n",
    "        self.fc3 = BayesianLinear(self.hidden_units, self.classes, self.mu_init, self.rho_init, self.prior_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.experiment == 'classification':\n",
    "            x = x.view(-1, self.input_shape) # Flatten images\n",
    "        x = self.fc1_activation(self.fc1(x))\n",
    "        x = self.fc2_activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        return self.fc1.log_prior + self.fc2.log_prior + self.fc3.log_prior\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        return self.fc1.log_variational_posterior + self.fc2.log_variational_posterior + self.fc3.log_variational_posterior\n",
    "\n",
    "\n",
    "    def get_nll(self, outputs, target, sigma=1.):\n",
    "        if self.experiment == 'regression': #  -(.5 * (target - outputs) ** 2).sum()\n",
    "            nll = -torch.distributions.Normal(outputs, sigma).log_prob(target).sum()\n",
    "        elif self.experiment == 'classification':\n",
    "            nll = nn.CrossEntropyLoss(reduction='sum')(outputs, target)\n",
    "        return nll\n",
    "\n",
    "    def sample_elbo(self, x, target, beta, samples, sigma=1.):\n",
    "        log_prior = torch.zeros(1).to(device)\n",
    "        log_variational_posterior = torch.zeros(1).to(device)\n",
    "        negative_log_likelihood = torch.zeros(1).to(device)\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = self.forward(x)\n",
    "            log_prior += self.log_prior()\n",
    "            log_variational_posterior += self.log_variational_posterior()\n",
    "            negative_log_likelihood += self.get_nll(output, target, sigma)\n",
    "\n",
    "        log_prior = beta*(log_prior / samples)\n",
    "        log_variational_posterior = beta*(log_variational_posterior / samples) \n",
    "        negative_log_likelihood = negative_log_likelihood / samples\n",
    "        loss = log_variational_posterior - log_prior + negative_log_likelihood\n",
    "        return loss, log_prior, log_variational_posterior, negative_log_likelihood    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b153a860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.565196Z",
     "iopub.status.busy": "2025-06-05T12:14:43.564937Z",
     "iopub.status.idle": "2025-06-05T12:14:43.570278Z",
     "shell.execute_reply": "2025-06-05T12:14:43.569605Z"
    },
    "papermill": {
     "duration": 0.010589,
     "end_time": "2025-06-05T12:14:43.571397",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.560808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "        self.input_shape = model_params['input_shape']\n",
    "        self.classes = model_params['classes']\n",
    "        self.batch_size = model_params['batch_size']\n",
    "        self.hidden_units = model_params['hidden_units']\n",
    "        self.experiment = model_params['experiment']\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_shape, self.hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_units, self.hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_units, self.classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.experiment == 'classification':\n",
    "            x = x.view(-1, self.input_shape) # Flatten images\n",
    "        \n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "# class MLP_Dropout(nn.Module):\n",
    "#     def __init__(self, model_params):\n",
    "#         super().__init__()\n",
    "#         self.input_shape = model_params['input_shape']\n",
    "#         self.classes = model_params['classes']\n",
    "#         self.batch_size = model_params['batch_size']\n",
    "#         self.hidden_units = model_params['hidden_units']\n",
    "#         self.experiment = model_params['experiment']\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(self.input_shape, self.hidden_units),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(self.hidden_units, self.hidden_units),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(self.hidden_units, self.classes))\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         if self.experiment == 'classification':\n",
    "#             x = x.view(-1, self.input_shape) # Flatten images\n",
    "       \n",
    "#         x = self.net(x)\n",
    "#         return x\n",
    "\n",
    "#     def enable_dropout(self):\n",
    "#         ''' Enable the dropout layers during test-time '''\n",
    "#         for m in self.modules():\n",
    "#             if m.__class__.__name__.startswith('Dropout'):\n",
    "#                 m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1415ff5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.579348Z",
     "iopub.status.busy": "2025-06-05T12:14:43.579140Z",
     "iopub.status.idle": "2025-06-05T12:14:43.584970Z",
     "shell.execute_reply": "2025-06-05T12:14:43.584309Z"
    },
    "papermill": {
     "duration": 0.01097,
     "end_time": "2025-06-05T12:14:43.585947",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.574977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegConfig:\n",
    "    # save_dir = './saved_models'\n",
    "    train_size = 1024\n",
    "    batch_size = 128\n",
    "    lr = 1e-3\n",
    "    epochs = 100 #1000\n",
    "    train_samples = 5                   # number of train samples for MC gradients\n",
    "    test_samples = 10                   # number of test samples for MC averaging\n",
    "    num_test_points = 400               # number of test points\n",
    "    experiment = 'regression'\n",
    "    hidden_units = 400                  # number of hidden units\n",
    "    noise_tolerance = .1                # log likelihood sigma\n",
    "    mu_init = [-0.2, 0.2]               # range for mean \n",
    "    rho_init = [-5, -4]                 # range for rho_param\n",
    "    prior_init = [0.5, -0, -6]        # mixture weight, log(stddev_1), log(stddev_2)\n",
    "   \n",
    "\n",
    "class RLConfig:\n",
    "    data_dir = '/kaggle/input/mushroom/agaricus-lepiota.data' \n",
    "    batch_size = 64\n",
    "    num_batches = 64\n",
    "    buffer_size = batch_size * num_batches  # buffer to track latest batch of mushrooms\n",
    "    lr = 1e-4\n",
    "    training_steps = 5000 # 50000\n",
    "    experiment = 'regression'\n",
    "    hidden_units = 100                      # number of hidden units\n",
    "    mu_init = [-0.2, 0.2]                   # range for mean \n",
    "    rho_init = [-5, -4]                     # range for rho_param\n",
    "    prior_init = [0.5, -0, -6]              # mixture weight, log(stddev_1), log(stddev_2)\n",
    "\n",
    "class ClassConfig:\n",
    "    batch_size = 128\n",
    "    lr = 1e-3 # 1e-5 fa schifo, 1e-4 parte da 8% errore, 1e-3 parte da 5%\n",
    "    epochs = 1 #600\n",
    "    hidden_units = 1200\n",
    "    experiment = 'classification'\n",
    "    dropout = False\n",
    "    train_samples = 1 # 10 è troppo lento\n",
    "    test_samples = 10\n",
    "    x_shape = 28 * 28                       # x shape\n",
    "    classes = 10                            # number of output classes\n",
    "    mu_init = [-0.2, 0.2]                   # range for mean \n",
    "    rho_init = [-5, -4]                     # range for rho_param\n",
    "    prior_init = [0.5, -0, -8]             # mixture weight, log(stddev_1), log(stddev_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c601dda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.594090Z",
     "iopub.status.busy": "2025-06-05T12:14:43.593481Z",
     "iopub.status.idle": "2025-06-05T12:14:43.601829Z",
     "shell.execute_reply": "2025-06-05T12:14:43.601104Z"
    },
    "papermill": {
     "duration": 0.013353,
     "end_time": "2025-06-05T12:14:43.602893",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.589540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y # vedere\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def read_data_rl(data_dir):\n",
    "    df = pd.read_csv(data_dir, sep=',', header=None)\n",
    "    df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "    X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "    Y = df['class']\n",
    "\n",
    "    # transform to one-hot encoding\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(Y)\n",
    "    Y_encoded = label_encoder.transform(Y)\n",
    "    oh_X = X.copy()\n",
    "    for feature in X.columns:\n",
    "        label_encoder.fit(X[feature])\n",
    "        oh_X[feature] = label_encoder.transform(X[feature])\n",
    "\n",
    "    oh_encoder = preprocessing.OneHotEncoder()\n",
    "    oh_encoder.fit(oh_X)\n",
    "    oh_X = oh_encoder.transform(oh_X).toarray()\n",
    "\n",
    "    return oh_X, Y_encoded\n",
    "\n",
    "def create_data_reg(train_size):\n",
    "    np.random.seed(0)\n",
    "    xs = np.random.uniform(low=0., high=0.6, size=train_size)\n",
    "    \n",
    "    eps = np.random.normal(loc=0., scale=0.02, size=[train_size])\n",
    "\n",
    "    ys = xs + 0.3 * np.sin(2*np.pi * (xs + eps)) + 0.3 * np.sin(4*np.pi * (xs + eps)) + eps\n",
    "\n",
    "    xs = torch.from_numpy(xs).reshape(-1,1).float()\n",
    "    ys = torch.from_numpy(ys).reshape(-1,1).float()\n",
    "\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ca7d06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.610678Z",
     "iopub.status.busy": "2025-06-05T12:14:43.610137Z",
     "iopub.status.idle": "2025-06-05T12:14:43.613606Z",
     "shell.execute_reply": "2025-06-05T12:14:43.613106Z"
    },
    "papermill": {
     "duration": 0.008405,
     "end_time": "2025-06-05T12:14:43.614652",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.606247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_bnn_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#         'mu_init': config.mu_init,\n",
    "#         'rho_init': config.rho_init,\n",
    "#         'prior_init': config.prior_init\n",
    "#     }\n",
    "#     model = BayesianNetwork(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()\n",
    "\n",
    "# def load_mlp_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#     }\n",
    "#     model = MLP(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()\n",
    "\n",
    "# def load_dropout_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#         'dropout': True\n",
    "#     }\n",
    "#     model = MLP_Dropout(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a60acf53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.622194Z",
     "iopub.status.busy": "2025-06-05T12:14:43.621945Z",
     "iopub.status.idle": "2025-06-05T12:14:43.627059Z",
     "shell.execute_reply": "2025-06-05T12:14:43.626619Z"
    },
    "papermill": {
     "duration": 0.009917,
     "end_time": "2025-06-05T12:14:43.628093",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.618176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_regression_plot(X_test, y_test, train_ds):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    plt.plot(X_test, np.median(y_test, axis=0), label='Median Posterior Predictive')\n",
    "    \n",
    "    # Range\n",
    "    plt.fill_between(\n",
    "        X_test.reshape(-1), \n",
    "        np.percentile(y_test, 0, axis=0), \n",
    "        np.percentile(y_test, 100, axis=0), \n",
    "        alpha = 0.2, color='orange', label='Range') #color='blue',\n",
    "    \n",
    "    # interquartile range\n",
    "    plt.fill_between(\n",
    "        X_test.reshape(-1), \n",
    "        np.percentile(y_test, 25, axis=0), \n",
    "        np.percentile(y_test, 75, axis=0), \n",
    "        alpha = 0.4,  label='Interquartile Range') #color='red',\n",
    "    \n",
    "    plt.scatter(train_ds.dataset.X, train_ds.dataset.y, label='Training data', marker='x', alpha=0.5, color='k', s=2)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.xlim([-0.6, 1.4])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b6da4d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.635697Z",
     "iopub.status.busy": "2025-06-05T12:14:43.635482Z",
     "iopub.status.idle": "2025-06-05T12:14:43.645142Z",
     "shell.execute_reply": "2025-06-05T12:14:43.644447Z"
    },
    "papermill": {
     "duration": 0.014768,
     "end_time": "2025-06-05T12:14:43.646286",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.631518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Classification():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.n_samples = parameters['train_samples']\n",
    "        self.test_samples = parameters['test_samples']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.classes = parameters['classes']\n",
    "        self.mu_init = parameters['mu_init']\n",
    "        self.rho_init = parameters['rho_init']\n",
    "        self.prior_init = parameters['prior_init']\n",
    "        self.best_acc = 0.\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.classes,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment,\n",
    "            'mu_init': self.mu_init,\n",
    "            'rho_init': self.rho_init,\n",
    "            'prior_init': self.prior_init,\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n",
    "        # print(f'Classification Task {self.label} Parameters: ')\n",
    "        # print(f'number of samples: {self.n_samples}')\n",
    "        # print(\"BNN Parameters: \")\n",
    "        # print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for idx, (x, y) in enumerate(tqdm(train_data)):\n",
    "            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)            \n",
    "            net_loss = self.loss_info[0]\n",
    "            net_loss.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = torch.zeros(size=[self.batch_size, self.classes]).to(device)\n",
    "        for _ in torch.arange(self.test_samples):\n",
    "            out = torch.nn.Softmax(dim=1)(self.net(X))\n",
    "            probs = probs + out / self.test_samples\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds, probs\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader):\n",
    "                X, y = data\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds, _ = self.predict(X)\n",
    "                total += self.batch_size\n",
    "                correct += (preds == y).sum().item()\n",
    "        self.acc = correct / total\n",
    "        print(f'validation accuracy: {self.acc}')  \n",
    "        return self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d834acd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.654417Z",
     "iopub.status.busy": "2025-06-05T12:14:43.654006Z",
     "iopub.status.idle": "2025-06-05T12:14:43.663336Z",
     "shell.execute_reply": "2025-06-05T12:14:43.662636Z"
    },
    "papermill": {
     "duration": 0.01452,
     "end_time": "2025-06-05T12:14:43.664330",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.649810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Classification():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.classes = parameters['classes']\n",
    "        self.best_acc = 0.\n",
    "        self.dropout = parameters['dropout']\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.classes,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment,\n",
    "            'dropout': self.dropout,\n",
    "        }\n",
    "        if self.dropout:\n",
    "            self.net = MLP_Dropout(model_params).to(device)\n",
    "            print('MLP Dropout Parameters: ')\n",
    "            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "        else:\n",
    "            self.net = MLP(model_params).to(device)\n",
    "            print('MLP Parameters: ')\n",
    "            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "        self.optimiser = torch.optim.SGD(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for _, (x, y) in enumerate(tqdm(train_data)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = torch.nn.functional.cross_entropy(self.net(x), y, reduction='sum')\n",
    "            self.loss_info.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = torch.nn.Softmax(dim=1)(self.net(X))\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds, probs\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader):\n",
    "                X, y = data\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds, _ = self.predict(X)\n",
    "                total += self.batch_size\n",
    "                correct += (preds == y).sum().item()\n",
    "        self.acc = correct / total\n",
    "        print(f'{self.label} validation accuracy: {self.acc}') \n",
    "        return self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afffb6d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:14:43.672059Z",
     "iopub.status.busy": "2025-06-05T12:14:43.671535Z",
     "iopub.status.idle": "2025-06-05T12:15:18.936454Z",
     "shell.execute_reply": "2025-06-05T12:15:18.935606Z"
    },
    "papermill": {
     "duration": 35.270693,
     "end_time": "2025-06-05T12:15:18.938175",
     "exception": false,
     "start_time": "2025-06-05T12:14:43.667482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.7MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.18MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.9MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.57MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:15<00:00, 24.86it/s]\n",
      "100%|██████████| 78/78 [00:17<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.9425080128205128\n",
      "Valid Error 5.749 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def class_trainer():\n",
    "    config = ClassConfig\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x * 255 / 126.),  # divide as in paper, * 255 gives better results\n",
    "        ])\n",
    "\n",
    "    train_data = datasets.MNIST(\n",
    "            root='data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform)\n",
    "    test_data = datasets.MNIST(\n",
    "            root='data',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transform)\n",
    "\n",
    "    valid_size = 1 / 6\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(valid_size * num_train)\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            sampler=train_sampler,\n",
    "            drop_last=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config.batch_size,\n",
    "            sampler=valid_sampler,\n",
    "            drop_last=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=True)\n",
    "\n",
    "    params = {\n",
    "        'lr': config.lr,\n",
    "        'hidden_units': config.hidden_units,\n",
    "        'experiment': config.experiment,\n",
    "        'dropout': config.dropout,\n",
    "        'batch_size': config.batch_size,\n",
    "        'epochs': config.epochs,\n",
    "        'x_shape': config.x_shape,\n",
    "        'classes': config.classes,\n",
    "        'num_batches': len(train_loader),\n",
    "        'train_samples': config.train_samples,\n",
    "        'test_samples': config.test_samples,\n",
    "        'mu_init': config.mu_init,\n",
    "        'rho_init': config.rho_init,\n",
    "        'prior_init': config.prior_init,\n",
    "    }\n",
    "\n",
    "    model = BNN_Classification('bnn_classification', {**params})\n",
    "    #model = MLP_Classification('mlp_classification', {**params})\n",
    "    \n",
    "    epochs = config.epochs\n",
    "    for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            model.train_step(train_loader)\n",
    "            valid_acc = model.evaluate(valid_loader)\n",
    "            # test_acc = model.evaluate(test_loader)\n",
    "            print('Valid Error', round(100 * (1 - valid_acc), 3), '%',)\n",
    "            model.scheduler.step()\n",
    "            if model.acc > model.best_acc:\n",
    "                model.best_acc = model.acc\n",
    "                # torch.save(model.net.state_dict(), model.save_model_path)\n",
    "\n",
    "\n",
    "#spezzare tuning con lr=1e-3 e 1e-4\n",
    "\n",
    "\n",
    "class_trainer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b201f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:18.965099Z",
     "iopub.status.busy": "2025-06-05T12:15:18.964585Z",
     "iopub.status.idle": "2025-06-05T12:15:18.968957Z",
     "shell.execute_reply": "2025-06-05T12:15:18.968313Z"
    },
    "papermill": {
     "duration": 0.018433,
     "end_time": "2025-06-05T12:15:18.970088",
     "exception": false,
     "start_time": "2025-06-05T12:15:18.951655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import SubsetRandomSampler\n",
    "# from torchvision import datasets, transforms\n",
    "# from itertools import product\n",
    "\n",
    "# def class_trainer(config):\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Lambda(lambda x: x * 255. / 126.),\n",
    "#     ])\n",
    "\n",
    "#     train_data = datasets.MNIST(\n",
    "#         root='data',\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transform)\n",
    "#     test_data = datasets.MNIST(\n",
    "#         root='data',\n",
    "#         train=False,\n",
    "#         download=True,\n",
    "#         transform=transform)\n",
    "\n",
    "#     valid_size = 1 / 6\n",
    "#     num_train = len(train_data)\n",
    "#     indices = list(range(num_train))\n",
    "#     split = int(valid_size * num_train)\n",
    "#     train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "#     train_sampler = SubsetRandomSampler(train_idx)\n",
    "#     valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         sampler=train_sampler,\n",
    "#         drop_last=True)\n",
    "#     valid_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         sampler=valid_sampler,\n",
    "#         drop_last=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         shuffle=False,\n",
    "#         drop_last=True)\n",
    "\n",
    "#     params = {\n",
    "#         'lr': config['lr'],\n",
    "#         'hidden_units': config['hidden_units'],\n",
    "#         'experiment': config['experiment'],\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'epochs': config['epochs'],\n",
    "#         'x_shape': config['x_shape'],\n",
    "#         'classes': config['classes'],\n",
    "#         'num_batches': len(train_loader),\n",
    "#         'train_samples': config['train_samples'],\n",
    "#         'test_samples': config['test_samples'],\n",
    "#         'mu_init': config['mu_init'],\n",
    "#         'rho_init': config['rho_init'],\n",
    "#         'prior_init': config['prior_init'],\n",
    "#     }\n",
    "\n",
    "#     model = BNN_Classification('bnn_classification', {**params, 'dropout': False})\n",
    "\n",
    "#     best_val_acc = 0\n",
    "#     for epoch in range(config['epochs']):\n",
    "#         print(f'Epoch {epoch + 1}/{config[\"epochs\"]}')\n",
    "#         model.train_step(train_loader)\n",
    "#         valid_acc = model.evaluate(valid_loader)\n",
    "#         print('Valid Error', round(100 * (1 - valid_acc), 3), '%')\n",
    "#         model.scheduler.step()\n",
    "#         if model.acc > model.best_acc:\n",
    "#             model.best_acc = model.acc\n",
    "#             best_val_acc = valid_acc\n",
    "#             # torch.save(model.net.state_dict(), \"best_model.pt\")\n",
    "\n",
    "#     return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d4e8888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:18.995889Z",
     "iopub.status.busy": "2025-06-05T12:15:18.995415Z",
     "iopub.status.idle": "2025-06-05T12:15:18.999384Z",
     "shell.execute_reply": "2025-06-05T12:15:18.998531Z"
    },
    "papermill": {
     "duration": 0.017966,
     "end_time": "2025-06-05T12:15:19.000526",
     "exception": false,
     "start_time": "2025-06-05T12:15:18.982560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_grid_search():\n",
    "#     # Define the grid\n",
    "#     param_grid = {\n",
    "#         'lr': [1e-3, 1e-4],\n",
    "#         'batch_size': [64, 128],\n",
    "#         'hidden_units': [128, 256],\n",
    "#     }\n",
    "\n",
    "#     # Fixed config options\n",
    "#     base_config = {\n",
    "#         'experiment': 'grid_search',\n",
    "#         'epochs': 5,\n",
    "#         'x_shape': (1, 28, 28),\n",
    "#         'classes': 10,\n",
    "#         'train_samples': 60000,\n",
    "#         'test_samples': 10000,\n",
    "#         'mu_init': 0,\n",
    "#         'rho_init': -3,\n",
    "#         'prior_init': 0,\n",
    "#     }\n",
    "\n",
    "#     best_acc = 0\n",
    "#     best_params = None\n",
    "\n",
    "#     # Iterate over all combinations\n",
    "#     for lr, batch_size, hidden_units in product(param_grid['lr'], param_grid['batch_size'], param_grid['hidden_units']):\n",
    "#         config = {\n",
    "#             **base_config,\n",
    "#             'lr': lr,\n",
    "#             'batch_size': batch_size,\n",
    "#             'hidden_units': hidden_units\n",
    "#         }\n",
    "\n",
    "#         print(f\"\\nRunning with config: {config}\")\n",
    "#         acc = class_trainer(config)\n",
    "\n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_params = config\n",
    "\n",
    "#     print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
    "#     print(f\"Best Config: {best_params}\")\n",
    "\n",
    "# run_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3473348e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.025720Z",
     "iopub.status.busy": "2025-06-05T12:15:19.025519Z",
     "iopub.status.idle": "2025-06-05T12:15:19.033976Z",
     "shell.execute_reply": "2025-06-05T12:15:19.033497Z"
    },
    "papermill": {
     "duration": 0.022275,
     "end_time": "2025-06-05T12:15:19.034935",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.012660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Regression():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.n_samples = parameters['train_samples']\n",
    "        self.test_samples = parameters['test_samples']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.y_shape = parameters['y_shape']\n",
    "        self.noise_tol = parameters['noise_tolerance']\n",
    "        self.lr = parameters['lr']\n",
    "        self.best_loss = np.inf\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.y_shape,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment'],\n",
    "            'mu_init': parameters['mu_init'],\n",
    "            'rho_init': parameters['rho_init'],\n",
    "            'prior_init': parameters['prior_init']\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=500, gamma=0.5)\n",
    "        # print(f'Regression Task {self.label} Parameters: ')\n",
    "        # print(f'number of samples: {self.n_samples}, noise tolerance: {self.noise_tol}')\n",
    "        print(\"BNN Parameters: \")\n",
    "        print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, mu_init: {parameters[\"mu_init\"]}, rho_init: {parameters[\"rho_init\"]}, prior_init: {parameters[\"prior_init\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for idx, (x, y) in enumerate(train_data):\n",
    "            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples, sigma=self.noise_tol)\n",
    "            net_loss = self.loss_info[0]\n",
    "            net_loss.backward()\n",
    "            self.optimiser.step()\n",
    "        self.epoch_loss = net_loss.item()\n",
    "\n",
    "    def evaluate(self, x_test):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test = np.zeros((self.test_samples, x_test.shape[0]))\n",
    "            for s in range(self.test_samples):\n",
    "                tmp = self.net(x_test.to(device)).detach().cpu().numpy()\n",
    "                y_test[s,:] = tmp.reshape(-1)\n",
    "            return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5744d03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.059491Z",
     "iopub.status.busy": "2025-06-05T12:15:19.059305Z",
     "iopub.status.idle": "2025-06-05T12:15:19.066469Z",
     "shell.execute_reply": "2025-06-05T12:15:19.065937Z"
    },
    "papermill": {
     "duration": 0.02084,
     "end_time": "2025-06-05T12:15:19.067453",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.046613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Regression():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.y_shape = parameters['y_shape']\n",
    "        self.best_loss = np.inf\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.y_shape,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment\n",
    "        }\n",
    "        self.net = MLP(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(\"MLP Parameters: \")\n",
    "        print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for _, (x, y) in enumerate(train_data):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = torch.nn.functional.mse_loss(self.net(x), y, reduction='sum')\n",
    "            self.loss_info.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "        self.epoch_loss = self.loss_info.item()\n",
    "\n",
    "    def evaluate(self, x_test):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test = self.net(x_test.to(device)).detach().cpu().numpy()\n",
    "            return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f67de2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.092953Z",
     "iopub.status.busy": "2025-06-05T12:15:19.092433Z",
     "iopub.status.idle": "2025-06-05T12:15:19.098572Z",
     "shell.execute_reply": "2025-06-05T12:15:19.097912Z"
    },
    "papermill": {
     "duration": 0.020308,
     "end_time": "2025-06-05T12:15:19.099546",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.079238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reg_trainer():\n",
    "    config = RegConfig\n",
    "    X, Y = create_data_reg(train_size=config.train_size)\n",
    "    train_loader = PrepareData(X, Y)\n",
    "    train_loader = DataLoader(train_loader, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    params = {\n",
    "        'lr': config.lr,\n",
    "        'hidden_units': config.hidden_units,\n",
    "        'experiment': config.experiment,\n",
    "        'batch_size': config.batch_size,\n",
    "        'num_batches': len(train_loader),\n",
    "        'x_shape': X.shape[1],\n",
    "        'y_shape': Y.shape[1],\n",
    "        'train_samples': config.train_samples,\n",
    "        'test_samples': config.test_samples,\n",
    "        'noise_tolerance': config.noise_tolerance,\n",
    "        'mu_init': config.mu_init,\n",
    "        'rho_init': config.rho_init,\n",
    "        'prior_init': config.prior_init,\n",
    "    }\n",
    "\n",
    "    model = BNN_Regression('bnn_regression', {**params})\n",
    "    #model = MLP_Regression('mlp_regression', {**params})\n",
    "\n",
    "    epochs = config.epochs\n",
    "    print(f\"Initialising training on {device}...\")\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "        model.train_step(train_loader)\n",
    "        model.scheduler.step()\n",
    "        # save best model\n",
    "        if model.epoch_loss < model.best_loss:\n",
    "            model.best_loss = model.epoch_loss\n",
    "            # torch.save(model.net.state_dict(), model.save_model_path)\n",
    "\n",
    "    # evaluate\n",
    "    print(\"Evaluating and generating plots...\")\n",
    "    x_test = torch.linspace(-2., 2, config.num_test_points).reshape(-1, 1)\n",
    "    \n",
    "    # model.net.load_state_dict(torch.load(model.save_model_path, map_location=torch.device(device)))\n",
    "    y_test = model.evaluate(x_test)\n",
    "    \n",
    "    #create_regression_plot(x_test.cpu().numpy(), y_test.reshape(1, -1), train_loader) #per mlp regression\n",
    "   \n",
    "    create_regression_plot(x_test.cpu().numpy(), y_test, train_loader)\n",
    "\n",
    "\n",
    "#reg_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069a934d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.124636Z",
     "iopub.status.busy": "2025-06-05T12:15:19.124422Z",
     "iopub.status.idle": "2025-06-05T12:15:19.135649Z",
     "shell.execute_reply": "2025-06-05T12:15:19.134997Z"
    },
    "papermill": {
     "duration": 0.024821,
     "end_time": "2025-06-05T12:15:19.136632",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.111811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self, label, bandit_params, x, y):\n",
    "        self.n_samples = bandit_params['n_samples']\n",
    "        self.buffer_size = bandit_params['buffer_size']\n",
    "        self.batch_size = bandit_params['batch_size']\n",
    "        self.num_batches = bandit_params['num_batches']\n",
    "        self.lr = bandit_params['lr']\n",
    "        self.epsilon = bandit_params['epsilon']\n",
    "        self.cumulative_regrets = [0]\n",
    "        self.buffer_x, self.buffer_y = [], []\n",
    "        self.x, self.y = x, y\n",
    "        self.label = label\n",
    "        self.init_net(bandit_params)\n",
    "        self.tp, self.tn, self.fp, self.fn = 0, 0, 0, 0\n",
    "\n",
    "    def get_agent_reward(self, eaten, edible):\n",
    "        if not eaten:\n",
    "            return 0\n",
    "        if eaten and edible:\n",
    "            return 5\n",
    "        elif eaten and not edible:\n",
    "            return 5 if np.random.rand() > 0.5 else -35\n",
    "\n",
    "    def get_oracle_reward(self, edible):\n",
    "        return 5*edible \n",
    "\n",
    "    def take_action(self, mushroom):\n",
    "        context, edible = self.x[mushroom], self.y[mushroom]\n",
    "        eat_tuple = torch.FloatTensor(np.concatenate((context, [1, 0]))).unsqueeze(0).to(device)\n",
    "        reject_tuple = torch.FloatTensor(np.concatenate((context, [0, 1]))).unsqueeze(0).to(device)\n",
    "\n",
    "        # evaluate reward for actions\n",
    "        with torch.no_grad():\n",
    "            self.net.eval()\n",
    "            reward_eat = sum([self.net(eat_tuple) for _ in range(self.n_samples)]).item()\n",
    "            reward_reject = sum([self.net(reject_tuple) for _ in range(self.n_samples)]).item()\n",
    "\n",
    "        eat = reward_eat > reward_reject\n",
    "        # epsilon-greedy agent\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            eat = (np.random.rand() < 0.5)\n",
    "        agent_reward = self.get_agent_reward(eat, edible)\n",
    "\n",
    "        # record bandit action\n",
    "        if edible and eat:\n",
    "            self.tp += 1\n",
    "        elif edible and not eat:\n",
    "            self.fn += 1\n",
    "        elif not edible and eat:\n",
    "            self.fp += 1\n",
    "        else:\n",
    "            self.tn += 1\n",
    "\n",
    "        # record context, action, reward\n",
    "        action = torch.Tensor([1, 0] if eat else [0, 1])\n",
    "        self.buffer_x.append(np.concatenate((context, action)))\n",
    "        self.buffer_y.append(agent_reward)\n",
    "\n",
    "        # calculate regret\n",
    "        regret = self.get_oracle_reward(edible) - agent_reward\n",
    "        self.cumulative_regrets.append(self.cumulative_regrets[-1]+regret)\n",
    "\n",
    "    def update(self, mushroom):\n",
    "        self.take_action(mushroom)\n",
    "        l = len(self.buffer_x)\n",
    "\n",
    "        if l <= self.batch_size:\n",
    "            idx_pool = int(self.batch_size//l + 1)*list(range(l))\n",
    "            idx_pool = np.random.permutation(idx_pool[-self.batch_size:])\n",
    "        elif l > self.batch_size and l < self.buffer_size:\n",
    "            idx_pool = int(l//self.batch_size)*self.batch_size\n",
    "            idx_pool = np.random.permutation(list(range(l))[-idx_pool:])\n",
    "        else:\n",
    "            idx_pool = np.random.permutation(list(range(l))[-self.buffer_size:])\n",
    "\n",
    "        context_pool = torch.Tensor([self.buffer_x[i] for i in idx_pool]).to(device)\n",
    "        value_pool = torch.Tensor([self.buffer_y[i] for i in idx_pool]).to(device)\n",
    "        \n",
    "        for i in range(0, len(idx_pool), self.batch_size):\n",
    "            self.loss_info = self.loss_step(context_pool[i:i+self.batch_size], value_pool[i:i+self.batch_size], i//self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb135884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.161648Z",
     "iopub.status.busy": "2025-06-05T12:15:19.161446Z",
     "iopub.status.idle": "2025-06-05T12:15:19.170367Z",
     "shell.execute_reply": "2025-06-05T12:15:19.169709Z"
    },
    "papermill": {
     "duration": 0.022485,
     "end_time": "2025-06-05T12:15:19.171389",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.148904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Bandit(Bandit):\n",
    "    def __init__(self, label, *args):\n",
    "        super().__init__(label, *args)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x.shape[1]+2,\n",
    "            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment'],\n",
    "            'mu_init': parameters['mu_init'],\n",
    "            'rho_init': parameters['rho_init'],\n",
    "            'prior_init': parameters['prior_init']\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(\"BNN Parameters: \")\n",
    "        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def loss_step(self, x, y, batch_id):\n",
    "        beta = 2 ** (self.num_batches - (batch_id + 1)) / (2 ** self.num_batches - 1) \n",
    "        self.net.train()\n",
    "        self.net.zero_grad()\n",
    "        loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)\n",
    "        net_loss = loss_info[0]\n",
    "        net_loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return loss_info\n",
    "\n",
    "class Greedy_Bandit(Bandit):\n",
    "    def __init__(self, label, *args):\n",
    "        super().__init__(label, *args)\n",
    "        self.writer = SummaryWriter(comment=f\"_{label}_training\"),\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x.shape[1]+2,\n",
    "            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment']\n",
    "        }\n",
    "        self.net = MLP(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(f'Bandit {self.label} Parameters: ')\n",
    "        print(f'buffer_size: {self.buffer_size}, batch size: {self.batch_size}, number of samples: {self.n_samples}, epsilon: {self.epsilon}')\n",
    "        print(\"MLP Parameters: \")\n",
    "        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def loss_step(self, x, y, batch_id):\n",
    "        self.net.train()\n",
    "        self.net.zero_grad()\n",
    "        net_loss = torch.nn.functional.mse_loss(self.net(x).squeeze(), y, reduction='sum')\n",
    "        net_loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return net_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2b139d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T12:15:19.195614Z",
     "iopub.status.busy": "2025-06-05T12:15:19.195411Z",
     "iopub.status.idle": "2025-06-05T12:15:19.200811Z",
     "shell.execute_reply": "2025-06-05T12:15:19.200198Z"
    },
    "papermill": {
     "duration": 0.018663,
     "end_time": "2025-06-05T12:15:19.201777",
     "exception": false,
     "start_time": "2025-06-05T12:15:19.183114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rl_trainer():\n",
    "    config = RLConfig\n",
    "    X, Y = read_data_rl(config.data_dir)\n",
    "\n",
    "    params = {\n",
    "        'buffer_size': config.buffer_size,\n",
    "        'batch_size': config.batch_size,\n",
    "        'num_batches': config.num_batches,\n",
    "        'lr': config.lr,\n",
    "        'hidden_units': config.hidden_units,\n",
    "        'experiment': config.experiment,\n",
    "        'mu_init': config.mu_init,\n",
    "        'rho_init': config.rho_init,\n",
    "        'prior_init': config.prior_init\n",
    "    }\n",
    "\n",
    "    bandit = BNN_Bandit('bnn_bandit', {**params, 'n_samples':2, 'epsilon':0}, X, Y)\n",
    "    \n",
    "    training_steps = config.training_steps\n",
    "    print(f\"Initialising training on {device}...\")\n",
    "    training_data_len = len(X)\n",
    "    for step in tqdm(range(training_steps)):\n",
    "        mushroom = np.random.randint(training_data_len)\n",
    "        bandit.update(mushroom)\n",
    "        bandit.scheduler.step()\n",
    "\n",
    "    # Plot cumulative regret\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(bandit.cumulative_regrets, label='Cumulative Regret')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    plt.title('Cumulative Regret over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#rl_trainer()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7591461,
     "sourceId": 12061096,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.404537,
   "end_time": "2025-06-05T12:15:22.113310",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T12:14:26.708773",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
