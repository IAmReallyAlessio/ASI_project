{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac5f72c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:30.364582Z",
     "iopub.status.busy": "2025-06-05T13:14:30.364368Z",
     "iopub.status.idle": "2025-06-05T13:14:41.839589Z",
     "shell.execute_reply": "2025-06-05T13:14:41.838999Z"
    },
    "papermill": {
     "duration": 11.482141,
     "end_time": "2025-06-05T13:14:41.840975",
     "exception": false,
     "start_time": "2025-06-05T13:14:30.358834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c56346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.850311Z",
     "iopub.status.busy": "2025-06-05T13:14:41.849973Z",
     "iopub.status.idle": "2025-06-05T13:14:41.866819Z",
     "shell.execute_reply": "2025-06-05T13:14:41.865945Z"
    },
    "papermill": {
     "duration": 0.022934,
     "end_time": "2025-06-05T13:14:41.868278",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.845344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define class for scale mixture gaussian prior\n",
    "class ScaleMixtureGaussian:                               \n",
    "    def __init__(self, mixture_weight, stddev_1, stddev_2):\n",
    "        super().__init__()\n",
    "        # mixture_weight is the weight for the first gaussian\n",
    "        self.mixture_weight = mixture_weight\n",
    "        # stddev_1 and stddev_2 are the standard deviations for the two gaussians\n",
    "        self.stddev_1 = stddev_1\n",
    "        self.stddev_2 = stddev_2\n",
    "        # create two normal distributions with the specified standard deviations\n",
    "        self.gaussian1 = torch.distributions.Normal(0,stddev_1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0,stddev_2)\n",
    "\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        prob1 = torch.exp(self.gaussian1.log_prob(x))\n",
    "        prob2 = torch.exp(self.gaussian2.log_prob(x))\n",
    "        return (torch.log(self.mixture_weight * prob1 + (1-self.mixture_weight) * prob2)).sum()\n",
    "    \n",
    "# define class for gaussian node\n",
    "class GaussianNode:\n",
    "    def __init__(self, mean, rho_param):\n",
    "        super().__init__()\n",
    "        self.mean = mean\n",
    "        self.rho_param = rho_param\n",
    "        self.normal = torch.distributions.Normal(0,1)\n",
    "    \n",
    "    # Calculate the standard deviation from the rho parameter\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho_param))\n",
    "\n",
    "    # Sample from the Gaussian node\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.rho_param.size()).cuda()\n",
    "        return self.mean + self.sigma() * epsilon\n",
    "    \n",
    "    # Calculate the KL divergence between the prior and the variational posterior\n",
    "    def log_prob(self, x):\n",
    "        return (-math.log(math.sqrt(2 * math.pi)) - torch.log(self.sigma()) - ((x - self.mean) ** 2) / (2 * self.sigma() ** 2)).sum()\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, mu_init, rho_init, prior_init):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the parameters for the weights and biases\n",
    "        self.weight_mean = nn.Parameter(torch.empty(out_features, in_features).uniform_(*mu_init))\n",
    "        self.weight_rho_param = nn.Parameter(torch.empty(out_features, in_features).uniform_(*rho_init))\n",
    "        self.weight = GaussianNode(self.weight_mean, self.weight_rho_param)\n",
    "\n",
    "        self.bias_mean = nn.Parameter(torch.empty(out_features).uniform_(*mu_init))\n",
    "        self.bias_rho_param = nn.Parameter(torch.empty(out_features).uniform_(*rho_init))\n",
    "        self.bias = GaussianNode(self.bias_mean, self.bias_rho_param)\n",
    "        \n",
    "        self.weight_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n",
    "        self.bias_prior = ScaleMixtureGaussian(prior_init[0], math.exp(prior_init[1]), math.exp(prior_init[2]))\n",
    "\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.weight.sample()\n",
    "        bias = self.bias.sample()\n",
    "\n",
    "        return nn.functional.linear(x, weight, bias)\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "        self.input_shape = model_params['input_shape']\n",
    "        self.classes = model_params['classes']\n",
    "        self.batch_size = model_params['batch_size']\n",
    "        self.hidden_units = model_params['hidden_units']\n",
    "        self.experiment = model_params['experiment']\n",
    "        self.mu_init = model_params['mu_init']\n",
    "        self.rho_init = model_params['rho_init']\n",
    "        self.prior_init = model_params['prior_init']\n",
    "\n",
    "        self.fc1 = BayesianLinear(self.input_shape, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n",
    "        self.fc1_activation = nn.ReLU()\n",
    "        self.fc2 = BayesianLinear(self.hidden_units, self.hidden_units, self.mu_init, self.rho_init, self.prior_init)\n",
    "        self.fc2_activation = nn.ReLU()\n",
    "        self.fc3 = BayesianLinear(self.hidden_units, self.classes, self.mu_init, self.rho_init, self.prior_init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.experiment == 'classification':\n",
    "            x = x.view(-1, self.input_shape) # Flatten images\n",
    "        x = self.fc1_activation(self.fc1(x))\n",
    "        x = self.fc2_activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        return self.fc1.log_prior + self.fc2.log_prior + self.fc3.log_prior\n",
    "    \n",
    "    def log_variational_posterior(self):\n",
    "        return self.fc1.log_variational_posterior + self.fc2.log_variational_posterior + self.fc3.log_variational_posterior\n",
    "\n",
    "\n",
    "    def get_nll(self, outputs, target, sigma=1.):\n",
    "        if self.experiment == 'regression': #  -(.5 * (target - outputs) ** 2).sum()\n",
    "            nll = -torch.distributions.Normal(outputs, sigma).log_prob(target).sum()\n",
    "        elif self.experiment == 'classification':\n",
    "            nll = nn.CrossEntropyLoss(reduction='sum')(outputs, target)\n",
    "        return nll\n",
    "\n",
    "    def sample_elbo(self, x, target, beta, samples, sigma=1.):\n",
    "        log_prior = torch.zeros(1).to(device)\n",
    "        log_variational_posterior = torch.zeros(1).to(device)\n",
    "        negative_log_likelihood = torch.zeros(1).to(device)\n",
    "\n",
    "        for i in range(samples):\n",
    "            output = self.forward(x)\n",
    "            log_prior += self.log_prior()\n",
    "            log_variational_posterior += self.log_variational_posterior()\n",
    "            negative_log_likelihood += self.get_nll(output, target, sigma)\n",
    "\n",
    "        log_prior = beta*(log_prior / samples)\n",
    "        log_variational_posterior = beta*(log_variational_posterior / samples) \n",
    "        negative_log_likelihood = negative_log_likelihood / samples\n",
    "        loss = log_variational_posterior - log_prior + negative_log_likelihood\n",
    "        return loss, log_prior, log_variational_posterior, negative_log_likelihood    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb2a0b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.878035Z",
     "iopub.status.busy": "2025-06-05T13:14:41.877253Z",
     "iopub.status.idle": "2025-06-05T13:14:41.884065Z",
     "shell.execute_reply": "2025-06-05T13:14:41.883164Z"
    },
    "papermill": {
     "duration": 0.012854,
     "end_time": "2025-06-05T13:14:41.885436",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.872582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, model_params):\n",
    "        super().__init__()\n",
    "        self.input_shape = model_params['input_shape']\n",
    "        self.classes = model_params['classes']\n",
    "        self.batch_size = model_params['batch_size']\n",
    "        self.hidden_units = model_params['hidden_units']\n",
    "        self.experiment = model_params['experiment']\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_shape, self.hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_units, self.hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_units, self.classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.experiment == 'classification':\n",
    "            x = x.view(-1, self.input_shape) # Flatten images\n",
    "        \n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "# class MLP_Dropout(nn.Module):\n",
    "#     def __init__(self, model_params):\n",
    "#         super().__init__()\n",
    "#         self.input_shape = model_params['input_shape']\n",
    "#         self.classes = model_params['classes']\n",
    "#         self.batch_size = model_params['batch_size']\n",
    "#         self.hidden_units = model_params['hidden_units']\n",
    "#         self.experiment = model_params['experiment']\n",
    "\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(self.input_shape, self.hidden_units),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(self.hidden_units, self.hidden_units),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(self.hidden_units, self.classes))\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         if self.experiment == 'classification':\n",
    "#             x = x.view(-1, self.input_shape) # Flatten images\n",
    "       \n",
    "#         x = self.net(x)\n",
    "#         return x\n",
    "\n",
    "#     def enable_dropout(self):\n",
    "#         ''' Enable the dropout layers during test-time '''\n",
    "#         for m in self.modules():\n",
    "#             if m.__class__.__name__.startswith('Dropout'):\n",
    "#                 m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491f9a90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.895004Z",
     "iopub.status.busy": "2025-06-05T13:14:41.894779Z",
     "iopub.status.idle": "2025-06-05T13:14:41.903891Z",
     "shell.execute_reply": "2025-06-05T13:14:41.903120Z"
    },
    "papermill": {
     "duration": 0.015858,
     "end_time": "2025-06-05T13:14:41.905247",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.889389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegConfig:\n",
    "    # save_dir = './saved_models'\n",
    "    train_size = 1024\n",
    "    batch_size = 128\n",
    "    lr = 1e-3\n",
    "    epochs = 100 #1000\n",
    "    train_samples = 5                   # number of train samples for MC gradients\n",
    "    test_samples = 10                   # number of test samples for MC averaging\n",
    "    num_test_points = 400               # number of test points\n",
    "    experiment = 'regression'\n",
    "    hidden_units = 400                  # number of hidden units\n",
    "    noise_tolerance = .1                # log likelihood sigma\n",
    "    mu_init = [-0.2, 0.2]               # range for mean \n",
    "    rho_init = [-5, -4]                 # range for rho_param\n",
    "    prior_init = [0.5, -0, -6]        # mixture weight, log(stddev_1), log(stddev_2)\n",
    "   \n",
    "\n",
    "class RLConfig:\n",
    "    data_dir = '/kaggle/input/mushroom/agaricus-lepiota.data' \n",
    "    batch_size = 64\n",
    "    num_batches = 64\n",
    "    buffer_size = batch_size * num_batches  # buffer to track latest batch of mushrooms\n",
    "    lr = 1e-4\n",
    "    training_steps = 5000 # 50000\n",
    "    experiment = 'regression'\n",
    "    hidden_units = 100                      # number of hidden units\n",
    "    mu_init = [-0.2, 0.2]                   # range for mean \n",
    "    rho_init = [-5, -4]                     # range for rho_param\n",
    "    prior_init = [0.5, -0, -6]              # mixture weight, log(stddev_1), log(stddev_2)\n",
    "\n",
    "class ClassConfig:\n",
    "    batch_size = 128\n",
    "    lr = 1e-3 # 1e-5 fa schifo, 1e-4 parte da 8% errore, 1e-3 parte da 5%\n",
    "    epochs = 1 #600\n",
    "    hidden_units = 1200\n",
    "    experiment = 'classification'\n",
    "    dropout = False\n",
    "    train_samples = 1 # 10 è troppo lento\n",
    "    test_samples = 10\n",
    "    x_shape = 28 * 28                       # x shape\n",
    "    classes = 10                            # number of output classes\n",
    "    mu_init = [-0.2, 0.2]                   # range for mean \n",
    "    rho_init = [-5, -4]                     # range for rho_param\n",
    "    prior_init = [0.5, -0, -8]             # mixture weight, log(stddev_1), log(stddev_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b22f704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.919361Z",
     "iopub.status.busy": "2025-06-05T13:14:41.919153Z",
     "iopub.status.idle": "2025-06-05T13:14:41.928310Z",
     "shell.execute_reply": "2025-06-05T13:14:41.927544Z"
    },
    "papermill": {
     "duration": 0.018197,
     "end_time": "2025-06-05T13:14:41.929610",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.911413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrepareData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        if not torch.is_tensor(X):\n",
    "            self.X = torch.from_numpy(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "        if not torch.is_tensor(y):\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = y # vedere\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def read_data_rl(data_dir):\n",
    "    df = pd.read_csv(data_dir, sep=',', header=None)\n",
    "    df.columns = ['class','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment',\n",
    "         'gill-spacing','gill-size','gill-color','stalk-shape','stalk-root',\n",
    "         'stalk-surf-above-ring','stalk-surf-below-ring','stalk-color-above-ring','stalk-color-below-ring',\n",
    "         'veil-type','veil-color','ring-number','ring-type','spore-color','population','habitat']\n",
    "    X = pd.DataFrame(df, columns=df.columns[1:len(df.columns)], index=df.index)\n",
    "    Y = df['class']\n",
    "\n",
    "    # transform to one-hot encoding\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    label_encoder.fit(Y)\n",
    "    Y_encoded = label_encoder.transform(Y)\n",
    "    oh_X = X.copy()\n",
    "    for feature in X.columns:\n",
    "        label_encoder.fit(X[feature])\n",
    "        oh_X[feature] = label_encoder.transform(X[feature])\n",
    "\n",
    "    oh_encoder = preprocessing.OneHotEncoder()\n",
    "    oh_encoder.fit(oh_X)\n",
    "    oh_X = oh_encoder.transform(oh_X).toarray()\n",
    "\n",
    "    return oh_X, Y_encoded\n",
    "\n",
    "def create_data_reg(train_size):\n",
    "    np.random.seed(0)\n",
    "    xs = np.random.uniform(low=0., high=0.6, size=train_size)\n",
    "    \n",
    "    eps = np.random.normal(loc=0., scale=0.02, size=[train_size])\n",
    "\n",
    "    ys = xs + 0.3 * np.sin(2*np.pi * (xs + eps)) + 0.3 * np.sin(4*np.pi * (xs + eps)) + eps\n",
    "\n",
    "    xs = torch.from_numpy(xs).reshape(-1,1).float()\n",
    "    ys = torch.from_numpy(ys).reshape(-1,1).float()\n",
    "\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5670d18b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.942891Z",
     "iopub.status.busy": "2025-06-05T13:14:41.942146Z",
     "iopub.status.idle": "2025-06-05T13:14:41.946892Z",
     "shell.execute_reply": "2025-06-05T13:14:41.946384Z"
    },
    "papermill": {
     "duration": 0.012378,
     "end_time": "2025-06-05T13:14:41.947961",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.935583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_bnn_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#         'mu_init': config.mu_init,\n",
    "#         'rho_init': config.rho_init,\n",
    "#         'prior_init': config.prior_init\n",
    "#     }\n",
    "#     model = BayesianNetwork(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()\n",
    "\n",
    "# def load_mlp_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#     }\n",
    "#     model = MLP(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()\n",
    "\n",
    "# def load_dropout_class_model(saved_model):\n",
    "#     config = ClassConfig\n",
    "#     model_params = {\n",
    "#         'input_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#         'dropout': True\n",
    "#     }\n",
    "#     model = MLP_Dropout(model_params)\n",
    "#     model.load_state_dict(torch.load(saved_model))\n",
    "\n",
    "#     return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a4a67b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.956206Z",
     "iopub.status.busy": "2025-06-05T13:14:41.956006Z",
     "iopub.status.idle": "2025-06-05T13:14:41.961286Z",
     "shell.execute_reply": "2025-06-05T13:14:41.960792Z"
    },
    "papermill": {
     "duration": 0.010586,
     "end_time": "2025-06-05T13:14:41.962293",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.951707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_regression_plot(X_test, y_test, train_ds):\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    plt.plot(X_test, np.median(y_test, axis=0), label='Median Posterior Predictive')\n",
    "    \n",
    "    # Range\n",
    "    plt.fill_between(\n",
    "        X_test.reshape(-1), \n",
    "        np.percentile(y_test, 0, axis=0), \n",
    "        np.percentile(y_test, 100, axis=0), \n",
    "        alpha = 0.2, color='orange', label='Range') #color='blue',\n",
    "    \n",
    "    # interquartile range\n",
    "    plt.fill_between(\n",
    "        X_test.reshape(-1), \n",
    "        np.percentile(y_test, 25, axis=0), \n",
    "        np.percentile(y_test, 75, axis=0), \n",
    "        alpha = 0.4,  label='Interquartile Range') #color='red',\n",
    "    \n",
    "    plt.scatter(train_ds.dataset.X, train_ds.dataset.y, label='Training data', marker='x', alpha=0.5, color='k', s=2)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.ylim([-1.5, 1.5])\n",
    "    plt.xlim([-0.6, 1.4])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57adb61",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.970456Z",
     "iopub.status.busy": "2025-06-05T13:14:41.970248Z",
     "iopub.status.idle": "2025-06-05T13:14:41.980844Z",
     "shell.execute_reply": "2025-06-05T13:14:41.980144Z"
    },
    "papermill": {
     "duration": 0.016111,
     "end_time": "2025-06-05T13:14:41.982004",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.965893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Classification():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.n_samples = parameters['train_samples']\n",
    "        self.test_samples = parameters['test_samples']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.classes = parameters['classes']\n",
    "        self.mu_init = parameters['mu_init']\n",
    "        self.rho_init = parameters['rho_init']\n",
    "        self.prior_init = parameters['prior_init']\n",
    "        self.best_acc = 0.\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.classes,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment,\n",
    "            'mu_init': self.mu_init,\n",
    "            'rho_init': self.rho_init,\n",
    "            'prior_init': self.prior_init,\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n",
    "        # print(f'Classification Task {self.label} Parameters: ')\n",
    "        # print(f'number of samples: {self.n_samples}')\n",
    "        # print(\"BNN Parameters: \")\n",
    "        # print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for idx, (x, y) in enumerate(tqdm(train_data)):\n",
    "            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)            \n",
    "            net_loss = self.loss_info[0]\n",
    "            net_loss.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = torch.zeros(size=[self.batch_size, self.classes]).to(device)\n",
    "        for _ in torch.arange(self.test_samples):\n",
    "            out = torch.nn.Softmax(dim=1)(self.net(X))\n",
    "            probs = probs + out / self.test_samples\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds, probs\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader):\n",
    "                X, y = data\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds, _ = self.predict(X)\n",
    "                total += self.batch_size\n",
    "                correct += (preds == y).sum().item()\n",
    "        self.acc = correct / total\n",
    "        print(f'validation accuracy: {self.acc}')  \n",
    "        return self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4b04d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:41.990447Z",
     "iopub.status.busy": "2025-06-05T13:14:41.990039Z",
     "iopub.status.idle": "2025-06-05T13:14:41.999377Z",
     "shell.execute_reply": "2025-06-05T13:14:41.998809Z"
    },
    "papermill": {
     "duration": 0.014708,
     "end_time": "2025-06-05T13:14:42.000432",
     "exception": false,
     "start_time": "2025-06-05T13:14:41.985724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Classification():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.classes = parameters['classes']\n",
    "        self.best_acc = 0.\n",
    "        self.dropout = parameters['dropout']\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.classes,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment,\n",
    "            'dropout': self.dropout,\n",
    "        }\n",
    "        if self.dropout:\n",
    "            self.net = MLP_Dropout(model_params).to(device)\n",
    "            print('MLP Dropout Parameters: ')\n",
    "            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "        else:\n",
    "            self.net = MLP(model_params).to(device)\n",
    "            print('MLP Parameters: ')\n",
    "            print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "        self.optimiser = torch.optim.SGD(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=100, gamma=0.5)\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for _, (x, y) in enumerate(tqdm(train_data)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = torch.nn.functional.cross_entropy(self.net(x), y, reduction='sum')\n",
    "            self.loss_info.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = torch.nn.Softmax(dim=1)(self.net(X))\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        return preds, probs\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.net.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader):\n",
    "                X, y = data\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds, _ = self.predict(X)\n",
    "                total += self.batch_size\n",
    "                correct += (preds == y).sum().item()\n",
    "        self.acc = correct / total\n",
    "        print(f'{self.label} validation accuracy: {self.acc}') \n",
    "        return self.acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8c449f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:42.008972Z",
     "iopub.status.busy": "2025-06-05T13:14:42.008756Z",
     "iopub.status.idle": "2025-06-05T13:14:42.012585Z",
     "shell.execute_reply": "2025-06-05T13:14:42.012122Z"
    },
    "papermill": {
     "duration": 0.009304,
     "end_time": "2025-06-05T13:14:42.013610",
     "exception": false,
     "start_time": "2025-06-05T13:14:42.004306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def class_trainer():\n",
    "#     config = ClassConfig\n",
    "    \n",
    "#     transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Lambda(lambda x: x * 255 / 126.),  # divide as in paper, * 255 gives better results\n",
    "#         ])\n",
    "\n",
    "#     train_data = datasets.MNIST(\n",
    "#             root='data',\n",
    "#             train=True,\n",
    "#             download=True,\n",
    "#             transform=transform)\n",
    "#     test_data = datasets.MNIST(\n",
    "#             root='data',\n",
    "#             train=False,\n",
    "#             download=True,\n",
    "#             transform=transform)\n",
    "\n",
    "#     valid_size = 1 / 6\n",
    "\n",
    "#     num_train = len(train_data)\n",
    "#     indices = list(range(num_train))\n",
    "#     split = int(valid_size * num_train)\n",
    "#     train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "#     train_sampler = SubsetRandomSampler(train_idx)\n",
    "#     valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#             train_data,\n",
    "#             batch_size=config.batch_size,\n",
    "#             sampler=train_sampler,\n",
    "#             drop_last=True)\n",
    "#     valid_loader = torch.utils.data.DataLoader(\n",
    "#             train_data,\n",
    "#             batch_size=config.batch_size,\n",
    "#             sampler=valid_sampler,\n",
    "#             drop_last=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#             test_data,\n",
    "#             batch_size=config.batch_size,\n",
    "#             shuffle=False,\n",
    "#             drop_last=True)\n",
    "\n",
    "#     params = {\n",
    "#         'lr': config.lr,\n",
    "#         'hidden_units': config.hidden_units,\n",
    "#         'experiment': config.experiment,\n",
    "#         'dropout': config.dropout,\n",
    "#         'batch_size': config.batch_size,\n",
    "#         'epochs': config.epochs,\n",
    "#         'x_shape': config.x_shape,\n",
    "#         'classes': config.classes,\n",
    "#         'num_batches': len(train_loader),\n",
    "#         'train_samples': config.train_samples,\n",
    "#         'test_samples': config.test_samples,\n",
    "#         'mu_init': config.mu_init,\n",
    "#         'rho_init': config.rho_init,\n",
    "#         'prior_init': config.prior_init,\n",
    "#     }\n",
    "\n",
    "#     model = BNN_Classification('bnn_classification', {**params})\n",
    "#     #model = MLP_Classification('mlp_classification', {**params})\n",
    "    \n",
    "#     epochs = config.epochs\n",
    "#     for epoch in range(epochs):\n",
    "#             print(f'Epoch {epoch+1}/{epochs}')\n",
    "#             model.train_step(train_loader)\n",
    "#             valid_acc = model.evaluate(valid_loader)\n",
    "#             # test_acc = model.evaluate(test_loader)\n",
    "#             print('Valid Error', round(100 * (1 - valid_acc), 3), '%',)\n",
    "#             model.scheduler.step()\n",
    "#             if model.acc > model.best_acc:\n",
    "#                 model.best_acc = model.acc\n",
    "                \n",
    "# #class_trainer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a660d2f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:42.021888Z",
     "iopub.status.busy": "2025-06-05T13:14:42.021504Z",
     "iopub.status.idle": "2025-06-05T13:14:42.029941Z",
     "shell.execute_reply": "2025-06-05T13:14:42.029459Z"
    },
    "papermill": {
     "duration": 0.013721,
     "end_time": "2025-06-05T13:14:42.030959",
     "exception": false,
     "start_time": "2025-06-05T13:14:42.017238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search_config = {\n",
    "#     'batch_size': [128],\n",
    "#     'lr': [1e-3, 1e-4],\n",
    "#     'epochs': [10],\n",
    "#     'hidden_units': [1200],\n",
    "#     'experiment': ['classification'],\n",
    "#     'dropout': [False],\n",
    "#     'train_samples': [1, 2, 5],\n",
    "#     'test_samples': [10],\n",
    "#     'x_shape': [28 * 28],\n",
    "#     'classes': [10],\n",
    "#     'mu_init': [[-0.2, 0.2]],\n",
    "#     'rho_init': [[-5, -4]],\n",
    "#     'prior_init': [\n",
    "#         [0.25, -0, -6],\n",
    "#         [0.25, -0, -7], \n",
    "#         [0.25, -1, -6], \n",
    "#         [0.25, -1, -7], \n",
    "#         [0.75, -0, -6],\n",
    "#         [0.75, -0, -7], \n",
    "#         [0.75, -1, -6], \n",
    "#         [0.75, -1, -7],       \n",
    "#     ]\n",
    "# }\n",
    "\n",
    "search_config = {\n",
    "    'batch_size': [128],\n",
    "    'lr': [1e-3, 1e-4],\n",
    "    'epochs': [1], #10\n",
    "    'hidden_units': [1200],\n",
    "    'experiment': ['classification'],\n",
    "    'dropout': [False],\n",
    "    'train_samples': [1],\n",
    "    'test_samples': [10],\n",
    "    'x_shape': [28 * 28],\n",
    "    'classes': [10],\n",
    "    'mu_init': [[-0.2, 0.2]],\n",
    "    'rho_init': [[-5, -4]],\n",
    "    'prior_init': [\n",
    "        [0.25, -0, -6],        \n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "\n",
    "def generate_param_combinations(param_grid):\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    for combo in itertools.product(*values):\n",
    "        yield dict(zip(keys, combo))\n",
    "\n",
    "\n",
    "def class_trainer(config):\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x * 255 / 126.),  # divide as in paper, * 255 gives better results\n",
    "        ])\n",
    "\n",
    "    train_data = datasets.MNIST(\n",
    "            root='data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transform)\n",
    "    # test_data = datasets.MNIST(\n",
    "    #         root='data',\n",
    "    #         train=False,\n",
    "    #         download=True,\n",
    "    #         transform=transform)\n",
    "\n",
    "    valid_size = 1 / 6\n",
    "\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(valid_size * num_train)\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            drop_last=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=config['batch_size'],\n",
    "            sampler=valid_sampler,\n",
    "            drop_last=True)\n",
    "    # test_loader = torch.utils.data.DataLoader(\n",
    "    #         test_data,\n",
    "    #         batch_size=config.batch_size,\n",
    "    #         shuffle=False,\n",
    "    #         drop_last=True)\n",
    "\n",
    "    params = deepcopy(config)\n",
    "    params['num_batches'] = len(train_loader)\n",
    "\n",
    "    model = BNN_Classification('bnn_classification', {**params})\n",
    "    #model = MLP_Classification('mlp_classification', {**params})\n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    for epoch in range(epochs):\n",
    "            print(f'Epoch {epoch+1}/{epochs}')\n",
    "            model.train_step(train_loader)\n",
    "            valid_acc = model.evaluate(valid_loader)\n",
    "            print('Valid Error', round(100 * (1 - valid_acc), 3), '%',)\n",
    "            model.scheduler.step()\n",
    "            if model.acc > model.best_acc:\n",
    "                model.best_acc = model.acc\n",
    "                \n",
    "\n",
    "    return model.best_acc, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef98e7ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:14:42.039203Z",
     "iopub.status.busy": "2025-06-05T13:14:42.039019Z",
     "iopub.status.idle": "2025-06-05T13:15:56.816990Z",
     "shell.execute_reply": "2025-06-05T13:15:56.816223Z"
    },
    "papermill": {
     "duration": 74.783633,
     "end_time": "2025-06-05T13:15:56.818321",
     "exception": false,
     "start_time": "2025-06-05T13:14:42.034688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying config: {'batch_size': 128, 'lr': 0.001, 'epochs': 1, 'hidden_units': 1200, 'experiment': 'classification', 'dropout': False, 'train_samples': 1, 'test_samples': 10, 'x_shape': 784, 'classes': 10, 'mu_init': [-0.2, 0.2], 'rho_init': [-5, -4], 'prior_init': [0.25, 0, -6]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.6MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.32MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:17<00:00, 21.69it/s]\n",
      "100%|██████████| 78/78 [00:18<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.9533253205128205\n",
      "Valid Error 4.667 %\n",
      "Trying config: {'batch_size': 128, 'lr': 0.0001, 'epochs': 1, 'hidden_units': 1200, 'experiment': 'classification', 'dropout': False, 'train_samples': 1, 'test_samples': 10, 'x_shape': 784, 'classes': 10, 'mu_init': [-0.2, 0.2], 'rho_init': [-5, -4], 'prior_init': [0.25, 0, -6]}\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 390/390 [00:15<00:00, 25.31it/s]\n",
      "100%|██████████| 78/78 [00:18<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.9099559294871795\n",
      "Valid Error 9.004 %\n",
      "Best Config:\n",
      "{'batch_size': 128, 'lr': 0.001, 'epochs': 1, 'hidden_units': 1200, 'experiment': 'classification', 'dropout': False, 'train_samples': 1, 'test_samples': 10, 'x_shape': 784, 'classes': 10, 'mu_init': [-0.2, 0.2], 'rho_init': [-5, -4], 'prior_init': [0.25, 0, -6]}\n",
      "Best Validation Accuracy: 0.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "best_config = None\n",
    "best_model = None\n",
    "\n",
    "for config in generate_param_combinations(search_config):\n",
    "    print(f\"Trying config: {config}\")\n",
    "    val_acc, model = class_trainer(config)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_config = deepcopy(config)\n",
    "        best_model = model\n",
    "\n",
    "print(\"Best Config:\")\n",
    "print(best_config)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee91a937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:56.860880Z",
     "iopub.status.busy": "2025-06-05T13:15:56.860660Z",
     "iopub.status.idle": "2025-06-05T13:15:56.864981Z",
     "shell.execute_reply": "2025-06-05T13:15:56.864391Z"
    },
    "papermill": {
     "duration": 0.026596,
     "end_time": "2025-06-05T13:15:56.865995",
     "exception": false,
     "start_time": "2025-06-05T13:15:56.839399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import SubsetRandomSampler\n",
    "# from torchvision import datasets, transforms\n",
    "# from itertools import product\n",
    "\n",
    "# def class_trainer(config):\n",
    "\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Lambda(lambda x: x * 255. / 126.),\n",
    "#     ])\n",
    "\n",
    "#     train_data = datasets.MNIST(\n",
    "#         root='data',\n",
    "#         train=True,\n",
    "#         download=True,\n",
    "#         transform=transform)\n",
    "#     test_data = datasets.MNIST(\n",
    "#         root='data',\n",
    "#         train=False,\n",
    "#         download=True,\n",
    "#         transform=transform)\n",
    "\n",
    "#     valid_size = 1 / 6\n",
    "#     num_train = len(train_data)\n",
    "#     indices = list(range(num_train))\n",
    "#     split = int(valid_size * num_train)\n",
    "#     train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "#     train_sampler = SubsetRandomSampler(train_idx)\n",
    "#     valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         sampler=train_sampler,\n",
    "#         drop_last=True)\n",
    "#     valid_loader = torch.utils.data.DataLoader(\n",
    "#         train_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         sampler=valid_sampler,\n",
    "#         drop_last=True)\n",
    "#     test_loader = torch.utils.data.DataLoader(\n",
    "#         test_data,\n",
    "#         batch_size=config['batch_size'],\n",
    "#         shuffle=False,\n",
    "#         drop_last=True)\n",
    "\n",
    "#     params = {\n",
    "#         'lr': config['lr'],\n",
    "#         'hidden_units': config['hidden_units'],\n",
    "#         'experiment': config['experiment'],\n",
    "#         'batch_size': config['batch_size'],\n",
    "#         'epochs': config['epochs'],\n",
    "#         'x_shape': config['x_shape'],\n",
    "#         'classes': config['classes'],\n",
    "#         'num_batches': len(train_loader),\n",
    "#         'train_samples': config['train_samples'],\n",
    "#         'test_samples': config['test_samples'],\n",
    "#         'mu_init': config['mu_init'],\n",
    "#         'rho_init': config['rho_init'],\n",
    "#         'prior_init': config['prior_init'],\n",
    "#     }\n",
    "\n",
    "#     model = BNN_Classification('bnn_classification', {**params, 'dropout': False})\n",
    "\n",
    "#     best_val_acc = 0\n",
    "#     for epoch in range(config['epochs']):\n",
    "#         print(f'Epoch {epoch + 1}/{config[\"epochs\"]}')\n",
    "#         model.train_step(train_loader)\n",
    "#         valid_acc = model.evaluate(valid_loader)\n",
    "#         print('Valid Error', round(100 * (1 - valid_acc), 3), '%')\n",
    "#         model.scheduler.step()\n",
    "#         if model.acc > model.best_acc:\n",
    "#             model.best_acc = model.acc\n",
    "#             best_val_acc = valid_acc\n",
    "#             # torch.save(model.net.state_dict(), \"best_model.pt\")\n",
    "\n",
    "#     return best_val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869c517d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:56.907333Z",
     "iopub.status.busy": "2025-06-05T13:15:56.907149Z",
     "iopub.status.idle": "2025-06-05T13:15:56.910748Z",
     "shell.execute_reply": "2025-06-05T13:15:56.910008Z"
    },
    "papermill": {
     "duration": 0.025474,
     "end_time": "2025-06-05T13:15:56.911817",
     "exception": false,
     "start_time": "2025-06-05T13:15:56.886343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_grid_search():\n",
    "#     # Define the grid\n",
    "#     param_grid = {\n",
    "#         'lr': [1e-3, 1e-4],\n",
    "#         'batch_size': [64, 128],\n",
    "#         'hidden_units': [128, 256],\n",
    "#     }\n",
    "\n",
    "#     # Fixed config options\n",
    "#     base_config = {\n",
    "#         'experiment': 'grid_search',\n",
    "#         'epochs': 5,\n",
    "#         'x_shape': (1, 28, 28),\n",
    "#         'classes': 10,\n",
    "#         'train_samples': 60000,\n",
    "#         'test_samples': 10000,\n",
    "#         'mu_init': 0,\n",
    "#         'rho_init': -3,\n",
    "#         'prior_init': 0,\n",
    "#     }\n",
    "\n",
    "#     best_acc = 0\n",
    "#     best_params = None\n",
    "\n",
    "#     # Iterate over all combinations\n",
    "#     for lr, batch_size, hidden_units in product(param_grid['lr'], param_grid['batch_size'], param_grid['hidden_units']):\n",
    "#         config = {\n",
    "#             **base_config,\n",
    "#             'lr': lr,\n",
    "#             'batch_size': batch_size,\n",
    "#             'hidden_units': hidden_units\n",
    "#         }\n",
    "\n",
    "#         print(f\"\\nRunning with config: {config}\")\n",
    "#         acc = class_trainer(config)\n",
    "\n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_params = config\n",
    "\n",
    "#     print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
    "#     print(f\"Best Config: {best_params}\")\n",
    "\n",
    "# run_grid_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70aa99d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:56.953408Z",
     "iopub.status.busy": "2025-06-05T13:15:56.953168Z",
     "iopub.status.idle": "2025-06-05T13:15:56.962176Z",
     "shell.execute_reply": "2025-06-05T13:15:56.961666Z"
    },
    "papermill": {
     "duration": 0.031002,
     "end_time": "2025-06-05T13:15:56.963134",
     "exception": false,
     "start_time": "2025-06-05T13:15:56.932132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Regression():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.n_samples = parameters['train_samples']\n",
    "        self.test_samples = parameters['test_samples']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.y_shape = parameters['y_shape']\n",
    "        self.noise_tol = parameters['noise_tolerance']\n",
    "        self.lr = parameters['lr']\n",
    "        self.best_loss = np.inf\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.y_shape,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment'],\n",
    "            'mu_init': parameters['mu_init'],\n",
    "            'rho_init': parameters['rho_init'],\n",
    "            'prior_init': parameters['prior_init']\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=500, gamma=0.5)\n",
    "        # print(f'Regression Task {self.label} Parameters: ')\n",
    "        # print(f'number of samples: {self.n_samples}, noise tolerance: {self.noise_tol}')\n",
    "        print(\"BNN Parameters: \")\n",
    "        print(f'batch size: {self.batch_size}, x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, mu_init: {parameters[\"mu_init\"]}, rho_init: {parameters[\"rho_init\"]}, prior_init: {parameters[\"prior_init\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for idx, (x, y) in enumerate(train_data):\n",
    "            beta = 2 ** (self.num_batches - (idx + 1)) / (2 ** self.num_batches - 1) \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = self.net.sample_elbo(x, y, beta, self.n_samples, sigma=self.noise_tol)\n",
    "            net_loss = self.loss_info[0]\n",
    "            net_loss.backward()\n",
    "            self.optimiser.step()\n",
    "        self.epoch_loss = net_loss.item()\n",
    "\n",
    "    def evaluate(self, x_test):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test = np.zeros((self.test_samples, x_test.shape[0]))\n",
    "            for s in range(self.test_samples):\n",
    "                tmp = self.net(x_test.to(device)).detach().cpu().numpy()\n",
    "                y_test[s,:] = tmp.reshape(-1)\n",
    "            return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b60ea192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:57.006090Z",
     "iopub.status.busy": "2025-06-05T13:15:57.005851Z",
     "iopub.status.idle": "2025-06-05T13:15:57.013023Z",
     "shell.execute_reply": "2025-06-05T13:15:57.012476Z"
    },
    "papermill": {
     "duration": 0.030259,
     "end_time": "2025-06-05T13:15:57.013989",
     "exception": false,
     "start_time": "2025-06-05T13:15:56.983730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Regression():\n",
    "    def __init__(self, label, parameters):\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.lr = parameters['lr']\n",
    "        self.hidden_units = parameters['hidden_units']\n",
    "        self.experiment = parameters['experiment']\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.num_batches = parameters['num_batches']\n",
    "        self.x_shape = parameters['x_shape']\n",
    "        self.y_shape = parameters['y_shape']\n",
    "        self.best_loss = np.inf\n",
    "        self.init_net(parameters)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x_shape,\n",
    "            'classes': self.y_shape,\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'experiment': self.experiment\n",
    "        }\n",
    "        self.net = MLP(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(\"MLP Parameters: \")\n",
    "        print(f'batch size: {self.batch_size}, input shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        self.net.train()\n",
    "        for _, (x, y) in enumerate(train_data):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            self.net.zero_grad()\n",
    "            self.loss_info = torch.nn.functional.mse_loss(self.net(x), y, reduction='sum')\n",
    "            self.loss_info.backward()\n",
    "            self.optimiser.step()\n",
    "\n",
    "        self.epoch_loss = self.loss_info.item()\n",
    "\n",
    "    def evaluate(self, x_test):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test = self.net(x_test.to(device)).detach().cpu().numpy()\n",
    "            return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24d084a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:57.055408Z",
     "iopub.status.busy": "2025-06-05T13:15:57.055170Z",
     "iopub.status.idle": "2025-06-05T13:15:57.060970Z",
     "shell.execute_reply": "2025-06-05T13:15:57.060373Z"
    },
    "papermill": {
     "duration": 0.0279,
     "end_time": "2025-06-05T13:15:57.061989",
     "exception": false,
     "start_time": "2025-06-05T13:15:57.034089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reg_trainer():\n",
    "    config = RegConfig\n",
    "    X, Y = create_data_reg(train_size=config.train_size)\n",
    "    train_loader = PrepareData(X, Y)\n",
    "    train_loader = DataLoader(train_loader, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    params = {\n",
    "        'lr': config.lr,\n",
    "        'hidden_units': config.hidden_units,\n",
    "        'experiment': config.experiment,\n",
    "        'batch_size': config.batch_size,\n",
    "        'num_batches': len(train_loader),\n",
    "        'x_shape': X.shape[1],\n",
    "        'y_shape': Y.shape[1],\n",
    "        'train_samples': config.train_samples,\n",
    "        'test_samples': config.test_samples,\n",
    "        'noise_tolerance': config.noise_tolerance,\n",
    "        'mu_init': config.mu_init,\n",
    "        'rho_init': config.rho_init,\n",
    "        'prior_init': config.prior_init,\n",
    "    }\n",
    "\n",
    "    model = BNN_Regression('bnn_regression', {**params})\n",
    "    #model = MLP_Regression('mlp_regression', {**params})\n",
    "\n",
    "    epochs = config.epochs\n",
    "    print(f\"Initialising training on {device}...\")\n",
    "\n",
    "    # training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "        model.train_step(train_loader)\n",
    "        model.scheduler.step()\n",
    "        # save best model\n",
    "        if model.epoch_loss < model.best_loss:\n",
    "            model.best_loss = model.epoch_loss\n",
    "            # torch.save(model.net.state_dict(), model.save_model_path)\n",
    "\n",
    "    # evaluate\n",
    "    print(\"Evaluating and generating plots...\")\n",
    "    x_test = torch.linspace(-2., 2, config.num_test_points).reshape(-1, 1)\n",
    "    \n",
    "    # model.net.load_state_dict(torch.load(model.save_model_path, map_location=torch.device(device)))\n",
    "    y_test = model.evaluate(x_test)\n",
    "    \n",
    "    #create_regression_plot(x_test.cpu().numpy(), y_test.reshape(1, -1), train_loader) #per mlp regression\n",
    "   \n",
    "    create_regression_plot(x_test.cpu().numpy(), y_test, train_loader)\n",
    "\n",
    "\n",
    "#reg_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcc94f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:57.148082Z",
     "iopub.status.busy": "2025-06-05T13:15:57.147790Z",
     "iopub.status.idle": "2025-06-05T13:15:57.159821Z",
     "shell.execute_reply": "2025-06-05T13:15:57.159113Z"
    },
    "papermill": {
     "duration": 0.078531,
     "end_time": "2025-06-05T13:15:57.160958",
     "exception": false,
     "start_time": "2025-06-05T13:15:57.082427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self, label, bandit_params, x, y):\n",
    "        self.n_samples = bandit_params['n_samples']\n",
    "        self.buffer_size = bandit_params['buffer_size']\n",
    "        self.batch_size = bandit_params['batch_size']\n",
    "        self.num_batches = bandit_params['num_batches']\n",
    "        self.lr = bandit_params['lr']\n",
    "        self.epsilon = bandit_params['epsilon']\n",
    "        self.cumulative_regrets = [0]\n",
    "        self.buffer_x, self.buffer_y = [], []\n",
    "        self.x, self.y = x, y\n",
    "        self.label = label\n",
    "        self.init_net(bandit_params)\n",
    "        self.tp, self.tn, self.fp, self.fn = 0, 0, 0, 0\n",
    "\n",
    "    def get_agent_reward(self, eaten, edible):\n",
    "        if not eaten:\n",
    "            return 0\n",
    "        if eaten and edible:\n",
    "            return 5\n",
    "        elif eaten and not edible:\n",
    "            return 5 if np.random.rand() > 0.5 else -35\n",
    "\n",
    "    def get_oracle_reward(self, edible):\n",
    "        return 5*edible \n",
    "\n",
    "    def take_action(self, mushroom):\n",
    "        context, edible = self.x[mushroom], self.y[mushroom]\n",
    "        eat_tuple = torch.FloatTensor(np.concatenate((context, [1, 0]))).unsqueeze(0).to(device)\n",
    "        reject_tuple = torch.FloatTensor(np.concatenate((context, [0, 1]))).unsqueeze(0).to(device)\n",
    "\n",
    "        # evaluate reward for actions\n",
    "        with torch.no_grad():\n",
    "            self.net.eval()\n",
    "            reward_eat = sum([self.net(eat_tuple) for _ in range(self.n_samples)]).item()\n",
    "            reward_reject = sum([self.net(reject_tuple) for _ in range(self.n_samples)]).item()\n",
    "\n",
    "        eat = reward_eat > reward_reject\n",
    "        # epsilon-greedy agent\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            eat = (np.random.rand() < 0.5)\n",
    "        agent_reward = self.get_agent_reward(eat, edible)\n",
    "\n",
    "        # record bandit action\n",
    "        if edible and eat:\n",
    "            self.tp += 1\n",
    "        elif edible and not eat:\n",
    "            self.fn += 1\n",
    "        elif not edible and eat:\n",
    "            self.fp += 1\n",
    "        else:\n",
    "            self.tn += 1\n",
    "\n",
    "        # record context, action, reward\n",
    "        action = torch.Tensor([1, 0] if eat else [0, 1])\n",
    "        self.buffer_x.append(np.concatenate((context, action)))\n",
    "        self.buffer_y.append(agent_reward)\n",
    "\n",
    "        # calculate regret\n",
    "        regret = self.get_oracle_reward(edible) - agent_reward\n",
    "        self.cumulative_regrets.append(self.cumulative_regrets[-1]+regret)\n",
    "\n",
    "    def update(self, mushroom):\n",
    "        self.take_action(mushroom)\n",
    "        l = len(self.buffer_x)\n",
    "\n",
    "        if l <= self.batch_size:\n",
    "            idx_pool = int(self.batch_size//l + 1)*list(range(l))\n",
    "            idx_pool = np.random.permutation(idx_pool[-self.batch_size:])\n",
    "        elif l > self.batch_size and l < self.buffer_size:\n",
    "            idx_pool = int(l//self.batch_size)*self.batch_size\n",
    "            idx_pool = np.random.permutation(list(range(l))[-idx_pool:])\n",
    "        else:\n",
    "            idx_pool = np.random.permutation(list(range(l))[-self.buffer_size:])\n",
    "\n",
    "        context_pool = torch.Tensor([self.buffer_x[i] for i in idx_pool]).to(device)\n",
    "        value_pool = torch.Tensor([self.buffer_y[i] for i in idx_pool]).to(device)\n",
    "        \n",
    "        for i in range(0, len(idx_pool), self.batch_size):\n",
    "            self.loss_info = self.loss_step(context_pool[i:i+self.batch_size], value_pool[i:i+self.batch_size], i//self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "993a0255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:57.203435Z",
     "iopub.status.busy": "2025-06-05T13:15:57.203194Z",
     "iopub.status.idle": "2025-06-05T13:15:57.212554Z",
     "shell.execute_reply": "2025-06-05T13:15:57.211846Z"
    },
    "papermill": {
     "duration": 0.031749,
     "end_time": "2025-06-05T13:15:57.213588",
     "exception": false,
     "start_time": "2025-06-05T13:15:57.181839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BNN_Bandit(Bandit):\n",
    "    def __init__(self, label, *args):\n",
    "        super().__init__(label, *args)\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x.shape[1]+2,\n",
    "            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment'],\n",
    "            'mu_init': parameters['mu_init'],\n",
    "            'rho_init': parameters['rho_init'],\n",
    "            'prior_init': parameters['prior_init']\n",
    "        }\n",
    "        self.net = BayesianNetwork(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(\"BNN Parameters: \")\n",
    "        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def loss_step(self, x, y, batch_id):\n",
    "        beta = 2 ** (self.num_batches - (batch_id + 1)) / (2 ** self.num_batches - 1) \n",
    "        self.net.train()\n",
    "        self.net.zero_grad()\n",
    "        loss_info = self.net.sample_elbo(x, y, beta, self.n_samples)\n",
    "        net_loss = loss_info[0]\n",
    "        net_loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return loss_info\n",
    "\n",
    "class Greedy_Bandit(Bandit):\n",
    "    def __init__(self, label, *args):\n",
    "        super().__init__(label, *args)\n",
    "        self.writer = SummaryWriter(comment=f\"_{label}_training\"),\n",
    "    \n",
    "    def init_net(self, parameters):\n",
    "        model_params = {\n",
    "            'input_shape': self.x.shape[1]+2,\n",
    "            'classes': 1 if len(self.y.shape)==1 else self.y.shape[1],\n",
    "            'batch_size': self.batch_size,\n",
    "            'hidden_units': parameters['hidden_units'],\n",
    "            'experiment': parameters['experiment']\n",
    "        }\n",
    "        self.net = MLP(model_params).to(device)\n",
    "        self.optimiser = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=5000, gamma=0.5)\n",
    "        print(f'Bandit {self.label} Parameters: ')\n",
    "        print(f'buffer_size: {self.buffer_size}, batch size: {self.batch_size}, number of samples: {self.n_samples}, epsilon: {self.epsilon}')\n",
    "        print(\"MLP Parameters: \")\n",
    "        print(f'x shape: {model_params[\"input_shape\"]}, hidden units: {model_params[\"hidden_units\"]}, output shape: {model_params[\"classes\"]}, lr: {self.lr}')\n",
    "\n",
    "    def loss_step(self, x, y, batch_id):\n",
    "        self.net.train()\n",
    "        self.net.zero_grad()\n",
    "        net_loss = torch.nn.functional.mse_loss(self.net(x).squeeze(), y, reduction='sum')\n",
    "        net_loss.backward()\n",
    "        self.optimiser.step()\n",
    "        return net_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028fdf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T13:15:57.256548Z",
     "iopub.status.busy": "2025-06-05T13:15:57.256275Z",
     "iopub.status.idle": "2025-06-05T13:15:57.262350Z",
     "shell.execute_reply": "2025-06-05T13:15:57.261656Z"
    },
    "papermill": {
     "duration": 0.029086,
     "end_time": "2025-06-05T13:15:57.263416",
     "exception": false,
     "start_time": "2025-06-05T13:15:57.234330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def rl_trainer():\n",
    "    config = RLConfig\n",
    "    X, Y = read_data_rl(config.data_dir)\n",
    "\n",
    "    params = {\n",
    "        'buffer_size': config.buffer_size,\n",
    "        'batch_size': config.batch_size,\n",
    "        'num_batches': config.num_batches,\n",
    "        'lr': config.lr,\n",
    "        'hidden_units': config.hidden_units,\n",
    "        'experiment': config.experiment,\n",
    "        'mu_init': config.mu_init,\n",
    "        'rho_init': config.rho_init,\n",
    "        'prior_init': config.prior_init\n",
    "    }\n",
    "\n",
    "    bandit = BNN_Bandit('bnn_bandit', {**params, 'n_samples':2, 'epsilon':0}, X, Y)\n",
    "    \n",
    "    training_steps = config.training_steps\n",
    "    print(f\"Initialising training on {device}...\")\n",
    "    training_data_len = len(X)\n",
    "    for step in tqdm(range(training_steps)):\n",
    "        mushroom = np.random.randint(training_data_len)\n",
    "        bandit.update(mushroom)\n",
    "        bandit.scheduler.step()\n",
    "\n",
    "    # Plot cumulative regret\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(bandit.cumulative_regrets, label='Cumulative Regret')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Cumulative Regret')\n",
    "    plt.title('Cumulative Regret over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#rl_trainer()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7591461,
     "sourceId": 12061096,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 93.269423,
   "end_time": "2025-06-05T13:15:59.519864",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-05T13:14:26.250441",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
